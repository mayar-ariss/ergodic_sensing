{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Open poweshell in new terminal and run\n",
    "\n",
    "docker build -t sensing-whale \"C:\\Users\\mayar\\OneDrive - Massachusetts Institute of Technology\\Desktop\\energy-aware\"\n",
    "\n",
    "After building the image, use -v to mount the local DATA directory inside /workspace/data/ in the container:\n",
    "\n",
    "docker run -it --gpus all --shm-size=1g --ulimit memlock=-1 --ulimit stack=67108864 -v \"C:\\Users\\mayar\\OneDrive - Massachusetts Institute of Technology\\Desktop\\energy-aware\\DATA:/workspace/data\" -p 8888:8888 sensing-whale"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "to detect docker containers run: docker ps\n",
    "\n",
    "to stop docker container: docker stop 'insert container name'\n",
    "\n",
    "to delete docker container: docker rm 'insert container name'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imported Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standard Library Imports\n",
    "import logging\n",
    "import multiprocessing as mp\n",
    "import os\n",
    "import random\n",
    "from collections import defaultdict\n",
    "from datetime import datetime\n",
    "\n",
    "# Check current working directory and list files\n",
    "print(os.getcwd())\n",
    "print(os.listdir())\n",
    "\n",
    "# Numerical & Data Processing \n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import cudf\n",
    "import cupy as cp\n",
    "import cuspatial\n",
    "import cuml  # RAPIDS cuML for accelerated machine learning\n",
    "import numba\n",
    "\n",
    "# Check GPU Status\n",
    "!nvidia-smi\n",
    "\n",
    "# Geospatial Processing \n",
    "import geopandas as gpd\n",
    "from shapely.geometry import Polygon, Point\n",
    "import shapely\n",
    "\n",
    "# Visualization \n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from matplotlib.colors import Normalize, to_hex\n",
    "import folium\n",
    "import branca.colormap as cm\n",
    "import matplotlib.colors as mcolors\n",
    "import optuna.visualization\n",
    "import contextily as ctx\n",
    "\n",
    "# Statistical & Curve Fitting \n",
    "import scipy.stats as stats\n",
    "from scipy.optimize import curve_fit\n",
    "from scipy.stats import kstest\n",
    "\n",
    "# Machine Learning & Optimization \n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error\n",
    "from hyperopt import fmin, tpe, hp, STATUS_OK, Trials\n",
    "import optuna\n",
    "import xgboost as xgb\n",
    "\n",
    "# Utility \n",
    "from kneed import KneeLocator\n",
    "from tqdm import tqdm  # Import tqdm for the progress bar\n",
    "\n",
    "print(\"RAPIDS & required libraries loaded successfully!\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Cleaning and Preprocessing\n",
    "\n",
    "Loads and process multi-sheet Excel data\n",
    "\n",
    "1. **File Loading**: Reads all sheets from `2022_vitals.xlsx` without headers.\n",
    "2. **Column Naming**: Assigns predefined column names for consistency.\n",
    "3. **Data Alignment**: \n",
    "   - Fixes misaligned rows by detecting valid `deviceID`.\n",
    "   - Ensures all rows have the correct number of columns.\n",
    "4. **Filtering**:\n",
    "   - Removes invalid or duplicate header rows.\n",
    "   - Drops rows with zero values for latitude (`Lat`) and longitude (`Log`).\n",
    "5. **Indexing**: Resets the index and assigns a sequential 1-based index.\n",
    "6. **Output**: Saves the cleaned data to `2022_vitals_cleaned.xlsx` and previews it."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Mount the local data directory to Docker;\n",
    "docker run -it --gpus all --shm-size=1g --ulimit memlock=-1 --ulimit stack=67108864 -p 8888:8888 `\n",
    "    -v \"C:\\Users\\mayar\\OneDrive - Massachusetts Institute of Technology\\Desktop\\energy-aware\\DATA:/workspace/data\" `\n",
    "    rapids-custom-container\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import random\n",
    "import string\n",
    "\n",
    "# Adjusted file path for Docker (mounted volume)\n",
    "file_path = \"/workspace/data/2022_vitals.xlsx\"\n",
    "output_path = \"/workspace/data/2022_vitals_cleaned.xlsx\"\n",
    "\n",
    "# Specify the column names explicitly\n",
    "column_names = [\n",
    "    \"deviceID\", \"Timestamp\", \"Lat\", \"Log\", \"SOC_batt\", \"temp_batt\", \"volatge_batt\",\n",
    "    \"voltage_particle\", \"current_batt\", \"isCharging\", \"isCharginS\", \"isCharged\",\n",
    "    \"Temp_int\", \"Hum_int\", \"solar_current\", \"Cellular_signal_strength\", \"index\"\n",
    "]\n",
    "\n",
    "# Load all sheets into a dictionary\n",
    "sheets_dict = pd.read_excel(file_path, sheet_name=None, header=None)  # No header initially\n",
    "\n",
    "# Process each sheet\n",
    "processed_sheets = []\n",
    "for sheet_name, sheet_data in sheets_dict.items():\n",
    "    # Ensure the number of columns matches the expected number\n",
    "    sheet_data = sheet_data.iloc[:, :len(column_names)]\n",
    "\n",
    "    # Fix misaligned rows where the first column is invalid\n",
    "    def fix_alignment(row):\n",
    "        # Convert the row to a list\n",
    "        row_list = row.tolist()\n",
    "\n",
    "        # Find the first valid `deviceID` (assumes valid `deviceID` has > 5 characters)\n",
    "        for i, value in enumerate(row_list):\n",
    "            if isinstance(value, str) and len(value) > 5:  # Valid `deviceID` found\n",
    "                aligned_row = row_list[i:i + len(column_names)]\n",
    "                return aligned_row + [None] * (len(column_names) - len(aligned_row))\n",
    "\n",
    "        return [None] * len(column_names)\n",
    "\n",
    "    # Apply alignment fix to all rows\n",
    "    sheet_data = sheet_data.apply(fix_alignment, axis=1, result_type=\"expand\")\n",
    "\n",
    "    # Assign column names\n",
    "    sheet_data.columns = column_names\n",
    "\n",
    "    # Drop rows where 'deviceID' is still invalid or starts with \"deviceID\"\n",
    "    sheet_data = sheet_data[sheet_data['deviceID'].notna()]\n",
    "    sheet_data = sheet_data[sheet_data['deviceID'] != \"deviceID\"]\n",
    "\n",
    "    # Append processed sheet\n",
    "    processed_sheets.append(sheet_data)\n",
    "\n",
    "# Concatenate all sheets into one DataFrame\n",
    "df = pd.concat(processed_sheets, ignore_index=True)\n",
    "\n",
    "# Drop rows where Lat or Log is 0\n",
    "df = df[(df['Lat'] != 0) & (df['Log'] != 0)]\n",
    "\n",
    "# Replace SOC_batt values below 0 with 0\n",
    "df.loc[df['SOC_batt'] < 0, 'SOC_batt'] = 0\n",
    "\n",
    "# Correct indexing column to start at 1 and increment sequentially\n",
    "df.reset_index(drop=True, inplace=True)\n",
    "df['index'] = df.index + 1\n",
    "\n",
    "# === Anonymize deviceIDs with random 3-letter uppercase strings ===\n",
    "unique_ids = df['deviceID'].unique()\n",
    "\n",
    "def generate_random_name(existing):\n",
    "    while True:\n",
    "        name = ''.join(random.choices(string.ascii_uppercase, k=3))\n",
    "        if name not in existing:\n",
    "            return name\n",
    "\n",
    "existing_names = set()\n",
    "id_map = {}\n",
    "for original_id in unique_ids:\n",
    "    short_name = generate_random_name(existing_names)\n",
    "    id_map[original_id] = short_name\n",
    "    existing_names.add(short_name)\n",
    "\n",
    "df['deviceID'] = df['deviceID'].map(id_map)\n",
    "\n",
    "# Save cleaned data back to Excel\n",
    "df.to_excel(output_path, index=False)\n",
    "\n",
    "# Print cleaned data preview\n",
    "print(\"Data cleaning completed. Saved to:\", output_path)\n",
    "print(df.head())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Spatiotemporal Binning and Stationary Period Detection**\n",
    "\n",
    "This enables **spatial binning**, **stationary period detection**, and **temporal filtering** for robust movement analysis.\n",
    "\n",
    "## **1. Timestamp Conversion**\n",
    "The Unix timestamp $ T_i $ is converted into a standard datetime format:\n",
    "\n",
    "$$\n",
    "T_i^{\\text{datetime}} = T_i^{\\text{unix}} \\times \\frac{1}{86400} + \\text{epoch}\n",
    "$$\n",
    "\n",
    "where:\n",
    "- $ T_i^{\\text{unix}} $ is the raw Unix timestamp in **seconds**,\n",
    "- $ 86400 $ seconds = **1 day**,\n",
    "- **Epoch** is the reference starting time (January 1, 1970).\n",
    "\n",
    "## **2. Ensuring Numeric Latitude and Longitude**\n",
    "We enforce that latitude ($ \\text{Lat} $) and longitude ($ \\text{Log} $) are real-valued:\n",
    "\n",
    "$$\n",
    "\\text{Lat}, \\text{Log} \\in \\mathbb{R}\n",
    "$$\n",
    "\n",
    "Non-numeric values are coerced to **NaN**.\n",
    "\n",
    "## **3. Discretization into a 40m Grid**\n",
    "### **3.1 Latitude Grid Resolution**\n",
    "Since the **Earth's meridional circumference** is approximately **40,030 km**, the degree-to-meter conversion near the equator is:\n",
    "\n",
    "$$\n",
    "1^\\circ \\approx 111,320 \\text{ meters}\n",
    "$$\n",
    "\n",
    "Thus, the spatial resolution of a **40m grid** in latitude is:\n",
    "\n",
    "$$\n",
    "\\Delta \\text{Lat} = \\frac{120}{111320}\n",
    "$$\n",
    "\n",
    "The **grid-aligned latitude** is computed as:\n",
    "\n",
    "$$\n",
    "\\text{Lat\\_Grid} = \\left\\lfloor \\frac{\\text{Lat}}{\\Delta \\text{Lat}} \\right\\rfloor \\times \\Delta \\text{Lat}\n",
    "$$\n",
    "\n",
    "### **3.2 Longitude Grid Resolution**\n",
    "Unlike latitude, **longitude spacing** varies with latitude due to Earth’s curvature. The **longitude degree-to-meter conversion** is:\n",
    "\n",
    "$$\n",
    "1^\\circ \\approx 111320 \\times \\cos(\\text{Lat})\n",
    "$$\n",
    "\n",
    "Thus, the **longitude resolution** at a given latitude is:\n",
    "\n",
    "$$\n",
    "\\Delta \\text{Log} = \\frac{40}{111320 \\cos(\\text{Lat})}\n",
    "$$\n",
    "\n",
    "The **grid-aligned longitude** is computed as:\n",
    "\n",
    "$$\n",
    "\\text{Log\\_Grid} = \\left\\lfloor \\frac{\\text{Log}}{\\Delta \\text{Log}} \\right\\rfloor \\times \\Delta \\text{Log}\n",
    "$$\n",
    "\n",
    "## **4. Sorting by Time and Device**\n",
    "To track movement **chronologically** for each vehicle, we sort:\n",
    "\n",
    "$$\n",
    "\\text{Sort}(df, \\text{by}=[\\text{deviceID}, \\text{Timestamp}])\n",
    "$$\n",
    "\n",
    "## **5. Identifying Stationary Periods**\n",
    "For each vehicle, we determine if it remained in the same grid cell over consecutive timestamps:\n",
    "\n",
    "$$\n",
    "\\text{Same\\_Grid}_i =\n",
    "\\begin{cases} \n",
    "1, & (\\text{Lat\\_Grid}_i = \\text{Lat\\_Grid}_{i-1}) \\land (\\text{Log\\_Grid}_i = \\text{Log\\_Grid}_{i-1}) \\\\\n",
    "0, & \\text{otherwise}\n",
    "\\end{cases}\n",
    "$$\n",
    "\n",
    "where:\n",
    "- $ \\text{Same\\_Grid}_i = 1 $ means no movement occurred.\n",
    "- $ \\text{Same\\_Grid}_i = 0 $ means movement occurred.\n",
    "\n",
    "## **6. Computing Time Spent in a Grid Cell**\n",
    "The time difference between consecutive records within the same grid is:\n",
    "\n",
    "$$\n",
    "\\Delta t_i = T_i - T_{i-1}\n",
    "$$\n",
    "\n",
    "The **total duration** a vehicle spends within a specific grid cell before moving is:\n",
    "\n",
    "$$\n",
    "\\text{Cumulative\\_Time}_{i} = \\sum_{k=1}^{i} \\Delta t_k\n",
    "$$\n",
    "\n",
    "where:\n",
    "- The summation continues **until movement occurs**.\n",
    "\n",
    "## **7. Assigning a Group ID to Each Stationary Period**\n",
    "A **unique group identifier** is assigned to each stationary period using a cumulative sum:\n",
    "\n",
    "$$\n",
    "\\text{Group}_i =\n",
    "\\sum_{j=1}^{i} (1 - \\text{Same\\_Grid}_j)\n",
    "$$\n",
    "\n",
    "Each transition into a **new grid cell** increments the group ID.\n",
    "\n",
    "## **8. Removing Prolonged Stationary Vehicles**\n",
    "Vehicles remaining in the **same grid for over x hours** (xxx seconds) are excluded:\n",
    "\n",
    "$$\n",
    "\\text{Remove } i \\text{ if } \\text{Cumulative\\_Time}_i \\geq xxx \\text{ sec}\n",
    "$$\n",
    "\n",
    "## **9. Cleanup**\n",
    "All intermediate columns used for calculations are dropped to optimize storage.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert Unix timestamp to datetime\n",
    "df['Timestamp'] = pd.to_datetime(df['Timestamp'], unit='s')\n",
    "\n",
    "# Ensure 'Lat' and 'Log' are numeric\n",
    "df['Lat'] = pd.to_numeric(df['Lat'], errors='coerce')\n",
    "df['Log'] = pd.to_numeric(df['Log'], errors='coerce')\n",
    "\n",
    "# Spatial Resolution: 40m grid\n",
    "grid_size=40\n",
    "lat_resolution = grid_size / 111320 \n",
    "df['Lat_Grid'] = (df['Lat'] // lat_resolution) * lat_resolution\n",
    "\n",
    "# Longitude resolution depends on latitude\n",
    "df['Lon_Resolution'] = grid_size / (111320 * np.cos(np.radians(df['Lat'])))\n",
    "df['Log_Grid'] = (df['Log'] // df['Lon_Resolution']) * df['Lon_Resolution']\n",
    "\n",
    "# Drop auxiliary column\n",
    "df = df.drop(columns=['Lon_Resolution'])\n",
    "\n",
    "# Step 1: Sort by deviceID and Timestamp\n",
    "df = df.sort_values(by=['deviceID', 'Timestamp'])\n",
    "\n",
    "# Step 2: Detect continuous stationary periods\n",
    "df['Prev_Lat_Grid'] = df.groupby('deviceID')['Lat_Grid'].shift(1)\n",
    "df['Prev_Log_Grid'] = df.groupby('deviceID')['Log_Grid'].shift(1)\n",
    "df['Prev_Timestamp'] = df.groupby('deviceID')['Timestamp'].shift(1)\n",
    "\n",
    "# Step 3: Identify whether the taxi has stayed in the same grid\n",
    "df['Same_Grid'] = (df['Lat_Grid'] == df['Prev_Lat_Grid']) & (df['Log_Grid'] == df['Prev_Log_Grid'])\n",
    "\n",
    "# Step 4: Compute time spent in the grid continuously\n",
    "df['Time_Diff'] = (df['Timestamp'] - df['Prev_Timestamp']).dt.total_seconds()\n",
    "\n",
    "# Step 5: Assign a group ID that resets when the taxi leaves a grid\n",
    "df['Group'] = (~df['Same_Grid']).cumsum()\n",
    "\n",
    "# Step 6: Compute total time spent in each visit to the grid\n",
    "df['Cumulative_Time'] = df.groupby(['deviceID', 'Lat_Grid', 'Log_Grid', 'Group'])['Time_Diff'].cumsum()\n",
    "\n",
    "# Ensure 'Cumulative_Time' is not NaN or negative (if any filtering was done previously)\n",
    "df_filtered = df[df['Cumulative_Time'] > 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.rcParams.update({\n",
    "    \"text.usetex\": False,  # True if you have LaTeX installed; otherwise keep False and use mathtext\n",
    "    \"font.size\": 14,\n",
    "    \"axes.titlesize\": 16,\n",
    "    \"axes.labelsize\": 16,\n",
    "    \"legend.fontsize\": 12,\n",
    "    \"xtick.labelsize\": 12,\n",
    "    \"ytick.labelsize\": 12,\n",
    "    \"font.family\": \"serif\"\n",
    "})\n",
    "\n",
    "fig=plt.figure(figsize=(18, 6), dpi=300)\n",
    "device_colors =[\n",
    "    \"#1f77b4\",  # muted blue\n",
    "    \"#ff7f0e\",  # orange\n",
    "    \"#2ca02c\",  # green\n",
    "    \"#d62728\",  # red\n",
    "    \"#9467bd\",  # purple\n",
    "]\n",
    "\n",
    "# Group by device and collect lists of capture intervals (Time_Diff)\n",
    "intervals_per_device = [\n",
    "    (device_id, group['Time_Diff'].dropna().values)\n",
    "    for device_id, group in df_filtered.groupby('deviceID')\n",
    "]\n",
    "\n",
    "for i, (device_id, intervals) in enumerate(intervals_per_device):\n",
    "    sorted_intervals = np.sort(intervals)\n",
    "    cdf = np.arange(1, len(sorted_intervals) + 1) / len(sorted_intervals)\n",
    "\n",
    "    # --- Histogram subplot with log-spaced bins ---\n",
    "    plt.subplot(1, 2, 1)\n",
    "    min_val = max(np.min(intervals), 1e-1)  # avoid zero or negative\n",
    "    max_val = np.max(intervals)\n",
    "    log_bins = np.logspace(np.log10(min_val), np.log10(max_val), 700)\n",
    "\n",
    "    plt.hist(intervals, bins=log_bins, alpha=0.5, color=device_colors[i], label=f'Device {device_id}', histtype='stepfilled')\n",
    "    plt.xscale('linear')\n",
    "    plt.xlabel('Capture Interval (seconds)')\n",
    "    plt.ylabel('Count')\n",
    "    plt.title('Histogram of Capture Intervals')\n",
    "    plt.grid(False, which='both')\n",
    "    plt.xlim(20, 40) \n",
    "    plt.legend(loc='lower right')\n",
    "\n",
    "    # --- CDF subplot ---\n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.plot(sorted_intervals, cdf, lw=1.5, color=device_colors[i], label=f'Device {device_id}')\n",
    "    plt.xscale('log')\n",
    "    plt.xlabel('Capture Interval (seconds, log scale)')\n",
    "    plt.ylabel('Empirical CDF')\n",
    "    plt.title('CDF of Capture Intervals')\n",
    "    plt.grid(False)\n",
    "    plt.legend(loc='lower right')\n",
    "\n",
    "output_path = \"capture_intervals.pdf\"\n",
    "plt.savefig(output_path, format='pdf', bbox_inches='tight')\n",
    "print(f\"✅ Plot saved in working directory: {os.path.abspath(output_path)}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Identifying Optimal Threshold for Cumulative Time in Grids**\n",
    "\n",
    "## **1. Calculate Empirical Cumulative Distribution Function (ECDF)**\n",
    "The ECDF is calculated to provide insight into the distribution of cumulative time spent in each grid. The sorted cumulative times and corresponding ECDF values are computed as:\n",
    "\n",
    "$\n",
    "\\text{sorted\\_times} = \\text{np.sort}(\\text{cumulative\\_time\\_hours})\n",
    "$\n",
    "\n",
    "$\n",
    "\\text{ecdf\\_values} = \\frac{i}{N}, \\quad i = 1, 2, \\ldots, N\n",
    "$\n",
    "\n",
    "Where $N$ is the total number of data points.\n",
    "\n",
    "## **2. Optimal Threshold Detection Using Kneedle Algorithm**\n",
    "The **Kneedle Algorithm** is applied to find the threshold point where the slope of the ECDF curve changes most significantly. This point is considered the optimal threshold that separates short-term and long-term grid occupations.\n",
    "\n",
    "$\n",
    "\\text{kneedle} = \\text{KneeLocator}(\\text{sorted\\_times}, \\text{ecdf\\_values}, S=1.0, \\text{curve}=\"concave\", \\text{direction}=\"increasing\")\n",
    "$\n",
    "\n",
    "The detected knee point (optimal threshold) is converted back to seconds:\n",
    "\n",
    "$\n",
    "\\text{optimal\\_threshold\\_seconds} = \\text{optimal\\_threshold\\_hours} \\times 3600\n",
    "$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert Cumulative_Time to hours for easier interpretation\n",
    "cumulative_time_hours = df_filtered['Cumulative_Time'] / 3600\n",
    "\n",
    "# Calculate ECDF values for analysis\n",
    "sorted_times = np.sort(cumulative_time_hours)\n",
    "ecdf_values = np.arange(1, len(sorted_times) + 1) / len(sorted_times)\n",
    "\n",
    "# Find the optimal threshold using the Kneedle algorithm\n",
    "kneedle = KneeLocator(sorted_times, ecdf_values, S=1.0, curve=\"concave\", direction=\"increasing\")\n",
    "optimal_threshold_hours = kneedle.knee\n",
    "optimal_threshold_seconds = optimal_threshold_hours * 3600\n",
    "\n",
    "# Plot 1: Histogram with Optimal Threshold\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.hist(cumulative_time_hours, bins=100, color='skyblue', label='Cumulative Time Distribution')\n",
    "plt.axvline(optimal_threshold_hours, color='red', linestyle='--', label=f'Optimal Threshold = {optimal_threshold_hours:.2f} hours')\n",
    "plt.title('Distribution of Cumulative Time Spent in Each Grid (With Optimal Threshold)')\n",
    "plt.xlabel('Cumulative Time Spent in Grid (hours)')\n",
    "plt.ylabel('Frequency')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()\n",
    "\n",
    "# Plot 2: ECDF with Optimal Threshold\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.ecdfplot(cumulative_time_hours, label='ECDF of Cumulative Time')\n",
    "plt.axvline(optimal_threshold_hours, color='red', linestyle='--', label=f'Optimal Threshold = {optimal_threshold_hours:.2f} hours')\n",
    "plt.title('ECDF of Cumulative Time Spent in Each Grid (With Optimal Threshold)')\n",
    "plt.xlabel('Cumulative Time Spent in Grid (hours)')\n",
    "plt.ylabel('Proportion of Data Points')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()\n",
    "\n",
    "# Display the optimal threshold\n",
    "print(f\"Optimal Threshold Found: {optimal_threshold_hours:.2f} hours ({optimal_threshold_seconds:.0f} seconds)\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "original_entry_count = len(df)\n",
    "\n",
    "# Step 7: Remove vehicles that stayed continuously in the same grid for more than optimal_threshold_seconds\n",
    "df = df[~(df['Cumulative_Time'] >= optimal_threshold_seconds)]\n",
    "\n",
    "removed_entries = original_entry_count - len(df)\n",
    "\n",
    "# Drop helper columns\n",
    "df = df.drop(columns=['Prev_Lat_Grid', 'Prev_Log_Grid', 'Prev_Timestamp', 'Same_Grid', 'Time_Diff', 'Group', 'Cumulative_Time'])\n",
    "\n",
    "# Print the number of removed entries\n",
    "print(f\"Number of entries removed due to vehicles staying continuously in the same grid for more than {optimal_threshold_hours} hours: {removed_entries}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Spatial Grid Aggregation\n",
    "\n",
    "This script generates a **40m x 40m geospatial grid** and counts the number of data points within each grid cell:\n",
    "\n",
    "1. **Dynamic Boundary Definition**: \n",
    "   - Extracts min/max latitude and longitude from the dataset.\n",
    "2. **Grid Construction**:\n",
    "   - Defines **40m resolution** for latitude and dynamically calculates longitude resolution.\n",
    "   - Iterates over the spatial extent to generate **polygonal grid cells**.\n",
    "3. **GeoDataFrame Creation**:\n",
    "   - Converts the grid into a `GeoDataFrame` (`grid_gdf`).\n",
    "   - Converts data points into a `GeoDataFrame` (`df_gdf`).\n",
    "4. **Spatial Aggregation**:\n",
    "   - Checks which points fall within each grid cell.\n",
    "   - Increments the count of data points within corresponding grid polygons."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dynamically define the bounds from the DataFrame\n",
    "min_lat = df['Lat'].min()\n",
    "max_lat = df['Lat'].max()\n",
    "min_lon = df['Log'].min()\n",
    "max_lon = df['Log'].max()\n",
    "\n",
    "lon_resolution_at_lat = lambda lat: grid_size / (111320 * np.cos(np.radians(lat)))\n",
    "\n",
    "# Generate grid of polygons\n",
    "grid = []\n",
    "lat = min_lat\n",
    "while lat < max_lat:\n",
    "    lon = min_lon\n",
    "    while lon < max_lon:\n",
    "        lon_res = lon_resolution_at_lat(lat)\n",
    "        grid.append(Polygon([\n",
    "            (lon, lat),\n",
    "            (lon + lon_res, lat),\n",
    "            (lon + lon_res, lat + lat_resolution),\n",
    "            (lon, lat + lat_resolution)\n",
    "        ]))\n",
    "        lon += lon_res\n",
    "    lat += lat_resolution\n",
    "\n",
    "# Create an empty GeoDataFrame for the grid\n",
    "grid_gdf = gpd.GeoDataFrame({'geometry': grid, 'Count': 0}, crs=\"EPSG:4326\")\n",
    "\n",
    "# Create a GeoDataFrame for the points in df\n",
    "df_gdf = gpd.GeoDataFrame(df, geometry=gpd.points_from_xy(df['Log'], df['Lat']), crs=\"EPSG:4326\")\n",
    "\n",
    "# Perform spatial join (vectorized operation)\n",
    "joined = gpd.sjoin(df_gdf, grid_gdf, how=\"left\", predicate=\"within\")\n",
    "\n",
    "# Count points per grid cell\n",
    "grid_gdf['Count'] = joined.groupby(joined.index_right).size()\n",
    "\n",
    "# Fill NaN with 0 for empty grid cells\n",
    "grid_gdf['Count'].fillna(0, inplace=True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **SOC Depletion Modeling with Sensor OFF Strategies**\n",
    "\n",
    "## **Context and Objective**\n",
    "This implementation models the **State of Charge (SOC) depletion** of sensor-equipped vehicles while applying **dynamic sensor OFF strategies** based on pre-defined **time thresholds**. The goal is to assess the impact of temporarily turning **OFF** sensors to conserve energy, tracking **SOC savings**, and evaluating the difference in battery depletion rates under different sensor control policies.\n",
    "\n",
    "The core of this method involves:\n",
    "1. **Tracking the energy savings** from sensor OFF periods.\n",
    "2. **Modeling sensor activation behavior** based on pre-defined **time thresholds**.\n",
    "3. **Computing the modified SOC depletion** with stored energy savings.\n",
    "4. **Comparing the baseline SOC depletion with sensor control policies.**\n",
    "\n",
    "## **1. Data Preparation and Sorting**\n",
    "The dataset is first copied and indexed sequentially to ensure **chronological order**:\n",
    "\n",
    "$$\n",
    "\\text{Sort}(df, \\text{by}=[\\text{Timestamp}])\n",
    "$$\n",
    "\n",
    "This sorting is essential for maintaining **temporal consistency**, ensuring that energy tracking occurs in the correct **sequential order**.\n",
    "\n",
    "## **2. Definition of Sensor OFF Strategy**\n",
    "A sensor OFF threshold $ T_{\\text{threshold}} $ is defined, which determines how long a **grid cell** can remain **inactive** before reactivation is allowed. Given:\n",
    "\n",
    "$$\n",
    "T_{\\text{threshold}} = \\{ 12 \\text{ sec} \\}\n",
    "$$\n",
    "\n",
    "a sensor remains **OFF** if a vehicle has recently occupied the same **grid cell** within the threshold:\n",
    "\n",
    "$$\n",
    "\\text{Sensor\\_ON}_i =\n",
    "\\begin{cases} \n",
    "0, & \\text{if } (t_i - t_{\\text{last sensed}}) < T_{\\text{threshold}} \\\\\n",
    "1, & \\text{otherwise}\n",
    "\\end{cases}\n",
    "$$\n",
    "\n",
    "where:\n",
    "- $ t_i $ is the **current timestamp**.\n",
    "- $ t_{\\text{last sensed}} $ is the timestamp of the **last recorded sensor activation**.\n",
    "- $ T_{\\text{threshold}} $ is the **predefined time limit** for keeping the sensor OFF.\n",
    "\n",
    "## **3. Energy Storage Mechanism**\n",
    "During **sensor OFF periods**, the energy that would have been consumed is tracked using a **stored energy accumulation model**. The change in **SOC depletion rate** due to the sensor OFF mechanism is computed as:\n",
    "\n",
    "$$\n",
    "\\Delta \\text{SOC}_i = \\max(0, \\text{SOC}_{i-1} - \\text{SOC}_i)\n",
    "$$\n",
    "\n",
    "where:\n",
    "- $ \\text{SOC}_i $ is the **current SOC measurement**.\n",
    "- $ \\text{SOC}_{i-1} $ is the **previous SOC measurement**.\n",
    "\n",
    "Stored energy savings accumulate over time:\n",
    "\n",
    "$$\n",
    "\\text{Energy\\_Saved}_i = \\sum_{j=1}^{i} \\Delta \\text{SOC}_j, \\quad \\text{if Sensor\\_ON} = 0\n",
    "$$\n",
    "\n",
    "where:\n",
    "- $ \\text{Energy\\_Saved}_i $ represents the **total accumulated SOC savings**.\n",
    "- The accumulation occurs **only when the sensor is OFF**.\n",
    "\n",
    "## **4. Dynamic SOC Update with Energy Savings**\n",
    "The SOC for a given time step is **recomputed** using the stored energy:\n",
    "\n",
    "$$\n",
    "\\text{SOC\\_batt}_i' = \\text{SOC\\_batt}_i + \\text{Energy\\_Saved}_i\n",
    "$$\n",
    "\n",
    "where:\n",
    "- $ \\text{SOC\\_batt}_i' $ is the **corrected SOC value** incorporating energy savings.\n",
    "- $ \\text{SOC\\_batt}_i $ is the **original SOC value**.\n",
    "- $ \\text{Energy\\_Saved}_i $ represents the **cumulative stored SOC improvements**.\n",
    "\n",
    "To ensure **SOC does not exceed 100%**, the final corrected SOC values are clipped:\n",
    "\n",
    "$$\n",
    "\\text{SOC\\_batt}_i' = \\min(100, \\text{SOC\\_batt}_i + \\text{Energy\\_Saved}_i)\n",
    "$$\n",
    "\n",
    "## **5. Baseline and Threshold Comparisons**\n",
    "To assess the impact of sensor OFF strategies, SOC depletion is **compared across different thresholds**. The average daily SOC depletion per device is computed as:\n",
    "\n",
    "$$\n",
    "\\text{SOC\\_depletion} = \\frac{1}{N} \\sum_{i=1}^{N} \\text{SOC\\_batt}_i'\n",
    "$$\n",
    "\n",
    "where:\n",
    "- $ N $ is the number of **time steps in a day**.\n",
    "- $ \\text{SOC\\_batt}_i' $ represents the **SOC levels incorporating sensor OFF savings**.\n",
    "\n",
    "The **baseline depletion rate** without any sensor OFF strategy is also computed:\n",
    "\n",
    "$$\n",
    "\\text{Baseline\\_SOC} = \\frac{1}{N} \\sum_{i=1}^{N} \\text{SOC\\_batt}_i\n",
    "$$\n",
    "\n",
    "where no energy savings are incorporated.\n",
    "\n",
    "## **6. Expected Outcomes and Justification**\n",
    "This method enables:\n",
    "- **Quantification of energy savings** from sensor OFF strategies.\n",
    "- **Comparison of SOC depletion trends** across different sensor OFF thresholds.\n",
    "- **Validation of sensor optimization policies** to maximize battery lifespan.\n",
    "\n",
    "By implementing **grid-based sensing optimization**, the system **reduces unnecessary sensor activations**, ensuring **longer sensor endurance** while maintaining **effective data collection**.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['Date'] = df['Timestamp'].dt.date  # Extract the date\n",
    "df_temp = df.copy().reset_index(drop=True)  # Ensure indices are sequential\n",
    "df_temp=df_temp.sort_values(by=['Timestamp']).reset_index(drop=True) \n",
    "\n",
    "# Define different time thresholds to compare\n",
    "time_thresholds = {\n",
    "    # \"3 sec\": 3,\n",
    "    \"360 sec\": 360\n",
    "}\n",
    "\n",
    "# Create a dictionary to store SOC and sensor states for each threshold\n",
    "soc_depletion_results = {}\n",
    "\n",
    "# Iterate over different time thresholds\n",
    "for label, TIME_THRESHOLD in time_thresholds.items():\n",
    "\n",
    "    # Track last sensed timestamp, and stored energy during OFF periods\n",
    "    last_sensed_time = {}\n",
    "    stored_energy = {}\n",
    "\n",
    "    # Previous date\n",
    "    prev_date=None\n",
    "    \n",
    "    for i in range(len(df_temp)):\n",
    "        row = df_temp.iloc[i]\n",
    "        grid_key = (row['Lat_Grid'], row['Log_Grid'])  # Unique grid identifier\n",
    "        current_time = row['Timestamp']\n",
    "        current_date = row['Date']\n",
    "        device= row['deviceID']\n",
    "\n",
    "        # Initialise inter-row differences when OFF\n",
    "        d_diff_prev=0 \n",
    "        \n",
    "        # Reset stored energy at the start of a new day\n",
    "        if prev_date is not None and current_date != prev_date:\n",
    "            stored_energy={}  # Reset stored energy for all grid cells\n",
    "            df_temp.loc[i:, f'Energy_Saved_{label}'] = 0  # Reset energy savings for the new day\n",
    "            print(f\"[RESET] Reset stored energy for new day: {current_date}\")\n",
    "\n",
    "        prev_date = current_date  # Update previous date tracker\n",
    "\n",
    "        if df_temp.loc[i, 'SOC_batt']>99:\n",
    "            stored_energy[grid_key]=0\n",
    "            df_temp.loc[i:, f'Energy_Saved_{label}']=0\n",
    "\n",
    "        # If the grid was sensed recently (within the threshold), turn OFF the sensor\n",
    "        if grid_key in last_sensed_time and (current_time - last_sensed_time[grid_key]).total_seconds() < TIME_THRESHOLD:\n",
    "            df_temp.at[i, f'Sensor_ON_{label}'] = False   \n",
    "\n",
    "            # Accumulate stored energy\n",
    "            if i > 0 and pd.notna(df_temp.iloc[i - 1]['SOC_batt']) and pd.notna(row['SOC_batt']):\n",
    "            \n",
    "                # Find the last preceding row for this device\n",
    "                if device == df_temp.iloc[i-1]['deviceID']:\n",
    "                    d_diff = max(0, df_temp.iloc[i - 1]['SOC_batt'] - row['SOC_batt']) #current inter-row difference\n",
    "                    diff = max(d_diff_prev, d_diff)\n",
    "                    stored_energy[grid_key] = df_temp.loc[i-1, f'Energy_Saved_{label}']\n",
    "                    stored_energy[grid_key] += diff\n",
    "                    \n",
    "                    if diff != 0: \n",
    "                        df_temp.loc[i:, f'Energy_Saved_{label}'] = stored_energy[grid_key]\n",
    "                    else:\n",
    "                        df_temp.loc[i:, f'Energy_Saved_{label}'] = df_temp.loc[i-1, f'Energy_Saved_{label}']\n",
    "\n",
    "                    d_diff_prev=diff\n",
    "                    print(f\"[OFF]: Accumulated {diff:.2f}% for device {device}. Total stored: {stored_energy[grid_key]:.2f}%\")\n",
    "\n",
    "                else:\n",
    "                    d_diff = max(0, df_temp.loc[df_temp.deviceID == device, :]['SOC_batt'].iloc[-1] - row['SOC_batt']) #current inter-row difference\n",
    "                    diff = max(d_diff_prev, d_diff)\n",
    "                    stored_energy[grid_key] = df_temp.loc[df_temp.deviceID == device, :][f'Energy_Saved_{label}'].iloc[-1]\n",
    "                    stored_energy[grid_key] += diff\n",
    "                    \n",
    "                    if diff != 0: \n",
    "                        df_temp.loc[i:, f'Energy_Saved_{label}'] = stored_energy[grid_key]\n",
    "                    else:\n",
    "                        df_temp.loc[i:, f'Energy_Saved_{label}'] = df_temp.loc[df_temp.deviceID == device, :][f'Energy_Saved_{label}'].iloc[-1]\n",
    "\n",
    "                    d_diff_prev=diff\n",
    "                    print(f\"CHANGE [OFF]: Accumulated {diff:.2f}% for device {device}. Total stored: {stored_energy[grid_key]:.2f}%\")\n",
    "\n",
    "        else:\n",
    "            # Update last sensed time when the sensor turns ON\n",
    "            last_sensed_time[grid_key] = current_time\n",
    "            df_temp.at[i, f'Sensor_ON_{label}'] = True\n",
    "            d_diff_prev=0\n",
    "\n",
    "            if device == df_temp.iloc[i-1]['deviceID']:\n",
    "\n",
    "                # Ensure stored_energy is initialized per grid cell without overwriting previous values\n",
    "                if grid_key not in stored_energy:\n",
    "                    if i > 0:\n",
    "                        #Carry forward the stored energy from the last known row\n",
    "                        stored_energy[grid_key] = df_temp.loc[i-1, f'Energy_Saved_{label}']\n",
    "                        df_temp.loc[i:, f'Energy_Saved_{label}'] = stored_energy[grid_key]\n",
    "                    elif i == 0:\n",
    "                        stored_energy[grid_key] = 0  # First iteration, no prior energy \n",
    "                        df_temp.loc[i:, f'Energy_Saved_{label}'] = stored_energy[grid_key]  \n",
    "                print(f\"[ON]: device {device}. Total stored: {stored_energy[grid_key]:.2f}%\")\n",
    "\n",
    "            else:\n",
    "                 # Ensure stored_energy is initialized per grid cell without overwriting previous values\n",
    "                if grid_key not in stored_energy:\n",
    "                    if i > 0:\n",
    "                        #Carry forward the stored energy from the last known row\n",
    "                        stored_energy[grid_key] = df_temp.loc[df_temp.deviceID == device, :][f'Energy_Saved_{label}'].iloc[-1]\n",
    "                        df_temp.loc[i:, f'Energy_Saved_{label}'] = stored_energy[grid_key]\n",
    "                    elif i == 0:\n",
    "                        stored_energy[grid_key] = 0  # First iteration, no prior energy \n",
    "                        df_temp.loc[i:, f'Energy_Saved_{label}'] = stored_energy[grid_key]                 \n",
    "                \n",
    "                print(f\"CHANGE [ON]: device {device}. Total stored: {stored_energy[grid_key]:.2f}%\")\n",
    "\n",
    "    \n",
    "    # Compute new SOC_batt with savings\n",
    "    df_temp[f'SOC_batt_{label}'] = df_temp['SOC_batt'] + df_temp[f'Energy_Saved_{label}']\n",
    "    df_temp[f'SOC_batt_{label}'] = df_temp[f'SOC_batt_{label}'].clip(upper=100)\n",
    "\n",
    "    # Compute SOC depletion for this threshold\n",
    "    daily_soc = df_temp.groupby(['Date', 'deviceID'])[f'SOC_batt_{label}'].mean()\n",
    "    soc_depletion_results[label] = daily_soc\n",
    "\n",
    "\n",
    "# Baseline: Compute SOC depletion without constraints\n",
    "soc_depletion_results[\"Baseline\"] = df_temp.groupby(['Date', 'deviceID'])['SOC_batt'].mean()\n",
    "\n",
    "# Convert results to a DataFrame for plotting\n",
    "soc_depletion_df = pd.DataFrame(soc_depletion_results)\n",
    "\n",
    "# Save the updated dataset with sensor states and energy savings for each threshold\n",
    "output_path = \"/workspace/data/updated_SOC_batt_with_energy_savings.xlsx\"\n",
    "df_temp.to_excel(output_path, index=False)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "import os\n",
    "\n",
    "# Define line styles and transparency levels for each threshold\n",
    "line_styles = {\n",
    "    \"Baseline\": \"--\",\n",
    "    \"360 sec\": \"-\"\n",
    "}\n",
    "\n",
    "alpha_values = {\n",
    "    \"Baseline\": 0.5,\n",
    "    \"360 sec\": 1\n",
    "}\n",
    "\n",
    "# Predefined colors for devices\n",
    "predefined_colors = [\n",
    "    \"#1f77b4\",  # muted blue\n",
    "    \"#ff7f0e\",  # orange\n",
    "    \"#2ca02c\",  # green\n",
    "    \"#d62728\",  # red\n",
    "    \"#9467bd\",  # purple\n",
    "]\n",
    "\n",
    "# Build set of all device IDs\n",
    "device_ids = set()\n",
    "for soc_series in soc_depletion_results.values():\n",
    "    device_ids.update(soc_series.index.get_level_values('deviceID').unique())\n",
    "\n",
    "# Create color map using consistent color order\n",
    "device_ids = sorted(device_ids)\n",
    "color_map = {device_id: predefined_colors[i % len(predefined_colors)] for i, device_id in enumerate(device_ids)}\n",
    "\n",
    "# Begin plot\n",
    "plt.figure(figsize=(14, 8), dpi=300)\n",
    "\n",
    "legend_entries = []\n",
    "\n",
    "# Plot each threshold's data per device\n",
    "for label, soc_series in soc_depletion_results.items():  # soc_series is MultiIndexed\n",
    "    for device_id in soc_series.index.get_level_values('deviceID').unique():\n",
    "        device_data = soc_series[soc_series.index.get_level_values('deviceID') == device_id]\n",
    "        line, = plt.plot(\n",
    "            device_data.index.get_level_values('Date'),\n",
    "            device_data.values,\n",
    "            linestyle=line_styles[label],\n",
    "            lw=1.5,\n",
    "            color=color_map[device_id],\n",
    "            alpha=alpha_values[label],\n",
    "            label=f\"Device {device_id} - {label}\"\n",
    "        )\n",
    "        legend_entries.append((f\"Device {device_id} - {label}\", line))\n",
    "\n",
    "# Sort legend entries by device color order then by threshold label\n",
    "device_order = list(color_map.keys())\n",
    "\n",
    "def extract_key(label):\n",
    "    parts = label.split(\" - \")\n",
    "    device = parts[0].replace(\"Device \", \"\")\n",
    "    threshold = parts[1] if len(parts) > 1 else \"\"\n",
    "    return (device_order.index(device), threshold)\n",
    "\n",
    "legend_entries = sorted(legend_entries, key=lambda x: extract_key(x[0]))\n",
    "sorted_labels, sorted_handles = zip(*[(label, handle) for label, handle in legend_entries])\n",
    "\n",
    "# Apply axis labels and title\n",
    "plt.xlabel('Date')\n",
    "plt.ylabel('SOC (%)')\n",
    "plt.title('SOC Depletion Across Devices')\n",
    "plt.xticks(rotation=45)\n",
    "plt.grid(False)\n",
    "\n",
    "# Plot sorted legend\n",
    "plt.legend(sorted_handles, sorted_labels, loc='lower right', fontsize=9)\n",
    "\n",
    "# Final layout and export\n",
    "plt.tight_layout()\n",
    "output_path = \"socdep.pdf\"\n",
    "plt.savefig(output_path, format='pdf', bbox_inches='tight')\n",
    "print(f\"Plot saved in working directory: {os.path.abspath(output_path)}\")\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Baseline vs. Adaptive Gamma Scaling for Grid Analysis\n",
    "\n",
    "### Baseline Calculation (Without Log Transformation)\n",
    "1. **Compute Gamma Scaling Parameters:**\n",
    "   The baseline gamma scaling factor is computed using the mean ($ \\mu $) and standard deviation ($ \\sigma $) of the counts in each grid cell. The **Coefficient of Variation (CV)** is defined as:\n",
    "\n",
    "   $\n",
    "   CV = \\frac{\\sigma}{\\mu}\n",
    "   $\n",
    "\n",
    "   The gamma scaling factor is then calculated as:\n",
    "\n",
    "   $\n",
    "   \\text{baseline\\_gamma} = \\frac{1}{1 + CV}\n",
    "   $\n",
    "\n",
    "2. **Apply Gamma Scaling:**\n",
    "   To scale the counts, the following transformation is applied:\n",
    "\n",
    "   $\n",
    "   \\text{Scaled\\_Count\\_Baseline} = (\\text{Count} + 1)^{\\text{baseline\\_gamma}}\n",
    "   $\n",
    "\n",
    "3. **Normalize Counts:**\n",
    "   The scaled counts are normalized to facilitate visual comparison between different grid cells.\n",
    "\n",
    "---\n",
    "\n",
    "### Adaptive Calculation (Sensor-On Data)\n",
    "1. **Filtering Sensor-On Data:**  \n",
    "   Only the rows where the sensor is active are considered for the analysis.\n",
    "\n",
    "2. **Perform Spatial Join:**  \n",
    "   The filtered data points are spatially joined with the adaptive grid cells to identify which grid cell each point belongs to.\n",
    "\n",
    "3. **Count Calculation:**  \n",
    "   The number of points per grid cell is aggregated using:\n",
    "\n",
    "   $\n",
    "   \\text{Count} = \\sum_{i=1}^{N} \\text{Point}_i\n",
    "   $\n",
    "\n",
    "4. **Apply Gamma Scaling:**  \n",
    "   Similar to the baseline calculation, the adaptive scaling factor is computed as:\n",
    "\n",
    "   $\n",
    "   \\text{adaptive\\_gamma} = \\frac{1}{1 + CV_{\\text{adaptive}}}\n",
    "   $\n",
    "\n",
    "   The scaled counts are then calculated as:\n",
    "\n",
    "   $\n",
    "   \\text{Scaled\\_Count\\_Adaptive} = (\\text{Count} + 1)^{\\text{adaptive\\_gamma}}\n",
    "   $\n",
    "\n",
    "5. **Normalize Counts:**  \n",
    "   The adaptive scaled counts are normalized in a similar fashion to the baseline counts.\n",
    "\n",
    "---\n",
    "\n",
    "### Common Scaling for Comparison\n",
    "1. **Determine Global Minimum and Maximum:**  \n",
    "   The global minimum and maximum values are computed for both baseline and adaptive scaled counts to ensure consistent scaling.\n",
    "\n",
    "2. **Sigmoid Scaling for Visualization:**  \n",
    "   To enhance contrast for visualization, a sigmoid function is applied to the normalized counts:\n",
    "\n",
    "   $\n",
    "   \\text{Visual} = \\frac{1}{1 + \\exp\\left(-k \\left(\\text{Normalized\\_Scaled} - 0.5\\right)\\right)}\n",
    "   $\n",
    "\n",
    "   Where:\n",
    "   - $ k $ is a tunable parameter that controls the sharpness of the sigmoid curve.\n",
    "   - $\\text{Normalized\\_Scaled}$ represents the normalized count values scaled to a range of [0, 1].\n",
    "\n",
    "---\n",
    "\n",
    "### Common $ v_{min} $ Calculation\n",
    "The common minimum value for visualization is determined by taking the minimum of the baseline and adaptive visual representations.\n",
    "\n",
    "---\n",
    "\n",
    "### Common $ v_{max} $ Calculation (Using Kneedle Algorithm)\n",
    "1. **Quantile Range Creation:**  \n",
    "   A range of quantile values ($ q $) from 0.90 to 1.0 is generated to capture the upper tail of the distribution.\n",
    "\n",
    "2. **CV Difference Calculation:**  \n",
    "   The coefficient of variation ($CV$) is calculated at each quantile cutoff for both baseline and adaptive visualizations. The difference between the two is then recorded as:\n",
    "\n",
    "   $\n",
    "   \\text{CV Difference} = \\left| \\text{CV}_{\\text{Baseline}} - \\text{CV}_{\\text{Adaptive}} \\right|\n",
    "   $\n",
    "\n",
    "3. **Filtering and Kneedle Detection:**  \n",
    "   Initial flat portions of the curve are removed using a threshold filter to eliminate insignificant changes.  \n",
    "   The **Kneedle Algorithm** is applied to detect the optimal quantile where the difference in CVs is most pronounced.\n",
    "\n",
    "4. **Determine Common $ v_{max} $:**  \n",
    "   The common maximum value is determined by taking the minimum of the baseline and adaptive quantiles at the optimal quantile level:\n",
    "\n",
    "   $\n",
    "   v_{\\text{max}} = \\min\\left( \\text{Baseline Quantile}, \\text{Adaptive Quantile} \\right)\n",
    "   $\n",
    "\n",
    "---\n",
    "\n",
    "### Visualization and Results Display\n",
    "The calculated values of $ v_{\\text{min}} $ and $ v_{\\text{max}} $ are applied for consistent and enhanced visualization of both baseline and adaptive counts. The optimal quantile provides a well-calibrated view of the difference between baseline and adaptive approaches.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make copies to avoid overwriting\n",
    "grid_gdf_baseline = grid_gdf.copy()\n",
    "grid_gdf_adaptive = grid_gdf.copy()\n",
    "\n",
    "### BASELINE\n",
    "# Compute Gamma Scaling on Raw Counts (Without Log Transformation)\n",
    "baseline_counts = df_temp.groupby(['Lat_Grid', 'Log_Grid']).size()\n",
    "mean_count_baseline = grid_gdf_baseline['Count'].mean()\n",
    "std_count_baseline = grid_gdf_baseline['Count'].std()\n",
    "CV_baseline = std_count_baseline / mean_count_baseline if mean_count_baseline > 0 else 1\n",
    "baseline_gamma = 1 / (1 + CV_baseline)\n",
    "\n",
    "# Apply gamma scaling directly on raw counts\n",
    "grid_gdf_baseline['Scaled_Count_Baseline'] = (grid_gdf_baseline['Count'] + 1) ** baseline_gamma\n",
    "\n",
    "# Normalize the Scaled Counts for Better Visual Comparison\n",
    "scaled_min_baseline = grid_gdf_baseline['Scaled_Count_Baseline'].min()\n",
    "scaled_max_baseline = grid_gdf_baseline['Scaled_Count_Baseline'].max()\n",
    "\n",
    "### ADAPTIVE\n",
    "# Define the label used in code\n",
    "label = \"360 sec\"\n",
    "\n",
    "# Filter the dataset to include only rows where the sensor is ON\n",
    "adaptive_df = df_temp[df_temp[f'Sensor_ON_{label}'] == True]\n",
    "\n",
    "# Create a GeoDataFrame for the points in df\n",
    "adaptive_df_gdf = gpd.GeoDataFrame(adaptive_df, geometry=gpd.points_from_xy(adaptive_df['Log'], adaptive_df['Lat']), crs=\"EPSG:4326\")\n",
    "\n",
    "# Perform spatial join for Adaptive case\n",
    "joined = gpd.sjoin(adaptive_df_gdf, grid_gdf_adaptive, how=\"left\", predicate=\"within\")\n",
    "grid_gdf_adaptive['Count'] = joined.groupby(joined.index_right).size()\n",
    "grid_gdf_adaptive['Count'].fillna(0, inplace=True)\n",
    "\n",
    "# Apply Gamma Scaling on Raw Counts (Without Log Transformation)\n",
    "adaptive_counts = adaptive_df.groupby(['Lat_Grid', 'Log_Grid']).size()\n",
    "mean_count_adaptive = grid_gdf_adaptive['Count'].mean()\n",
    "std_count_adaptive = grid_gdf_adaptive['Count'].std()\n",
    "CV_adaptive = std_count_adaptive / mean_count_adaptive if mean_count_adaptive > 0 else 1\n",
    "adaptive_gamma = 1 / (1 + CV_adaptive)\n",
    "\n",
    "# Apply gamma scaling directly on raw counts\n",
    "grid_gdf_adaptive['Scaled_Count_Adaptive'] = (grid_gdf_adaptive['Count'] + 1) ** adaptive_gamma\n",
    "\n",
    "# Normalize the Scaled Counts for Better Visual Comparison\n",
    "scaled_min_adaptive = grid_gdf_adaptive['Scaled_Count_Adaptive'].min()\n",
    "scaled_max_adaptive = grid_gdf_adaptive['Scaled_Count_Adaptive'].max()\n",
    "\n",
    "# Global min and max for both datasets\n",
    "global_min = min(scaled_min_baseline, scaled_min_adaptive)\n",
    "global_max = max(scaled_max_baseline, scaled_max_adaptive)\n",
    "print(f\"Global Min: {global_min}, Global Max: {global_max}\")\n",
    "\n",
    "# Apply Sigmoid Scaling ONLY for Visualization (Baseline)\n",
    "grid_gdf_baseline['Normalized_Scaled_Baseline'] = (grid_gdf_baseline['Scaled_Count_Baseline'] - global_min) / (global_max - global_min)\n",
    "k = 10  # Adjust this value for better contrast enhancement\n",
    "grid_gdf_baseline['Visual_Baseline'] = 1 / (1 + np.exp(-k * (grid_gdf_baseline['Normalized_Scaled_Baseline'] - 0.5)))\n",
    "\n",
    "# Apply Sigmoid Scaling ONLY for Visualization (Adaptive)\n",
    "grid_gdf_adaptive['Normalized_Scaled_Adaptive'] = (grid_gdf_adaptive['Scaled_Count_Adaptive'] - global_min) / (global_max - global_min)\n",
    "grid_gdf_adaptive['Visual_Adaptive'] = 1 / (1 + np.exp(-k * (grid_gdf_adaptive['Normalized_Scaled_Adaptive'] - 0.5)))\n",
    "\n",
    "\n",
    "### COMMON VMIN\n",
    "# Compute the common vmin for both Baseline and Adaptive visualizations\n",
    "common_vmin = min(\n",
    "    grid_gdf_baseline['Visual_Baseline'].min(),\n",
    "    grid_gdf_adaptive['Visual_Adaptive'].min()\n",
    ")\n",
    "\n",
    "print(f\"Computed common_vmin: {common_vmin:.4f}\")\n",
    "\n",
    "### COMMON VMAX \n",
    "\n",
    "# Generate quantile range\n",
    "quantile_range = np.linspace(0.90, 1, 1000)\n",
    "cv_differences = []\n",
    "\n",
    "for q in quantile_range:\n",
    "    baseline_q = grid_gdf_baseline['Visual_Baseline'].quantile(q)\n",
    "    adaptive_q = grid_gdf_adaptive['Visual_Adaptive'].quantile(q)\n",
    "    \n",
    "    cv_baseline = np.std(grid_gdf_baseline['Visual_Baseline'][grid_gdf_baseline['Visual_Baseline'] <= baseline_q]) / np.mean(grid_gdf_baseline['Visual_Baseline'][grid_gdf_baseline['Visual_Baseline'] <= baseline_q])\n",
    "    cv_adaptive = np.std(grid_gdf_adaptive['Visual_Adaptive'][grid_gdf_adaptive['Visual_Adaptive'] <= adaptive_q]) / np.mean(grid_gdf_adaptive['Visual_Adaptive'][grid_gdf_adaptive['Visual_Adaptive'] <= adaptive_q])\n",
    "    \n",
    "    cv_differences.append(abs(cv_baseline - cv_adaptive))\n",
    "\n",
    "# Filter out the initial flat part by thresholding\n",
    "threshold_value = 0.01  # Adjust this value if needed\n",
    "filtered_indices = np.where(np.array(cv_differences) > threshold_value)[0]\n",
    "\n",
    "# Apply Kneedle only if there's a significant increase\n",
    "if len(filtered_indices) > 0:\n",
    "    filtered_quantiles = quantile_range[filtered_indices]\n",
    "    filtered_cv_differences = np.array(cv_differences)[filtered_indices]\n",
    "\n",
    "    # Use kneed to find the optimal quantile\n",
    "    kneedle = KneeLocator(filtered_quantiles, filtered_cv_differences, curve='convex', direction='increasing')\n",
    "    optimal_quantile = round(kneedle.elbow, 3)  # Display to three decimals\n",
    "else:\n",
    "    # Fallback if no significant increase is detected\n",
    "    optimal_quantile = 0.990  # A sensible high quantile to use\n",
    "\n",
    "# Plotting the improvement trend\n",
    "plt.figure(figsize=(10, 5), dpi=300)\n",
    "plt.plot(quantile_range, cv_differences, color='blue', label='CV Difference')\n",
    "plt.axvline(optimal_quantile, color='red', linestyle='--', label=f'Optimal Quantile = {optimal_quantile:.3f}')\n",
    "plt.title('CV Difference vs Quantile Cutoff')\n",
    "plt.xlabel('Quantile Cutoff')\n",
    "plt.ylabel('CV Difference')\n",
    "plt.legend()\n",
    "plt.grid(False)\n",
    "\n",
    "print(f\"Optimal Quantile Found: {optimal_quantile:.3f}\")\n",
    "\n",
    "\n",
    "common_vmax = min(\n",
    "    grid_gdf_baseline['Visual_Baseline'].quantile(optimal_quantile),\n",
    "    grid_gdf_adaptive['Visual_Adaptive'].quantile(optimal_quantile)\n",
    ")\n",
    "\n",
    "print(f\"Computed common_vmin: {common_vmax:.4f}\")\n",
    "\n",
    "output_path = \"quantilecutoff.eps\"\n",
    "plt.savefig(output_path, format='eps', bbox_inches='tight')\n",
    "print(f\"Plot saved in working directory: {os.path.abspath(output_path)}\")\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Baseline Optimal Gamma: {baseline_gamma:.4f}\")\n",
    "\n",
    "colormap = cm.LinearColormap(\n",
    "    colors=['darkblue', 'cyan', 'yellow', 'orangered'],\n",
    "    vmin=common_vmin,\n",
    "    vmax=common_vmax\n",
    ")\n",
    "\n",
    "# Folium map\n",
    "m = folium.Map(location=[df['Lat'].mean(), df['Log'].mean()], zoom_start=12, tiles='Cartodb positron')\n",
    "\n",
    "# Add GeoJSON overlay\n",
    "geojson = folium.GeoJson(\n",
    "    grid_gdf_baseline,\n",
    "    style_function=lambda feature: {\n",
    "        'fillColor': colormap(feature['properties']['Visual_Baseline']),\n",
    "        'weight': 0.0,\n",
    "        'fillOpacity': 0.4\n",
    "    }\n",
    ").add_to(m)\n",
    "\n",
    "colormap.caption = \"Scaled Count Intensity (Baseline)\"\n",
    "colormap.add_to(m)  # Attach to the map\n",
    "\n",
    "m"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Baseline vs. Adaptive Sensing Statistics and Improvement Calculation\n",
    "\n",
    "### Baseline Statistics Calculation\n",
    "The baseline statistics are calculated based on all the measurements collected within grid cells. \n",
    "\n",
    "1. **Count Calculation:**\n",
    "   The total number of measurements per grid cell is calculated using:\n",
    "\n",
    "   $\n",
    "   \\text{Baseline Counts} = \\sum_{i=1}^{N} \\text{Point}_i\n",
    "   $\n",
    "\n",
    "   Where $ N $ is the total number of measurements per grid cell.\n",
    "\n",
    "2. **Statistical Metrics:**\n",
    "   - Mean count ($ \\mu_{\\text{baseline}} $):\n",
    "\n",
    "    $\n",
    "    \\mu_{\\text{baseline}} = \\frac{1}{M} \\sum_{j=1}^{M} C_j\n",
    "    $\n",
    "\n",
    "    Where $ M $ is the number of grid cells, and $ C_j $ is the count of measurements in cell $ j $.\n",
    "\n",
    "   - Standard deviation ($ \\sigma_{\\text{baseline}} $):\n",
    "\n",
    "    $\n",
    "    \\sigma_{\\text{baseline}} = \\sqrt{\\frac{1}{M} \\sum_{j=1}^{M} (C_j - \\mu_{\\text{baseline}})^2}\n",
    "    $\n",
    "\n",
    "   - Coefficient of Variation ($ CV_{\\text{baseline}} $):\n",
    "\n",
    "    $\n",
    "    CV_{\\text{baseline}} = \\frac{\\sigma_{\\text{baseline}}}{\\mu_{\\text{baseline}}}\n",
    "    $\n",
    "\n",
    "---\n",
    "\n",
    "### Adaptive Sensing Statistics Calculation\n",
    "The adaptive statistics are calculated based on the filtered measurements where the sensor is **ON**.\n",
    "\n",
    "1. **Count Calculation:**\n",
    "   Similar to the baseline calculation, but only considering the filtered dataset:\n",
    "\n",
    "   $\n",
    "   \\text{Adaptive Counts} = \\sum_{i=1}^{N'} \\text{Point}_i\n",
    "   $\n",
    "\n",
    "   Where $ N' \\leq N $ represents the subset of measurements where the sensor is active.\n",
    "\n",
    "2. **Statistical Metrics:**\n",
    "\n",
    "   - Mean count ($ \\mu_{\\text{adaptive}} $):\n",
    "\n",
    "    $\n",
    "    \\mu_{\\text{adaptive}} = \\frac{1}{M'} \\sum_{j=1}^{M'} C_j\n",
    "    $\n",
    "\n",
    "   - Standard deviation ($ \\sigma_{\\text{adaptive}} $):\n",
    "\n",
    "    $\n",
    "    \\sigma_{\\text{adaptive}} = \\sqrt{\\frac{1}{M'} \\sum_{j=1}^{M'} (C_j - \\mu_{\\text{adaptive}})^2}\n",
    "    $\n",
    "\n",
    "   - Coefficient of Variation ($ CV_{\\text{adaptive}} $):\n",
    "\n",
    "    $\n",
    "    CV_{\\text{adaptive}} = \\frac{\\sigma_{\\text{adaptive}}}{\\mu_{\\text{adaptive}}}\n",
    "    $\n",
    "\n",
    "---\n",
    "\n",
    "### Improvement in Spatial Uniformity\n",
    "The improvement in spatial uniformity between the baseline and adaptive sensing techniques is quantified using the change in the Coefficient of Variation:\n",
    "\n",
    "1. **Change in Coefficient of Variation ($ \\Delta CV $):**\n",
    "\n",
    "   $\n",
    "   \\Delta CV = CV_{\\text{baseline}} - CV_{\\text{adaptive}}\n",
    "   $\n",
    "\n",
    "2. **Percentage Improvement in Uniformity:**\n",
    "\n",
    "   $\n",
    "   \\text{Percentage Improvement} = \\left( \\frac{\\Delta CV}{CV_{\\text{baseline}}} \\right) \\times 100\n",
    "   $\n",
    "\n",
    "---\n",
    "\n",
    "### Visualization of Measurement Distribution\n",
    "The distributions of counts per grid cell for both baseline and adaptive sensing methods are plotted using histograms.\n",
    "\n",
    "- The histograms are compared side-by-side to illustrate differences in measurement uniformity.\n",
    "- The title, labels, and legend provide context for understanding the comparison.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute baseline statistics\n",
    "baseline_counts = df_temp.groupby(['Lat_Grid', 'Log_Grid']).size()\n",
    "mean_count_baseline = baseline_counts.mean()\n",
    "std_count_baseline = baseline_counts.std()\n",
    "cv_baseline = std_count_baseline / mean_count_baseline\n",
    "\n",
    "print(f\"Baseline Statistics:\")\n",
    "print(f\"Mean Count: {mean_count_baseline:.4f}\")\n",
    "print(f\"Standard Deviation: {std_count_baseline:.4f}\")\n",
    "print(f\"CV (Baseline): {cv_baseline:.4f}\")\n",
    "\n",
    "# Define the label used in code \n",
    "label = \"360 sec\"\n",
    "\n",
    "# Filter the dataset to include only rows where the sensor is ON\n",
    "adaptive_df = df_temp[df_temp[f'Sensor_ON_{label}'] == True]\n",
    "\n",
    "# Compute adaptive statistics\n",
    "adaptive_counts = adaptive_df.groupby(['Lat_Grid', 'Log_Grid']).size()\n",
    "mean_count_adaptive = adaptive_counts.mean()\n",
    "std_count_adaptive = adaptive_counts.std()\n",
    "cv_adaptive = std_count_adaptive / mean_count_adaptive\n",
    "\n",
    "print(f\"\\n Adaptive Sensing Statistics ({label}):\")\n",
    "print(f\"Mean Count: {mean_count_adaptive:.4f}\")\n",
    "print(f\"Standard Deviation: {std_count_adaptive:.4f}\")\n",
    "print(f\"CV (Adaptive): {cv_adaptive:.4f}\")\n",
    "\n",
    "# Calculate improvement in uniformity\n",
    "delta_cv = cv_baseline - cv_adaptive\n",
    "percentage_improvement = (delta_cv / cv_baseline) * 100\n",
    "\n",
    "print(f\"\\n Improvement in Spatial Uniformity (ΔCV): {delta_cv:.4f}\")\n",
    "print(f\" Percentage Improvement in Uniformity: {percentage_improvement:.2f}%\")\n",
    "\n",
    "# Plot histogram of counts for baseline and adaptive scenarios\n",
    "plt.figure(figsize=(12, 6))\n",
    "\n",
    "plt.hist(baseline_counts, bins=50, alpha=0.5, label='Baseline (All Measurements)')\n",
    "plt.hist(adaptive_counts, bins=50, alpha=0.5, label=f'Adaptive Sensing ({label})')\n",
    "\n",
    "plt.title('Distribution of Measurements per Grid Cell')\n",
    "plt.xlabel('Number of Measurements per Grid Cell')\n",
    "plt.ylabel('Frequency')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Adaptive Optimal Gamma: {adaptive_gamma:.4f}\")\n",
    "\n",
    "colormap = cm.LinearColormap(\n",
    "    colors=['darkblue', 'cyan', 'yellow', 'orangered'],\n",
    "    vmin=common_vmin,\n",
    "    vmax=common_vmax\n",
    ")\n",
    "\n",
    "# Folium map\n",
    "m = folium.Map(location=[df['Lat'].mean(), df['Log'].mean()], zoom_start=12, tiles='Cartodb positron')\n",
    "\n",
    "# Add GeoJSON overlay for the adaptive map\n",
    "geojson = folium.GeoJson(\n",
    "    grid_gdf_adaptive,\n",
    "    style_function=lambda feature: {\n",
    "        'fillColor': colormap(feature['properties']['Visual_Adaptive']),\n",
    "        'weight': 0.0,\n",
    "        'fillOpacity': 0.4\n",
    "    }\n",
    ").add_to(m)\n",
    "\n",
    "colormap.caption = \"Count Intensity (Adaptive)\"\n",
    "colormap.add_to(m)\n",
    "\n",
    "m"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Enhanced Difference Calculation for Visual Maps\n",
    "\n",
    "### Normalization of Visual Maps\n",
    "To allow for a consistent comparison between **Baseline** and **Adaptive** maps, both visual maps are normalized between $0$ and $1$ using min-max normalization:\n",
    "\n",
    "$\n",
    "\\text{Normalized\\_Visual\\_Baseline} = \\frac{\\text{Visual\\_Baseline} - \\text{visual\\_baseline\\_min}}{\\text{visual\\_baseline\\_max} - \\text{visual\\_baseline\\_min}}\n",
    "$\n",
    "\n",
    "$\n",
    "\\text{Normalized\\_Visual\\_Adaptive} = \\frac{\\text{Visual\\_Adaptive} - \\text{visual\\_adaptive\\_min}}{\\text{visual\\_adaptive\\_max} - \\text{visual\\_adaptive\\_min}}\n",
    "$\n",
    "\n",
    "Where:\n",
    "- $ \\text{visual\\_baseline\\_min}, \\text{visual\\_baseline\\_max} $ are the minimum and maximum values of the baseline visualization.\n",
    "- $ \\text{visual\\_adaptive\\_min}, \\text{visual\\_adaptive\\_max} $ are the minimum and maximum values of the adaptive visualization.\n",
    "\n",
    "---\n",
    "\n",
    "### Enhanced Difference Calculation\n",
    "The relative improvement is calculated by comparing the **Adaptive Visualization** against the **Baseline Visualization**. \n",
    "\n",
    "1. **Difference Calculation:**\n",
    "   $\n",
    "   \\text{Difference} = \\text{Normalized\\_Visual\\_Adaptive} - \\text{Normalized\\_Visual\\_Baseline}\n",
    "   $\n",
    "\n",
    "2. **Difference Sign Identification:**\n",
    "   The sign of the difference is identified to distinguish between positive and negative improvements.\n",
    "\n",
    "   $\n",
    "   \\text{Difference\\_Sign} = \\text{sign}(\\text{Difference})\n",
    "   $\n",
    "\n",
    "---\n",
    "\n",
    "### Sigmoid Scaling for Enhanced Visualization\n",
    "To enhance the contrast of the differences, a **sigmoid function** is applied. This mapping provides a more sensitive representation of differences, particularly useful for visual comparison.\n",
    "\n",
    "$\n",
    "\\text{Enhanced\\_Difference} = \\text{Difference\\_Sign} \\times \\left( \\frac{1}{1 + \\exp(-k \\cdot \\text{Difference})} - 0.5 \\right)\n",
    "$\n",
    "\n",
    "Where:\n",
    "- $ k $ is a tuning parameter controlling the steepness of the sigmoid function.  \n",
    "- Higher $ k $ values result in stronger contrast, highlighting even small differences.\n",
    "\n",
    "---\n",
    "\n",
    "### Normalization of Enhanced Differences\n",
    "The enhanced differences are normalized to a range of $ [0, 1] $ for visual mapping:\n",
    "\n",
    "$\n",
    "\\text{Enhanced\\_Difference\\_Vis} = \\frac{\\text{Enhanced\\_Difference} - \\text{enhanced\\_diff\\_min}}{\\text{enhanced\\_diff\\_max} - \\text{enhanced\\_diff\\_min}}\n",
    "$\n",
    "\n",
    "Where:\n",
    "- $ \\text{enhanced\\_diff\\_min} $ and $ \\text{enhanced\\_diff\\_max} $ are the minimum and maximum values of $ \\text{Enhanced\\_Difference} $.\n",
    "\n",
    "---\n",
    "\n",
    "### Colormap Definition\n",
    "To highlight the improvements, a **Blue-White Colormap** is defined, where:\n",
    "\n",
    "- **White (0):** Represents no improvement.  \n",
    "- **Blue (1):** Represents maximum improvement.\n",
    "\n",
    "---\n",
    "\n",
    "### Folium Map Generation\n",
    "A **Folium Map** is generated to visualize the improvements:\n",
    "\n",
    "1. **GeoJSON Overlay:**  \n",
    "   The difference map is added as a GeoJSON overlay with custom styling based on the normalized enhanced differences.\n",
    "\n",
    "2. **Colormap:**  \n",
    "   A colormap legend is added to the map to indicate the intensity of improvement."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ensure Both Visual Maps Are Normalized Between 0 and 1\n",
    "visual_baseline_min = grid_gdf_baseline['Visual_Baseline'].min()\n",
    "visual_baseline_max = grid_gdf_baseline['Visual_Baseline'].max()\n",
    "grid_gdf_baseline['Normalized_Visual_Baseline'] = (grid_gdf_baseline['Visual_Baseline'] - visual_baseline_min) / (visual_baseline_max - visual_baseline_min)\n",
    "\n",
    "visual_adaptive_min = grid_gdf_adaptive['Visual_Adaptive'].min()\n",
    "visual_adaptive_max = grid_gdf_adaptive['Visual_Adaptive'].max()\n",
    "grid_gdf_adaptive['Normalized_Visual_Adaptive'] = (grid_gdf_adaptive['Visual_Adaptive'] - visual_adaptive_min) / (visual_adaptive_max - visual_adaptive_min)\n",
    "\n",
    "# Calculate Relative Improvement (Enhanced Difference Calculation)\n",
    "grid_gdf_adaptive['Difference'] = grid_gdf_adaptive['Normalized_Visual_Adaptive'] - grid_gdf_baseline['Normalized_Visual_Baseline']\n",
    "grid_gdf_adaptive['Difference_Sign'] = np.sign(grid_gdf_adaptive['Difference'])\n",
    "\n",
    "# Sigmoid Scaling (Adjust k for more/less aggressive enhancement)\n",
    "# k = 30  # Increase for stronger contrast, decrease for smoother visualization\n",
    "grid_gdf_adaptive['Enhanced_Difference'] = grid_gdf_adaptive['Difference_Sign'] * (1 / (1 + np.exp(-k * grid_gdf_adaptive['Difference'])) - 0.5)\n",
    "\n",
    "# Normalization of Enhanced Differences for Optimal Visualization\n",
    "enhanced_diff_min = grid_gdf_adaptive['Enhanced_Difference'].min()\n",
    "enhanced_diff_max = grid_gdf_adaptive['Enhanced_Difference'].max()\n",
    "\n",
    "# Normalizing to range [0, 1] for Blue intensities\n",
    "grid_gdf_adaptive['Enhanced_Difference_Vis'] = (grid_gdf_adaptive['Enhanced_Difference'] - enhanced_diff_min) / (enhanced_diff_max - enhanced_diff_min)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a More Sensitive Colormap with Strong Contrast (Optimized Blue Scale)\n",
    "colormap = cm.LinearColormap(\n",
    "    colors=['white','blue'],  # Only Blue to highlight improvements\n",
    "    vmin=0,  # Minimum of the normalized data\n",
    "    vmax=1   # Maximum of the normalized data\n",
    ")\n",
    "\n",
    "# Folium map\n",
    "m = folium.Map(location=[df['Lat'].mean(), df['Log'].mean()], zoom_start=12, tiles='Cartodb positron')\n",
    "\n",
    "# Add GeoJSON overlay for the difference map\n",
    "geojson = folium.GeoJson(\n",
    "    grid_gdf_adaptive,\n",
    "    style_function=lambda feature: {\n",
    "        'fillColor': colormap(feature['properties']['Enhanced_Difference_Vis']),\n",
    "        'weight': 0.0,\n",
    "        'fillOpacity': 0.4\n",
    "    }\n",
    ").add_to(m)\n",
    "\n",
    "colormap.caption = \"Improvement Map (Uniformity Improvement in Blue)\"\n",
    "colormap.add_to(m)\n",
    "\n",
    "m"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import ttest_ind, mannwhitneyu, ks_2samp, levene\n",
    "\n",
    "# Compute Statistical Tests\n",
    "t_stat, t_pvalue = ttest_ind(baseline_counts, adaptive_counts, equal_var=False)\n",
    "u_stat, mw_pvalue = mannwhitneyu(baseline_counts, adaptive_counts, alternative='two-sided')\n",
    "ks_stat, ks_pvalue = ks_2samp(baseline_counts, adaptive_counts)\n",
    "levene_stat, levene_pvalue = levene(baseline_counts, adaptive_counts)\n",
    "\n",
    "# Improvement metrics\n",
    "delta_cv = cv_baseline - cv_adaptive\n",
    "percentage_improvement = (delta_cv / cv_baseline) * 100\n",
    "\n",
    "# Store results in a comparative DataFrame\n",
    "results_df = pd.DataFrame({\n",
    "    'Metric': ['Mean Count (μ)', 'Standard Deviation (σ)', 'Coefficient of Variation (CV)', \n",
    "               'Improvement in Uniformity (ΔCV)', 'Percentage Improvement in Uniformity', \n",
    "               'T-Test (p-value)', 'Mann-Whitney U (p-value)', 'Kolmogorov-Smirnov (p-value)', 'Levene Test (p-value)'],\n",
    "    'Baseline': [mean_count_baseline, std_count_baseline, cv_baseline, delta_cv, percentage_improvement,\n",
    "                 t_pvalue, mw_pvalue, ks_pvalue, levene_pvalue],\n",
    "    'Adaptive': [mean_count_adaptive, std_count_adaptive, cv_adaptive, np.nan, np.nan, np.nan, np.nan, np.nan, np.nan]\n",
    "})\n",
    "\n",
    "# Display the results\n",
    "print(results_df)\n",
    "\n",
    "# Log Transformation to make extreme values more visible\n",
    "baseline_counts_log = np.log1p(baseline_counts)  # Log transform baseline counts\n",
    "adaptive_counts_log = np.log1p(adaptive_counts)  # Log transform adaptive counts\n",
    "\n",
    "# Plot PDFs for Baseline vs. Adaptive (Log scale) with 50% transparent fills\n",
    "plt.figure(figsize=(10, 8), dpi=300)   \n",
    "sns.kdeplot(baseline_counts_log, label='Baseline', color=\"#d62728\", fill=True, alpha=0.3)\n",
    "sns.kdeplot(adaptive_counts_log, label='Adaptive', color=\"#2ca02c\", fill=True, alpha=0.3)\n",
    "plt.title('Log Transformed Probability Density Functions')\n",
    "plt.xlabel('Log(Number of Measurements per Grid Cell)')\n",
    "plt.ylabel('Density')\n",
    "plt.legend(loc='upper right')\n",
    "plt.xlim(0.5, 2)\n",
    "plt.grid(False)\n",
    "\n",
    "# Save plot\n",
    "output_path = \"logpde.pdf\"\n",
    "plt.savefig(output_path, format='pdf', bbox_inches='tight')\n",
    "print(f\"Plot saved in working directory: {os.path.abspath(output_path)}\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Energy Efficiency Analysis\n",
    "if 'SOC_batt' in df_temp.columns and 'SOC_batt_360 sec' in df_temp.columns:\n",
    "    # Calculate total SOC for baseline and adaptive approaches (sum of SOC values)\n",
    "    total_soc_baseline = df_temp['SOC_batt'].sum()\n",
    "    total_soc_adaptive = df_temp['SOC_batt_360 sec'].sum()\n",
    "\n",
    "    # Compute energy savings as the difference in total SOC\n",
    "    energy_savings = total_soc_adaptive - total_soc_baseline\n",
    "    percentage_energy_savings = (energy_savings / total_soc_baseline) * 100\n",
    "\n",
    "    # Prepare a DataFrame for energy efficiency analysis\n",
    "    energy_efficiency_df = pd.DataFrame({\n",
    "        'Metric': ['Total SOC (Baseline)', 'Total SOC (Adaptive)', \n",
    "                   'Energy Savings (Absolute)', 'Percentage Energy Savings'],\n",
    "        'Value': [total_soc_baseline, total_soc_adaptive, energy_savings, percentage_energy_savings]\n",
    "    })\n",
    "    \n",
    "    # Plot total SOC comparison with restricted x-axis range\n",
    "    plt.figure(figsize=(10, 8), dpi=300)\n",
    "    sns.kdeplot(df_temp['SOC_batt'], label='Baseline', color=\"#d62728\", fill=True, alpha=0.3)\n",
    "    sns.kdeplot(df_temp['SOC_batt_360 sec'], label='Adaptive', color=\"#2ca02c\", fill=True, alpha=0.3)\n",
    "    plt.title('Total SOC Comparison')\n",
    "    plt.xlabel('SOC [%]')\n",
    "    plt.ylabel('Density')\n",
    "    plt.xlim(0, 100)  # Crop the x-axis to the valid range of SOC values\n",
    "    plt.legend(loc='upper right')\n",
    "    plt.grid(False)\n",
    "    output_path = \"totsoc.pdf\"\n",
    "    plt.savefig(output_path, format='pdf', bbox_inches='tight')\n",
    "    print(f\"Plot saved in working directory: {os.path.abspath(output_path)}\")\n",
    "    plt.show()\n",
    "\n",
    "    print(energy_efficiency_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Temporal Analysis of CV Over Time (Every 5 Minutes, Displayed Every 30 Minutes)\n",
    "df_temp['Hour'] = pd.to_datetime(df_temp['Timestamp']).dt.strftime('%H:%M')\n",
    "\n",
    "# Convert to datetime format and round to nearest 5-minute interval\n",
    "df_temp['Hour'] = pd.to_datetime(df_temp['Hour']).dt.floor('15T').dt.strftime('%H:%M')\n",
    "\n",
    "# Group by 5-minute intervals and calculate CV for baseline and adaptive approaches\n",
    "hourly_stats = df_temp.groupby('Hour').agg({\n",
    "    'SOC_batt': ['mean', 'std'],\n",
    "    'SOC_batt_360 sec': ['mean', 'std']\n",
    "})\n",
    "\n",
    "# Compute CV for each 5-minute interval\n",
    "hourly_stats['CV_Baseline'] = hourly_stats[('SOC_batt', 'std')] / hourly_stats[('SOC_batt', 'mean')]\n",
    "hourly_stats['CV_Adaptive'] = hourly_stats[('SOC_batt_360 sec', 'std')] / hourly_stats[('SOC_batt_360 sec', 'mean')]\n",
    "\n",
    "# Extract available range from the data\n",
    "available_times = pd.to_datetime(hourly_stats.index)\n",
    "start_time = available_times.min()\n",
    "end_time = available_times.max()\n",
    "\n",
    "# Generate x-ticks for every half-hour within the available range\n",
    "x_ticks = pd.date_range(start=start_time, end=end_time, freq='60T').strftime('%H:%M')\n",
    "\n",
    "# Plot Temporal Changes in CV\n",
    "plt.figure(figsize=(12, 6), dpi=300)\n",
    "plt.plot(hourly_stats.index, hourly_stats['CV_Baseline'], label='Baseline', marker='o', color=\"#d62728\")\n",
    "plt.plot(hourly_stats.index, hourly_stats['CV_Adaptive'], label='Adaptive', marker='o', color=\"#2ca02c\")\n",
    "plt.title('Temporal Changes in Coefficient of Variation')\n",
    "plt.xlabel('Time of the Day')\n",
    "plt.ylabel('Coefficient of Variation')\n",
    "plt.xticks(x_ticks, rotation=45)\n",
    "plt.legend(loc='upper right')\n",
    "output_path = \"deltacv.eps\"\n",
    "plt.savefig(output_path, format='eps', bbox_inches='tight')\n",
    "print(f\"Plot saved in working directory: {os.path.abspath(output_path)}\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neighbors import NearestNeighbors\n",
    "from kneed import KneeLocator\n",
    "from joblib import Parallel, delayed\n",
    "# Ensure required columns\n",
    "if 'Lat_Grid' not in grid_gdf_adaptive.columns or 'Log_Grid' not in grid_gdf_adaptive.columns:\n",
    "    grid_gdf_adaptive['Lat_Grid'] = adaptive_df['Lat_Grid']\n",
    "    grid_gdf_adaptive['Log_Grid'] = adaptive_df['Log_Grid']\n",
    "\n",
    "# Calculate 'Difference' if missing\n",
    "if 'Difference' not in grid_gdf_adaptive.columns:\n",
    "    grid_gdf_adaptive['Difference'] = grid_gdf_adaptive['Normalized_Visual_Adaptive'] - grid_gdf_baseline['Normalized_Visual_Baseline']\n",
    "\n",
    "# Prepare spatial data\n",
    "filtered_data = grid_gdf_adaptive\n",
    "spatial_data = filtered_data[['Lat_Grid', 'Log_Grid', 'Difference']].dropna()\n",
    "X = spatial_data[['Lat_Grid', 'Log_Grid']].values\n",
    "\n",
    "# Step 1: Function to evaluate a single k\n",
    "def evaluate_k(k, X):\n",
    "    try:\n",
    "        nn = NearestNeighbors(n_neighbors=k)\n",
    "        nn.fit(X)\n",
    "        distances, _ = nn.kneighbors(X)\n",
    "        k_distances = np.sort(distances[:, k-1])\n",
    "        kneedle = KneeLocator(np.arange(len(k_distances)), k_distances, curve='convex', direction='increasing')\n",
    "        knee_idx = kneedle.knee\n",
    "        knee_distance = k_distances[knee_idx] if knee_idx is not None else np.nan\n",
    "        return k, knee_distance, k_distances\n",
    "    except:\n",
    "        return k, np.nan, None\n",
    "\n",
    "# Step 2: Parallel analysis over k_values\n",
    "def analyze_k_distance_parallel(X, k_values=range(5, 300, 5), n_jobs=-1):\n",
    "    results = Parallel(n_jobs=n_jobs)(\n",
    "        delayed(evaluate_k)(k, X) for k in k_values\n",
    "    )\n",
    "\n",
    "    optimal_distances = {k: d for k, d, _ in results}\n",
    "    k_distances_dict = {k: dist for k, _, dist in results if dist is not None}\n",
    "\n",
    "    # Convert dictionary to lists for sorting and analysis\n",
    "    k_list = list(optimal_distances.keys())\n",
    "    distance_list = list(optimal_distances.values())\n",
    "    distances_array = np.array([d if d is not np.nan else 0 for d in distance_list])\n",
    "    \n",
    "    # Compute slopes (first derivative)\n",
    "    slopes = np.diff(distances_array)\n",
    "    \n",
    "    # Identify the sharpest jump (best_k)\n",
    "    best_k_index = np.argmax(slopes)\n",
    "    best_k = k_list[best_k_index]\n",
    "    best_k_distance = optimal_distances[best_k]\n",
    "\n",
    "    # Find `k_min`: Start of gradual increase before the sharp rise\n",
    "    window_size = 3  # Number of consecutive increasing slopes to detect\n",
    "    for i in range(best_k_index - 1, -1, -1):\n",
    "        if np.all(slopes[i - window_size + 1: i + 1] > 0):\n",
    "            k_min = k_list[i]\n",
    "            break\n",
    "    else:\n",
    "        k_min = k_list[0]\n",
    "\n",
    "    # Detect `k_max` (when the slope stabilizes)\n",
    "    normalized_slopes = slopes / np.max(slopes)\n",
    "    stabilization_indices = np.where(normalized_slopes < 0.05)[0]\n",
    "\n",
    "    if len(stabilization_indices) > 0:\n",
    "        k_max = k_list[stabilization_indices[0] + best_k_index]\n",
    "    else:\n",
    "        k_max = k_list[-1]\n",
    "    \n",
    "    # Plot the best k-distance curve\n",
    "    if best_k in k_distances_dict:\n",
    "        best_k_distances = k_distances_dict[best_k]\n",
    "        plt.figure(figsize=(10, 5), dpi=300)\n",
    "        plt.plot(best_k_distances, label=f'{best_k}-distance curve')\n",
    "        knee_idx = np.where(best_k_distances == best_k_distance)[0][0] if not np.isnan(best_k_distance) else None\n",
    "        if knee_idx is not None:\n",
    "            plt.axvline(knee_idx, color='red', linestyle='--', label=f'Knee at index: {knee_idx}')\n",
    "            plt.axhline(best_k_distance, color='green', linestyle='--', label=f'Distance: {best_k_distance:.4f}')\n",
    "        plt.title(f'Best k-Distance Plot (k={best_k})')\n",
    "        plt.xlabel(\"Sorted Points\")\n",
    "        plt.ylabel(f\"Distance to {best_k}th Nearest Neighbor\")\n",
    "        plt.legend()\n",
    "        plt.grid(True)\n",
    "        plt.show()\n",
    "\n",
    "    # Plot evolution of thresholds and slopes on dual axes\n",
    "    fig, ax1 = plt.subplots(figsize=(10, 5), dpi=300)\n",
    "\n",
    "    # Plot distance thresholds\n",
    "    color1 = \"#1f77b4\"\n",
    "    ax1.set_xlabel(\"$k$ Number of Nearest Neighbors\")\n",
    "    ax1.set_ylabel(\"$d$ Distance Threshold\", color=color1)\n",
    "    ax1.plot(k_list, distance_list, marker='.', color=color1, label='$d$')\n",
    "    ax1.tick_params(axis='y', labelcolor=color1)\n",
    "    ax1.axvline(k_min, color=\"#ff7f0e\", linestyle='--', label=f'$k_{{\\\\min}}$: {k_min}')\n",
    "    ax1.axvline(best_k, color=\"#2ca02c\", linestyle='--', label=f'$k_{{\\\\mathrm{{best}}}}$: {best_k}')\n",
    "    ax1.axvline(k_max, color=\"#9467bd\", linestyle='--', label=f'$k_{{\\\\max}}$: {k_max}')\n",
    "\n",
    "    # Create second y-axis for slope\n",
    "    ax2 = ax1.twinx()\n",
    "    color2 = \"#d62728\"\n",
    "    ax2.set_ylabel(r\"($\\Delta d / \\Delta k$) Rate of Change\", color=color2)\n",
    "    ax2.plot(k_list[:-1], slopes, marker='x', color=color2, label=r'$\\Delta d / \\Delta k$')\n",
    "    ax2.tick_params(axis='y', labelcolor=color2)\n",
    "\n",
    "    # Combine legends from both axes\n",
    "    lines1, labels1 = ax1.get_legend_handles_labels()\n",
    "    lines2, labels2 = ax2.get_legend_handles_labels()\n",
    "    fig.legend(lines1 + lines2, labels1 + labels2, loc='center right',  bbox_to_anchor=(0.9, 0.5), frameon=True)\n",
    "\n",
    "    plt.title(\"Evolution of Distance Threshold and its Gradient vs $k$\")\n",
    "    plt.grid(False)\n",
    "    output_path = \"thresholds.eps\"\n",
    "    plt.savefig(output_path, format='eps', bbox_inches='tight')\n",
    "    print(f\"Plot saved in working directory: {os.path.abspath(output_path)}\")\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "    return optimal_distances, k_min, best_k, k_max\n",
    "\n",
    "# Step 3: Run Analysis with a Full Range of k-values\n",
    "optimal_distances, k_min, k_best, k_max = analyze_k_distance_parallel(X)\n",
    "\n",
    "# Step 4: Generate Suggested Parameter Ranges Automatically\n",
    "n_points = X.shape[0]\n",
    "suggested_min_samples = list(range(k_min, k_max + 1, 5))  # Range of min_samples\n",
    "\n",
    "# Display Results\n",
    "print(f\"\\nTotal Number of Points: {n_points}\")\n",
    "print(\"\\nAuto-estimated min_samples range:\", suggested_min_samples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import ParameterSampler\n",
    "from hdbscan import validity_index\n",
    "import hdbscan\n",
    "from branca.colormap import LinearColormap\n",
    "from tqdm import tqdm\n",
    "from joblib import Parallel, delayed\n",
    "import random\n",
    "import warnings\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import os\n",
    "from itertools import product\n",
    "from mpl_toolkits.axes_grid1 import make_axes_locatable\n",
    "\n",
    "warnings.filterwarnings(\"ignore\", message=\"'force_all_finite' was renamed to 'ensure_all_finite'\")\n",
    "\n",
    "# Optional: set seed\n",
    "# SEED = 42\n",
    "percentage = 1  # Set percentage of total values for search\n",
    "# random.seed(SEED)\n",
    "\n",
    "# Define hyperparameter search space\n",
    "param_dist = {\n",
    "    'min_samples': np.arange(k_min, k_max + 1, 1),            \n",
    "    'min_cluster_size': np.arange(k_max, 235 + 1, 5),     \n",
    "    'cluster_selection_method': ['eom', 'leaf'],\n",
    "    'metric': ['euclidean', 'manhattan']\n",
    "}\n",
    "\n",
    "# Generate valid combinations where min_samples <= min_cluster_size\n",
    "param_samples = [\n",
    "    {\n",
    "        'min_samples': ms,\n",
    "        'min_cluster_size': mcs,\n",
    "        'cluster_selection_method': method,\n",
    "        'metric': metric\n",
    "    }\n",
    "    for ms, mcs, method, metric in product(\n",
    "        param_dist['min_samples'],\n",
    "        param_dist['min_cluster_size'],\n",
    "        param_dist['cluster_selection_method'],\n",
    "        param_dist['metric']\n",
    "    )\n",
    "    if ms <= mcs\n",
    "]\n",
    "\n",
    "# Randomly sample the parameter space\n",
    "k = round(len(param_samples) * percentage)\n",
    "param_samples = random.sample(param_samples, k)\n",
    "\n",
    "# Define function to run HDBSCAN on one parameter set\n",
    "def evaluate_hdbscan(params):\n",
    "    try:\n",
    "        model = hdbscan.HDBSCAN(\n",
    "            min_samples=params['min_samples'],\n",
    "            min_cluster_size=params['min_cluster_size'],\n",
    "            cluster_selection_method=params['cluster_selection_method'],\n",
    "            metric=params['metric'],\n",
    "            gen_min_span_tree=True\n",
    "        )\n",
    "        model.fit(X)\n",
    "        if len(np.unique(model.labels_)) > 1:\n",
    "            score = validity_index(X, model.labels_)\n",
    "        else:\n",
    "            score = np.nan\n",
    "        return {\n",
    "            'DBCV': score,\n",
    "            'min_samples': params['min_samples'],\n",
    "            'min_cluster_size': params['min_cluster_size'],\n",
    "            'metric': params['metric'],\n",
    "            'method': params['cluster_selection_method'],\n",
    "            'labels': model.labels_\n",
    "        }\n",
    "    except Exception:\n",
    "        return {\n",
    "            'DBCV': np.nan,\n",
    "            'min_samples': params['min_samples'],\n",
    "            'min_cluster_size': params['min_cluster_size'],\n",
    "            'metric': params['metric'],\n",
    "            'method': params['cluster_selection_method'],\n",
    "            'labels': np.full(X.shape[0], -1)\n",
    "        }\n",
    "\n",
    "# Run the search in parallel\n",
    "parallel_results = Parallel(n_jobs=-1, backend=\"loky\")(\n",
    "    delayed(evaluate_hdbscan)(params) for params in tqdm(param_samples, desc=\"Parallel HDBSCAN Search\", leave=True)\n",
    ")\n",
    "\n",
    "# Convert to DataFrame\n",
    "results_df = pd.DataFrame(parallel_results)\n",
    "\n",
    "# Identify best parameters\n",
    "best_row = results_df.loc[results_df['DBCV'].idxmax()]\n",
    "best_score = best_row['DBCV']\n",
    "best_params = best_row[['min_samples', 'min_cluster_size', 'metric', 'method']].to_dict()\n",
    "best_labels = best_row['labels']\n",
    "\n",
    "# Print best result\n",
    "print(f\"Best Parameters: {best_params}\")\n",
    "print(f\"Best DBCV score: {best_score}\")\n",
    "\n",
    "# Apply best clustering\n",
    "spatial_data = spatial_data.copy()\n",
    "spatial_data['HDBSCAN_Cluster_Label'] = best_labels\n",
    "\n",
    "# === Plot all combinations as 4 heatmaps in a single figure with shared colorbar ===\n",
    "combinations = [\n",
    "    ('eom', 'manhattan'),\n",
    "    ('eom', 'euclidean'),\n",
    "    ('leaf', 'manhattan'),\n",
    "    ('leaf', 'euclidean')\n",
    "]\n",
    "\n",
    "# Compute global DBCV range\n",
    "vmin = results_df['DBCV'].min()\n",
    "vmax = results_df['DBCV'].max()\n",
    "\n",
    "fig, axes = plt.subplots(2, 2, figsize=(18, 18), dpi=300)\n",
    "\n",
    "# Plot all heatmaps\n",
    "# === Plot all combinations ===\n",
    "for ax, (method, metric) in zip(axes.flat, combinations):\n",
    "    filtered = results_df[\n",
    "        (results_df['method'] == method) & (results_df['metric'] == metric)\n",
    "    ]\n",
    "    if filtered.empty:\n",
    "        ax.set_visible(False)\n",
    "        continue\n",
    "\n",
    "    heatmap_data = filtered.pivot(\n",
    "        index='min_cluster_size',\n",
    "        columns='min_samples',\n",
    "        values='DBCV'\n",
    "    )\n",
    "\n",
    "    if heatmap_data.empty:\n",
    "        ax.set_visible(False)\n",
    "        continue\n",
    "\n",
    "    sns.heatmap(\n",
    "        heatmap_data,\n",
    "        ax=ax,\n",
    "        annot=True,\n",
    "        fmt=\".2f\",\n",
    "        cmap=\"YlGnBu\",\n",
    "        vmin=vmin,\n",
    "        vmax=vmax,\n",
    "        cbar=False  # Suppress individual colorbars\n",
    "    )\n",
    "    ax.set_title(f\"{method.upper()} + {metric.capitalize()}\", fontsize=14)\n",
    "    ax.set_xlabel(\"min_samples\")\n",
    "    ax.set_ylabel(\"min_cluster_size\")\n",
    "\n",
    "# === Create standalone axis for shared colorbar ===\n",
    "# [left, bottom, width, height] in figure coordinates\n",
    "cbar_ax = fig.add_axes([0.93, 0.25, 0.015, 0.5])  \n",
    "sm = plt.cm.ScalarMappable(cmap=\"YlGnBu\", norm=plt.Normalize(vmin=vmin, vmax=vmax))\n",
    "sm.set_array([])  # Dummy array for colorbar\n",
    "fig.colorbar(sm, cax=cbar_ax, label=\"DBCV Score\")\n",
    "\n",
    "# === Final layout ===\n",
    "plt.suptitle(\"HDBSCAN DBCV Scores Across Hyperparameter Combinations\", fontsize=16)\n",
    "plt.tight_layout(rect=[0, 0, 0.9, 0.96])  # Reserve space for the colorbar\n",
    "\n",
    "# Save figure\n",
    "output_path = \"heatmap_grid_shared_colorbar.pdf\"\n",
    "plt.savefig(output_path, format='pdf', bbox_inches='tight')\n",
    "print(f\"✅ Grid with shared colorbar saved at: {os.path.abspath(output_path)}\")\n",
    "\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize on Folium map\n",
    "unique_labels = np.unique(best_labels)\n",
    "num_unique_labels = len(unique_labels)\n",
    "colormap = plt.get_cmap('hsv', num_unique_labels)\n",
    "colors = {label: colormap(i / num_unique_labels) for i, label in enumerate(unique_labels)}\n",
    "colors_hex = {\n",
    "    label: f'#{int(r*255):02x}{int(g*255):02x}{int(b*255):02x}'\n",
    "    for label, (r, g, b, _) in colors.items()\n",
    "}\n",
    "\n",
    "# Create map\n",
    "map_clusters = folium.Map(\n",
    "    location=[spatial_data['Lat_Grid'].mean(), spatial_data['Log_Grid'].mean()],\n",
    "    zoom_start=13, tiles='Cartodb dark_matter'\n",
    ")\n",
    "\n",
    "# Add points\n",
    "for _, row in spatial_data.iterrows():\n",
    "    label = row['HDBSCAN_Cluster_Label']\n",
    "    color = 'white' if label == -1 else colors_hex.get(label, 'gray')\n",
    "    folium.CircleMarker(\n",
    "        location=[row['Lat_Grid'], row['Log_Grid']],\n",
    "        radius=1,\n",
    "        color=color,\n",
    "        weight=0,\n",
    "        fill=True,\n",
    "        fill_color=color,\n",
    "        fill_opacity=0.7\n",
    "    ).add_to(map_clusters)\n",
    "\n",
    "# Add color legend\n",
    "cluster_colormap = LinearColormap(\n",
    "    colors=[colors_hex[label] for label in unique_labels if label != -1],\n",
    "    vmin=min(unique_labels),\n",
    "    vmax=max(unique_labels),\n",
    "    caption=\"Cluster Labels\"\n",
    ")\n",
    "cluster_colormap.add_to(map_clusters)\n",
    "\n",
    "# Show map\n",
    "map_clusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Open poweshell in new terminal and run\n",
    "\n",
    "docker build -t sensing-whale \"C:\\Users\\mayar\\OneDrive - Massachusetts Institute of Technology\\Desktop\\energy-aware\"\n",
    "\n",
    "After building the image, use -v to mount the local DATA directory inside /workspace/data/ in the container:\n",
    "\n",
    "docker run -it --gpus all --shm-size=1g --ulimit memlock=-1 --ulimit stack=67108864 -v \"C:\\Users\\mayar\\OneDrive - Massachusetts Institute of Technology\\Desktop\\energy-aware\\DATA:/workspace/data\" -p 8888:8888 sensing-whale"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "to detect docker containers run: docker ps\n",
    "\n",
    "to stop docker container: docker stop 'insert container name'\n",
    "\n",
    "to delete docker container: docker rm 'insert container name'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imported Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/workspace\n",
      "['data']\n",
      "Tue Mar 25 09:26:02 2025       \n",
      "+-----------------------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 555.58.02              Driver Version: 556.12         CUDA Version: 12.5     |\n",
      "|-----------------------------------------+------------------------+----------------------+\n",
      "| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |\n",
      "|                                         |                        |               MIG M. |\n",
      "|=========================================+========================+======================|\n",
      "|   0  NVIDIA GeForce RTX 4090 ...    On  |   00000000:01:00.0  On |                  N/A |\n",
      "| N/A   58C    P0             37W /  150W |     624MiB /  16376MiB |      0%      Default |\n",
      "|                                         |                        |                  N/A |\n",
      "+-----------------------------------------+------------------------+----------------------+\n",
      "                                                                                         \n",
      "+-----------------------------------------------------------------------------------------+\n",
      "| Processes:                                                                              |\n",
      "|  GPU   GI   CI        PID   Type   Process name                              GPU Memory |\n",
      "|        ID   ID                                                               Usage      |\n",
      "|=========================================================================================|\n",
      "|  No running processes found                                                             |\n",
      "+-----------------------------------------------------------------------------------------+\n",
      "RAPIDS & required libraries loaded successfully!\n"
     ]
    }
   ],
   "source": [
    "# Standard Library Imports\n",
    "import logging\n",
    "import multiprocessing as mp\n",
    "import os\n",
    "import random\n",
    "from collections import defaultdict\n",
    "from datetime import datetime\n",
    "\n",
    "# Check current working directory and list files\n",
    "print(os.getcwd())\n",
    "print(os.listdir())\n",
    "\n",
    "# Numerical & Data Processing \n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import cudf\n",
    "import cupy as cp\n",
    "import cuspatial\n",
    "import cuml  # RAPIDS cuML for accelerated machine learning\n",
    "import numba\n",
    "\n",
    "# Check GPU Status\n",
    "!nvidia-smi\n",
    "\n",
    "# Geospatial Processing \n",
    "import geopandas as gpd\n",
    "from shapely.geometry import Polygon, Point\n",
    "import shapely\n",
    "\n",
    "# Visualization \n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from matplotlib.colors import Normalize, to_hex\n",
    "import folium\n",
    "import branca.colormap as cm\n",
    "import matplotlib.colors as mcolors\n",
    "import optuna.visualization\n",
    "import contextily as ctx\n",
    "\n",
    "# Statistical & Curve Fitting \n",
    "import scipy.stats as stats\n",
    "from scipy.optimize import curve_fit\n",
    "from scipy.stats import kstest\n",
    "\n",
    "# Machine Learning & Optimization \n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error\n",
    "from hyperopt import fmin, tpe, hp, STATUS_OK, Trials\n",
    "import optuna\n",
    "import xgboost as xgb\n",
    "\n",
    "# Utility \n",
    "from kneed import KneeLocator\n",
    "from tqdm import tqdm  # Import tqdm for the progress bar\n",
    "\n",
    "print(\"RAPIDS & required libraries loaded successfully!\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Cleaning and Preprocessing\n",
    "\n",
    "Loads and process multi-sheet Excel data\n",
    "\n",
    "1. **File Loading**: Reads all sheets from `2022_vitals.xlsx` without headers.\n",
    "2. **Column Naming**: Assigns predefined column names for consistency.\n",
    "3. **Data Alignment**: \n",
    "   - Fixes misaligned rows by detecting valid `deviceID`.\n",
    "   - Ensures all rows have the correct number of columns.\n",
    "4. **Filtering**:\n",
    "   - Removes invalid or duplicate header rows.\n",
    "   - Drops rows with zero values for latitude (`Lat`) and longitude (`Log`).\n",
    "5. **Indexing**: Resets the index and assigns a sequential 1-based index.\n",
    "6. **Output**: Saves the cleaned data to `2022_vitals_cleaned.xlsx` and previews it."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Mount the local data directory to Docker;\n",
    "docker run -it --gpus all --shm-size=1g --ulimit memlock=-1 --ulimit stack=67108864 -p 8888:8888 `\n",
    "    -v \"C:\\Users\\mayar\\OneDrive - Massachusetts Institute of Technology\\Desktop\\energy-aware\\DATA:/workspace/data\" `\n",
    "    rapids-custom-container\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adjusted file path for Docker (mounted volume)\n",
    "file_path = \"/workspace/data/2022_vitals.xlsx\"\n",
    "output_path = \"/workspace/data/2022_vitals_cleaned.xlsx\"\n",
    "\n",
    "# Specify the column names explicitly\n",
    "column_names = [\n",
    "    \"deviceID\", \"Timestamp\", \"Lat\", \"Log\", \"SOC_batt\", \"temp_batt\", \"volatge_batt\",\n",
    "    \"voltage_particle\", \"current_batt\", \"isCharging\", \"isCharginS\", \"isCharged\",\n",
    "    \"Temp_int\", \"Hum_int\", \"solar_current\", \"Cellular_signal_strength\", \"index\"\n",
    "]\n",
    "\n",
    "# Load all sheets into a dictionary\n",
    "sheets_dict = pd.read_excel(file_path, sheet_name=None, header=None)  # No header initially\n",
    "\n",
    "# Process each sheet\n",
    "processed_sheets = []\n",
    "for sheet_name, sheet_data in sheets_dict.items():\n",
    "    # Ensure the number of columns matches the expected number\n",
    "    sheet_data = sheet_data.iloc[:, :len(column_names)]\n",
    "\n",
    "    # Fix misaligned rows where the first column is invalid\n",
    "    def fix_alignment(row):\n",
    "        # Convert the row to a list\n",
    "        row_list = row.tolist()\n",
    "\n",
    "        # Find the first valid `deviceID` (assumes valid `deviceID` has > 5 characters)\n",
    "        for i, value in enumerate(row_list):\n",
    "            if isinstance(value, str) and len(value) > 5:  # Valid `deviceID` found\n",
    "                # Align the row starting from the valid `deviceID`\n",
    "                aligned_row = row_list[i:i + len(column_names)]\n",
    "                # Ensure the row is padded or trimmed to match `column_names`\n",
    "                return aligned_row + [None] * (len(column_names) - len(aligned_row))\n",
    "\n",
    "        # If no valid `deviceID` found, return row of NaN\n",
    "        return [None] * len(column_names)\n",
    "\n",
    "    # Apply alignment fix to all rows\n",
    "    sheet_data = sheet_data.apply(fix_alignment, axis=1, result_type=\"expand\")\n",
    "    \n",
    "    # Assign column names\n",
    "    sheet_data.columns = column_names\n",
    "\n",
    "    # Drop rows where 'deviceID' is still invalid or starts with \"deviceID\"\n",
    "    sheet_data = sheet_data[sheet_data['deviceID'].notna()]\n",
    "    sheet_data = sheet_data[sheet_data['deviceID'] != \"deviceID\"]  # Remove rows starting with \"deviceID\"\n",
    "\n",
    "    # Append processed sheet\n",
    "    processed_sheets.append(sheet_data)\n",
    "\n",
    "# Concatenate all sheets into one DataFrame\n",
    "df = pd.concat(processed_sheets, ignore_index=True)\n",
    "\n",
    "# Drop rows where Lat or Log is 0\n",
    "df = df[(df['Lat'] != 0) & (df['Log'] != 0)]\n",
    "\n",
    "# Correct indexing column to start at 1 and increment sequentially\n",
    "df.reset_index(drop=True, inplace=True)  # Reset pandas index\n",
    "df['index'] = df.index + 1  # Create a 1-based index\n",
    "\n",
    "# Save cleaned data back to Excel\n",
    "df.to_excel(output_path, index=False)\n",
    "\n",
    "# Print cleaned data preview\n",
    "print(\"Data cleaning completed. Saved to:\", output_path)\n",
    "print(df.head())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Spatiotemporal Binning and Stationary Period Detection**\n",
    "\n",
    "This enables **spatial binning**, **stationary period detection**, and **temporal filtering** for robust movement analysis.\n",
    "\n",
    "## **1. Timestamp Conversion**\n",
    "The Unix timestamp $ T_i $ is converted into a standard datetime format:\n",
    "\n",
    "$$\n",
    "T_i^{\\text{datetime}} = T_i^{\\text{unix}} \\times \\frac{1}{86400} + \\text{epoch}\n",
    "$$\n",
    "\n",
    "where:\n",
    "- $ T_i^{\\text{unix}} $ is the raw Unix timestamp in **seconds**,\n",
    "- $ 86400 $ seconds = **1 day**,\n",
    "- **Epoch** is the reference starting time (January 1, 1970).\n",
    "\n",
    "## **2. Ensuring Numeric Latitude and Longitude**\n",
    "We enforce that latitude ($ \\text{Lat} $) and longitude ($ \\text{Log} $) are real-valued:\n",
    "\n",
    "$$\n",
    "\\text{Lat}, \\text{Log} \\in \\mathbb{R}\n",
    "$$\n",
    "\n",
    "Non-numeric values are coerced to **NaN**.\n",
    "\n",
    "## **3. Discretization into a 40m Grid**\n",
    "### **3.1 Latitude Grid Resolution**\n",
    "Since the **Earth's meridional circumference** is approximately **40,030 km**, the degree-to-meter conversion near the equator is:\n",
    "\n",
    "$$\n",
    "1^\\circ \\approx 111,320 \\text{ meters}\n",
    "$$\n",
    "\n",
    "Thus, the spatial resolution of a **40m grid** in latitude is:\n",
    "\n",
    "$$\n",
    "\\Delta \\text{Lat} = \\frac{120}{111320}\n",
    "$$\n",
    "\n",
    "The **grid-aligned latitude** is computed as:\n",
    "\n",
    "$$\n",
    "\\text{Lat\\_Grid} = \\left\\lfloor \\frac{\\text{Lat}}{\\Delta \\text{Lat}} \\right\\rfloor \\times \\Delta \\text{Lat}\n",
    "$$\n",
    "\n",
    "### **3.2 Longitude Grid Resolution**\n",
    "Unlike latitude, **longitude spacing** varies with latitude due to Earth’s curvature. The **longitude degree-to-meter conversion** is:\n",
    "\n",
    "$$\n",
    "1^\\circ \\approx 111320 \\times \\cos(\\text{Lat})\n",
    "$$\n",
    "\n",
    "Thus, the **longitude resolution** at a given latitude is:\n",
    "\n",
    "$$\n",
    "\\Delta \\text{Log} = \\frac{40}{111320 \\cos(\\text{Lat})}\n",
    "$$\n",
    "\n",
    "The **grid-aligned longitude** is computed as:\n",
    "\n",
    "$$\n",
    "\\text{Log\\_Grid} = \\left\\lfloor \\frac{\\text{Log}}{\\Delta \\text{Log}} \\right\\rfloor \\times \\Delta \\text{Log}\n",
    "$$\n",
    "\n",
    "## **4. Sorting by Time and Device**\n",
    "To track movement **chronologically** for each vehicle, we sort:\n",
    "\n",
    "$$\n",
    "\\text{Sort}(df, \\text{by}=[\\text{deviceID}, \\text{Timestamp}])\n",
    "$$\n",
    "\n",
    "## **5. Identifying Stationary Periods**\n",
    "For each vehicle, we determine if it remained in the same grid cell over consecutive timestamps:\n",
    "\n",
    "$$\n",
    "\\text{Same\\_Grid}_i =\n",
    "\\begin{cases} \n",
    "1, & (\\text{Lat\\_Grid}_i = \\text{Lat\\_Grid}_{i-1}) \\land (\\text{Log\\_Grid}_i = \\text{Log\\_Grid}_{i-1}) \\\\\n",
    "0, & \\text{otherwise}\n",
    "\\end{cases}\n",
    "$$\n",
    "\n",
    "where:\n",
    "- $ \\text{Same\\_Grid}_i = 1 $ means no movement occurred.\n",
    "- $ \\text{Same\\_Grid}_i = 0 $ means movement occurred.\n",
    "\n",
    "## **6. Computing Time Spent in a Grid Cell**\n",
    "The time difference between consecutive records within the same grid is:\n",
    "\n",
    "$$\n",
    "\\Delta t_i = T_i - T_{i-1}\n",
    "$$\n",
    "\n",
    "The **total duration** a vehicle spends within a specific grid cell before moving is:\n",
    "\n",
    "$$\n",
    "\\text{Cumulative\\_Time}_{i} = \\sum_{k=1}^{i} \\Delta t_k\n",
    "$$\n",
    "\n",
    "where:\n",
    "- The summation continues **until movement occurs**.\n",
    "\n",
    "## **7. Assigning a Group ID to Each Stationary Period**\n",
    "A **unique group identifier** is assigned to each stationary period using a cumulative sum:\n",
    "\n",
    "$$\n",
    "\\text{Group}_i =\n",
    "\\sum_{j=1}^{i} (1 - \\text{Same\\_Grid}_j)\n",
    "$$\n",
    "\n",
    "Each transition into a **new grid cell** increments the group ID.\n",
    "\n",
    "## **8. Removing Prolonged Stationary Vehicles**\n",
    "Vehicles remaining in the **same grid for over x hours** (xxx seconds) are excluded:\n",
    "\n",
    "$$\n",
    "\\text{Remove } i \\text{ if } \\text{Cumulative\\_Time}_i \\geq xxx \\text{ sec}\n",
    "$$\n",
    "\n",
    "## **9. Cleanup**\n",
    "All intermediate columns used for calculations are dropped to optimize storage.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 299,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert Unix timestamp to datetime\n",
    "df['Timestamp'] = pd.to_datetime(df['Timestamp'], unit='s')\n",
    "\n",
    "# Ensure 'Lat' and 'Log' are numeric\n",
    "df['Lat'] = pd.to_numeric(df['Lat'], errors='coerce')\n",
    "df['Log'] = pd.to_numeric(df['Log'], errors='coerce')\n",
    "\n",
    "# Spatial Resolution: 40m grid\n",
    "grid_size=40\n",
    "lat_resolution = grid_size / 111320 \n",
    "df['Lat_Grid'] = (df['Lat'] // lat_resolution) * lat_resolution\n",
    "\n",
    "# Longitude resolution depends on latitude\n",
    "df['Lon_Resolution'] = grid_size / (111320 * np.cos(np.radians(df['Lat'])))\n",
    "df['Log_Grid'] = (df['Log'] // df['Lon_Resolution']) * df['Lon_Resolution']\n",
    "\n",
    "# Drop auxiliary column\n",
    "df = df.drop(columns=['Lon_Resolution'])\n",
    "\n",
    "# Step 1: Sort by deviceID and Timestamp\n",
    "df = df.sort_values(by=['deviceID', 'Timestamp'])\n",
    "\n",
    "# Step 2: Detect continuous stationary periods\n",
    "df['Prev_Lat_Grid'] = df.groupby('deviceID')['Lat_Grid'].shift(1)\n",
    "df['Prev_Log_Grid'] = df.groupby('deviceID')['Log_Grid'].shift(1)\n",
    "df['Prev_Timestamp'] = df.groupby('deviceID')['Timestamp'].shift(1)\n",
    "\n",
    "# Step 3: Identify whether the taxi has stayed in the same grid\n",
    "df['Same_Grid'] = (df['Lat_Grid'] == df['Prev_Lat_Grid']) & (df['Log_Grid'] == df['Prev_Log_Grid'])\n",
    "\n",
    "# Step 4: Compute time spent in the grid continuously\n",
    "df['Time_Diff'] = (df['Timestamp'] - df['Prev_Timestamp']).dt.total_seconds()\n",
    "\n",
    "# Step 5: Assign a group ID that resets when the taxi leaves a grid\n",
    "df['Group'] = (~df['Same_Grid']).cumsum()\n",
    "\n",
    "# Step 6: Compute total time spent in each visit to the grid\n",
    "df['Cumulative_Time'] = df.groupby(['deviceID', 'Lat_Grid', 'Log_Grid', 'Group'])['Time_Diff'].cumsum()\n",
    "\n",
    "# Ensure 'Cumulative_Time' is not NaN or negative (if any filtering was done previously)\n",
    "df_filtered = df[df['Cumulative_Time'] > 0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Identifying Optimal Threshold for Cumulative Time in Grids**\n",
    "\n",
    "## **1. Calculate Empirical Cumulative Distribution Function (ECDF)**\n",
    "The ECDF is calculated to provide insight into the distribution of cumulative time spent in each grid. The sorted cumulative times and corresponding ECDF values are computed as:\n",
    "\n",
    "$\n",
    "\\text{sorted\\_times} = \\text{np.sort}(\\text{cumulative\\_time\\_hours})\n",
    "$\n",
    "\n",
    "$\n",
    "\\text{ecdf\\_values} = \\frac{i}{N}, \\quad i = 1, 2, \\ldots, N\n",
    "$\n",
    "\n",
    "Where $N$ is the total number of data points.\n",
    "\n",
    "## **2. Optimal Threshold Detection Using Kneedle Algorithm**\n",
    "The **Kneedle Algorithm** is applied to find the threshold point where the slope of the ECDF curve changes most significantly. This point is considered the optimal threshold that separates short-term and long-term grid occupations.\n",
    "\n",
    "$\n",
    "\\text{kneedle} = \\text{KneeLocator}(\\text{sorted\\_times}, \\text{ecdf\\_values}, S=1.0, \\text{curve}=\"concave\", \\text{direction}=\"increasing\")\n",
    "$\n",
    "\n",
    "The detected knee point (optimal threshold) is converted back to seconds:\n",
    "\n",
    "$\n",
    "\\text{optimal\\_threshold\\_seconds} = \\text{optimal\\_threshold\\_hours} \\times 3600\n",
    "$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 300,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert Cumulative_Time to hours for easier interpretation\n",
    "cumulative_time_hours = df_filtered['Cumulative_Time'] / 3600\n",
    "\n",
    "# Calculate ECDF values for analysis\n",
    "sorted_times = np.sort(cumulative_time_hours)\n",
    "ecdf_values = np.arange(1, len(sorted_times) + 1) / len(sorted_times)\n",
    "\n",
    "# Find the optimal threshold using the Kneedle algorithm\n",
    "kneedle = KneeLocator(sorted_times, ecdf_values, S=1.0, curve=\"concave\", direction=\"increasing\")\n",
    "optimal_threshold_hours = kneedle.knee\n",
    "optimal_threshold_seconds = optimal_threshold_hours * 3600\n",
    "\n",
    "# Plot 1: Histogram with Optimal Threshold\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.hist(cumulative_time_hours, bins=100, color='skyblue', label='Cumulative Time Distribution')\n",
    "plt.axvline(optimal_threshold_hours, color='red', linestyle='--', label=f'Optimal Threshold = {optimal_threshold_hours:.2f} hours')\n",
    "plt.title('Distribution of Cumulative Time Spent in Each Grid (With Optimal Threshold)')\n",
    "plt.xlabel('Cumulative Time Spent in Grid (hours)')\n",
    "plt.ylabel('Frequency')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()\n",
    "\n",
    "# Plot 2: ECDF with Optimal Threshold\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.ecdfplot(cumulative_time_hours, label='ECDF of Cumulative Time')\n",
    "plt.axvline(optimal_threshold_hours, color='red', linestyle='--', label=f'Optimal Threshold = {optimal_threshold_hours:.2f} hours')\n",
    "plt.title('ECDF of Cumulative Time Spent in Each Grid (With Optimal Threshold)')\n",
    "plt.xlabel('Cumulative Time Spent in Grid (hours)')\n",
    "plt.ylabel('Proportion of Data Points')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()\n",
    "\n",
    "# Display the optimal threshold\n",
    "print(f\"Optimal Threshold Found: {optimal_threshold_hours:.2f} hours ({optimal_threshold_seconds:.0f} seconds)\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "original_entry_count = len(df)\n",
    "\n",
    "# Step 7: Remove vehicles that stayed continuously in the same grid for more than optimal_threshold_seconds\n",
    "df = df[~(df['Cumulative_Time'] >= optimal_threshold_seconds)]\n",
    "\n",
    "removed_entries = original_entry_count - len(df)\n",
    "\n",
    "# Drop helper columns\n",
    "df = df.drop(columns=['Prev_Lat_Grid', 'Prev_Log_Grid', 'Prev_Timestamp', 'Same_Grid', 'Time_Diff', 'Group', 'Cumulative_Time'])\n",
    "\n",
    "# Print the number of removed entries\n",
    "print(f\"Number of entries removed due to vehicles staying continuously in the same grid for more than {optimal_threshold_hours} hours: {removed_entries}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Spatial Grid Aggregation\n",
    "\n",
    "This script generates a **40m x 40m geospatial grid** and counts the number of data points within each grid cell:\n",
    "\n",
    "1. **Dynamic Boundary Definition**: \n",
    "   - Extracts min/max latitude and longitude from the dataset.\n",
    "2. **Grid Construction**:\n",
    "   - Defines **40m resolution** for latitude and dynamically calculates longitude resolution.\n",
    "   - Iterates over the spatial extent to generate **polygonal grid cells**.\n",
    "3. **GeoDataFrame Creation**:\n",
    "   - Converts the grid into a `GeoDataFrame` (`grid_gdf`).\n",
    "   - Converts data points into a `GeoDataFrame` (`df_gdf`).\n",
    "4. **Spatial Aggregation**:\n",
    "   - Checks which points fall within each grid cell.\n",
    "   - Increments the count of data points within corresponding grid polygons."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 302,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dynamically define the bounds from the DataFrame\n",
    "min_lat = df['Lat'].min()\n",
    "max_lat = df['Lat'].max()\n",
    "min_lon = df['Log'].min()\n",
    "max_lon = df['Log'].max()\n",
    "\n",
    "lon_resolution_at_lat = lambda lat: grid_size / (111320 * np.cos(np.radians(lat)))\n",
    "\n",
    "# Generate grid of polygons\n",
    "grid = []\n",
    "lat = min_lat\n",
    "while lat < max_lat:\n",
    "    lon = min_lon\n",
    "    while lon < max_lon:\n",
    "        lon_res = lon_resolution_at_lat(lat)\n",
    "        grid.append(Polygon([\n",
    "            (lon, lat),\n",
    "            (lon + lon_res, lat),\n",
    "            (lon + lon_res, lat + lat_resolution),\n",
    "            (lon, lat + lat_resolution)\n",
    "        ]))\n",
    "        lon += lon_res\n",
    "    lat += lat_resolution\n",
    "\n",
    "# Create an empty GeoDataFrame for the grid\n",
    "grid_gdf = gpd.GeoDataFrame({'geometry': grid, 'Count': 0}, crs=\"EPSG:4326\")\n",
    "\n",
    "# Create a GeoDataFrame for the points in df\n",
    "df_gdf = gpd.GeoDataFrame(df, geometry=gpd.points_from_xy(df['Log'], df['Lat']), crs=\"EPSG:4326\")\n",
    "\n",
    "# Perform spatial join (vectorized operation)\n",
    "joined = gpd.sjoin(df_gdf, grid_gdf, how=\"left\", predicate=\"within\")\n",
    "\n",
    "# Count points per grid cell\n",
    "grid_gdf['Count'] = joined.groupby(joined.index_right).size()\n",
    "\n",
    "# Fill NaN with 0 for empty grid cells\n",
    "grid_gdf['Count'].fillna(0, inplace=True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Battery Depletion\n",
    "\n",
    "Identify the **number of unique days** where at least one device’s battery **(SOC_batt)** dropped below **50%**:\n",
    "\n",
    "1. **Extract Date Information**:\n",
    "   - Converts the timestamp to **date-only format**.\n",
    "\n",
    "2. **Filter for Battery Depletion Events**:\n",
    "   - Selects records where `SOC_batt` is below **50%** (can be changed).\n",
    "\n",
    "3. **Count Unique Affected Days**:\n",
    "   - Computes the number of distinct days where a depletion event occurred."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 303,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assuming df is already loaded with the necessary data\n",
    "# Identify unique days where at least one device's SOC_batt dropped below 50%\n",
    "df['Date'] = df['Timestamp'].dt.date  # Extract the date\n",
    "days_with_depletion = df[df['SOC_batt'] < 50]['Date'].nunique()\n",
    "\n",
    "# Display the number of days with a battery drop below 50%\n",
    "days_with_depletion\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **XGBoost-Based SOC Forecasting with Optuna Hyperparameter Optimization**\n",
    "\n",
    "This implementation builds a **data-driven model** for **predicting battery State of Charge (SOC) depletion** in sensor-equipped vehicles. The model:\n",
    "1. **Extracts spatiotemporal and power-related features** from historical sensor data.\n",
    "2. **Trains an XGBoost regressor** to predict future SOC values.\n",
    "3. **Optimizes model hyperparameters** using Bayesian search via **Optuna**.\n",
    "4. **Performs multi-step forecasting**, predicting SOC for the next **seven time steps**.\n",
    "5. **Quantifies predictive uncertainty** and **adjusts dynamic SOC thresholds** for safety monitoring.\n",
    "\n",
    "## **1. Data Preprocessing and Feature Engineering**\n",
    "\n",
    "The dataset contains time-series measurements for multiple vehicles, each identified by a **deviceID**. The dataset is first sorted **chronologically** for each device:\n",
    "\n",
    "$$\n",
    "\\text{Sort}(df, \\text{by}=[\\text{deviceID}, \\text{Timestamp}])\n",
    "$$\n",
    "\n",
    "SOC and related features are converted to **numeric types** to ensure proper mathematical operations:\n",
    "\n",
    "$$\n",
    "X = \\{\\text{SOC\\_batt}, \\text{current\\_batt}, \\text{solar\\_current}, \\text{voltage\\_batt}, \\text{charging\\_status} \\}\n",
    "$$\n",
    "\n",
    "### **Feature Engineering**\n",
    "New predictive features are created, capturing both **short-term trends** and **time-based influences**:\n",
    "\n",
    "- **Hourly time representation:** $ \\text{Hour} = \\text{Timestamp.hour} $, capturing **daily charge-discharge patterns**.\n",
    "- **Lagged values:** Previous SOC and power readings are used as predictors:\n",
    "\n",
    "$$\n",
    "\\text{Prev\\_SOC}_t = \\text{SOC\\_batt}_{t-1}, \\quad \\text{Prev\\_Current}_t = \\text{current\\_batt}_{t-1}\n",
    "$$\n",
    "\n",
    "- **Rolling depletion rate:** Defined as the moving average of SOC depletion over a 5-step window:\n",
    "\n",
    "$$\n",
    "\\text{Depletion\\_Rate}_t = \\frac{1}{5} \\sum_{i=t-4}^{t} (\\text{SOC\\_batt}_i - \\text{SOC\\_batt}_{i-1})\n",
    "$$\n",
    "\n",
    "where $ \\text{Depletion\\_Rate}_t $ estimates **battery discharge trends**.\n",
    "\n",
    "The final **feature matrix** is:\n",
    "\n",
    "$$\n",
    "X = \\{ \\text{Hour}, \\text{Prev\\_SOC}, \\text{Prev\\_Current}, \\text{Prev\\_Solar\\_Current}, \\text{Prev\\_Voltage}, \\text{Prev\\_Charging}, \\text{Depletion\\_Rate} \\}\n",
    "$$\n",
    "\n",
    "and the target variable is:\n",
    "\n",
    "$$\n",
    "y = \\text{SOC\\_batt}\n",
    "$$\n",
    "\n",
    "## **2. Training the XGBoost Model with Bayesian Optimization**\n",
    "XGBoost, a **gradient-boosted tree regressor**, is trained to minimize **SOC prediction error**. Hyperparameter tuning is performed using **Optuna**, a Bayesian optimization framework.\n",
    "\n",
    "### **Optimization Objective**\n",
    "The model is optimized to **minimize the Mean Absolute Error (MAE)**:\n",
    "\n",
    "$$\n",
    "\\mathcal{L} = \\frac{1}{N} \\sum_{i=1}^{N} | y_i - \\hat{y}_i |\n",
    "$$\n",
    "\n",
    "where:\n",
    "- $ y_i $ is the **actual SOC value**,\n",
    "- $ \\hat{y}_i $ is the **predicted SOC**,\n",
    "- $ N $ is the number of test samples.\n",
    "\n",
    "### **Search Space for Hyperparameter Tuning**\n",
    "The following hyperparameters are optimized:\n",
    "- **Number of boosting rounds**: $ n_{\\text{estimators}} \\in [100, 500] $\n",
    "- **Learning rate**: $ \\eta \\in [0.06, 0.12] $ (log-uniform)\n",
    "- **Tree depth**: $ d \\in [4,6] $\n",
    "- **Subsampling ratio**: $ \\text{subsample} \\in [0.5,1.0] $\n",
    "- **Column sampling ratio**: $ \\text{colsample\\_bytree} \\in [0.5,1.0] $\n",
    "- **Regularization parameters**: $ \\lambda, \\alpha \\in [0.001, 10] $\n",
    "\n",
    "The Bayesian search selects hyperparameters that minimize **validation MAE**.\n",
    "\n",
    "## **3. Multi-Step Forecasting**\n",
    "To predict future SOC depletion, the trained model is used iteratively for **seven future time steps**.\n",
    "\n",
    "For each step $ t $, the next SOC is predicted as:\n",
    "\n",
    "$$\n",
    "\\hat{y}_t = f(X_t, \\theta^*)\n",
    "$$\n",
    "\n",
    "where:\n",
    "- $ f $ is the trained **XGBoost model**,\n",
    "- $ X_t $ contains the latest **SOC, charging state, and depletion rate**,\n",
    "- $ \\theta^* $ represents the **optimal hyperparameters**.\n",
    "\n",
    "Each prediction is fed back into the model, allowing **rolling forecasts**:\n",
    "\n",
    "$$\n",
    "X_{t+1} \\gets X_t, \\quad X_{t+1}[\\text{Prev\\_SOC}] = \\hat{y}_t\n",
    "$$\n",
    "\n",
    "ensuring **dynamic simulation of SOC depletion**.\n",
    "\n",
    "## **4. Uncertainty Estimation and Dynamic Thresholding**\n",
    "To ensure **safe operation**, a **dynamic SOC threshold** is computed for each future step, adjusting based on:\n",
    "- **Historical depletion rates**\n",
    "- **Charging behavior**\n",
    "- **Battery degradation uncertainty**\n",
    "\n",
    "A **Bayesian prior** is set on SOC:\n",
    "\n",
    "$$\n",
    "\\mu_{\\text{prior}} = \\mathbb{E}[y_{\\text{test}}], \\quad \\sigma_{\\text{prior}} = \\text{std}(y_{\\text{test}})\n",
    "$$\n",
    "\n",
    "Thresholds are adjusted based on:\n",
    "1. **Failure probability**:\n",
    "\n",
    "$$\n",
    "P_{\\text{failure}} = \\Phi\\left(\\frac{10 - \\mu_{\\text{prior}}}{\\sigma_{\\text{prior}}} \\right)\n",
    "$$\n",
    "\n",
    "where $ \\Phi $ is the **cumulative normal distribution**, modeling the probability of SOC dropping below 10%.\n",
    "\n",
    "2. **Dynamic SOC Threshold Computation**:\n",
    "\n",
    "$$\n",
    "\\text{Safe\\_SOC}_t = 30 + (10 \\cdot P_{\\text{failure}}) + (5 \\cdot \\sigma_t)\n",
    "$$\n",
    "\n",
    "where:\n",
    "- The **base threshold is 30% SOC**.\n",
    "- **Additional margins** are added based on **failure probability** and **rolling standard deviation**.\n",
    "\n",
    "Charging and solar effects further refine the threshold:\n",
    "\n",
    "$$\n",
    "\\text{Safe\\_SOC}_t = \\text{Safe\\_SOC}_t + 5 \\cdot \\mathbb{1}(\\text{Charging}) - 3 \\cdot \\mathbb{1}(\\text{Solar\\_High})\n",
    "$$\n",
    "\n",
    "ensuring **dynamic safety monitoring**.\n",
    "\n",
    "## **5. Evaluation Metrics**\n",
    "Final model performance is evaluated using:\n",
    "- **Mean Absolute Error (MAE)**:\n",
    "\n",
    "$$\n",
    "\\text{MAE} = \\frac{1}{N} \\sum_{i=1}^{N} | y_i - \\hat{y}_i |\n",
    "$$\n",
    "\n",
    "- **Root Mean Squared Error (RMSE)**:\n",
    "\n",
    "$$\n",
    "\\text{RMSE} = \\sqrt{\\frac{1}{N} \\sum_{i=1}^{N} (y_i - \\hat{y}_i)^2}\n",
    "$$\n",
    "\n",
    "- **Symmetric Mean Absolute Percentage Error (SMAPE)**:\n",
    "\n",
    "$$\n",
    "\\text{SMAPE} = 100 \\cdot \\frac{1}{N} \\sum_{i=1}^{N} \\frac{| y_i - \\hat{y}_i |}{( | y_i | + | \\hat{y}_i | ) / 2}\n",
    "$$\n",
    "\n",
    "Residual analysis confirms the **distribution of errors**, and rolling error plots show **stability over time**.\n",
    "\n",
    "## **6. Feature Importance Analysis Using XGBoost**\n",
    "\n",
    "Understanding **feature importance** is crucial in machine learning models to **interpret predictive behavior** and **assess model reliability**. In this study, we employ **XGBoost’s feature importance analysis** to quantify the impact of each input variable on **SOC (State of Charge) prediction**.\n",
    "\n",
    "XGBoost provides an **intrinsic feature ranking mechanism**, which assigns **importance scores** to each predictor based on how frequently and effectively it contributes to **minimizing prediction error**.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load and preprocess data\n",
    "df_threshold = df.copy()\n",
    "\n",
    "# Sort and prepare dataset\n",
    "df_threshold = df_threshold.sort_values(by=['deviceID', 'Timestamp'])\n",
    "df_threshold['Timestamp'] = pd.to_datetime(df_threshold['Timestamp'])\n",
    "df_threshold.set_index('Timestamp', inplace=True)\n",
    "\n",
    "# Select a single device\n",
    "device_id = df_threshold['deviceID'].unique()[0]\n",
    "df_device = df_threshold[df_threshold['deviceID'] == device_id].copy()\n",
    "\n",
    "# Convert SOC and related features to numeric\n",
    "for col in ['SOC_batt', 'current_batt', 'solar_current', 'isCharginS', 'volatge_batt', 'isCharging']:\n",
    "    df_device[col] = pd.to_numeric(df_device[col], errors='coerce')\n",
    "\n",
    "df_device.dropna(inplace=True)\n",
    "\n",
    "# Feature Engineering\n",
    "df_device['Hour'] = df_device.index.hour\n",
    "df_device['Prev_SOC'] = df_device['SOC_batt'].shift(1)\n",
    "df_device['Prev_Current'] = df_device['current_batt'].shift(1)\n",
    "df_device['Prev_Solar_Current'] = df_device['solar_current'].shift(1)\n",
    "df_device['Prev_Solar'] = df_device['isCharginS'].shift(1)\n",
    "df_device['Prev_Voltage'] = df_device['volatge_batt'].shift(1)\n",
    "df_device['Prev_Charging'] = df_device['isCharging'].shift(1)\n",
    "\n",
    "# Compute rolling depletion rate\n",
    "df_device['Depletion_Rate'] = df_device['SOC_batt'].diff().rolling(window=5).mean()\n",
    "df_device['Depletion_Rate'].fillna(0, inplace=True)\n",
    "\n",
    "df_device.dropna(inplace=True)\n",
    "\n",
    "# Define input features and target\n",
    "features = ['Hour', 'Prev_SOC', 'Prev_Current', 'Prev_Solar_Current', 'Prev_Solar', 'Prev_Voltage', 'Prev_Charging', 'Depletion_Rate']\n",
    "X = df_device[features]\n",
    "y = df_device['SOC_batt']\n",
    "\n",
    "# Train-test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, shuffle=False)\n",
    "\n",
    "# Hyperparameter Tuning with Optuna\n",
    "def objective(trial):\n",
    "    params = {\n",
    "        'objective': 'reg:squarederror',\n",
    "        'n_estimators': trial.suggest_int('n_estimators', 100, 500),\n",
    "        'learning_rate': trial.suggest_float('learning_rate', 0.07, 0.12, log=True),  # Narrow range\n",
    "        'max_depth': trial.suggest_int('max_depth', 4, 6),  # Prevent deep overfitting\n",
    "        'subsample': trial.suggest_float('subsample', 0.5, 1.0),\n",
    "        'colsample_bytree': trial.suggest_float('colsample_bytree', 0.5, 1.0),\n",
    "        'gamma': trial.suggest_float('gamma', 0.01, 1.0, log=True),\n",
    "        'reg_alpha': trial.suggest_float('reg_alpha', 0.001, 10, log=True),\n",
    "        'reg_lambda': trial.suggest_float('reg_lambda', 0.001, 10, log=True)\n",
    "    }\n",
    "    model = xgb.XGBRegressor(**params)\n",
    "    model.fit(X_train, y_train)\n",
    "    y_pred = model.predict(X_test)\n",
    "    return mean_absolute_error(y_test, y_pred)\n",
    "\n",
    "# Run Optuna optimization\n",
    "# Define total trials\n",
    "N_TRIALS = 400\n",
    "STARTUP_TRIALS = 10\n",
    "\n",
    "# Suppress excessive Optuna logging\n",
    "optuna.logging.set_verbosity(optuna.logging.WARNING)  # Show only important messages\n",
    "\n",
    "# Initialize tqdm progress bar\n",
    "pbar = tqdm(total=N_TRIALS, desc=\"Optimization Progress\", position=0, leave=False, dynamic_ncols=True)\n",
    "\n",
    "# Callback function to update progress smoothly\n",
    "def progress_callback(study, trial):\n",
    "    pbar.update(1)  # Increment progress bar by 1\n",
    "    if study.best_value is not None:\n",
    "        pbar.set_postfix({\"Best MAE\": f\"{study.best_value:.4f}\"})  # Update in progress bar instead of printing\n",
    "\n",
    "# Run Optuna optimization\n",
    "study = optuna.create_study(\n",
    "    sampler=optuna.samplers.TPESampler(n_startup_trials=STARTUP_TRIALS),\n",
    "    direction='minimize'\n",
    ")\n",
    "study.optimize(objective, n_trials=N_TRIALS, callbacks=[progress_callback])\n",
    "\n",
    "# Close tqdm progress bar after completion\n",
    "pbar.close()\n",
    "\n",
    "# Train final model with best parameters\n",
    "best_params = study.best_params\n",
    "xgb_model = xgb.XGBRegressor(**best_params)\n",
    "xgb_model.fit(X_train, y_train)\n",
    "\n",
    "# Predict SOC\n",
    "y_pred = xgb_model.predict(X_test)\n",
    "\n",
    "# Multi-Step Forecasting (Predict Next 7 Steps)\n",
    "future_steps = 7\n",
    "X_future = X_test.copy()  # Copy of the last known test input\n",
    "predictions = np.zeros((len(X_test), future_steps))  # Shape: (test_size, future_steps)\n",
    "dynamic_thresholds = np.zeros((len(X_test), future_steps))  # Shape: (test_size, future_steps)\n",
    "\n",
    "# Initialize Bayesian prior for SOC distribution\n",
    "prior_mean = np.mean(y_test)  # Initial mean SOC\n",
    "prior_std = np.std(y_test)  # Initial standard deviation\n",
    "\n",
    "# Precompute failure probability for efficiency\n",
    "failure_prob_cache = stats.norm.cdf(10, loc=prior_mean, scale=prior_std)\n",
    "\n",
    "for step in range(future_steps):\n",
    "    y_future = xgb_model.predict(X_future)  # Predict future SOC\n",
    "    predictions[:, step] = y_future  # Store predictions\n",
    "\n",
    "    # Vectorized Depletion Rate Calculation\n",
    "    if step >= 5:\n",
    "        depletion_factor = np.mean(np.diff(predictions[:, max(0, step-5):step]), axis=1)\n",
    "    else:\n",
    "        depletion_factor = np.mean(np.diff(predictions[:, :step+1]), axis=1) if step > 0 else np.zeros(len(y_future))\n",
    "\n",
    "    # Rolling Standard Deviation Instead of Monte Carlo Simulations\n",
    "    prediction_uncertainty = np.std(predictions[:, max(0, step-5):step], axis=1)\n",
    "\n",
    "    # Dynamic Threshold Adjustment (Vectorized)\n",
    "    safe_soc = 30 + (10 * failure_prob_cache) + (5 * prediction_uncertainty)\n",
    "\n",
    "    # Charging & Solar Adjustments\n",
    "    safe_soc += (X_future['Prev_Charging'].values * 5)  # Add 5% if charging\n",
    "    safe_soc -= (X_future['Prev_Solar_Current'].values > 0) * 3  # Reduce by 3% if solar is strong\n",
    "\n",
    "    # Apply Bounds\n",
    "    safe_soc = np.clip(safe_soc, 10, 100)  # Keep in range\n",
    "    dynamic_thresholds[:, step] = safe_soc  # Store computed thresholds\n",
    "\n",
    "    # Update Prior for Next Step\n",
    "    prior_mean = np.mean(y_future)\n",
    "    prior_std = np.std(y_future)\n",
    "\n",
    "    # Update X_future for next step\n",
    "    X_future = X_future.copy()\n",
    "    X_future['Prev_SOC'] = y_future  # Use predicted SOC as input for next step\n",
    "\n",
    "\n",
    "# Compute Evaluation Metrics\n",
    "mae = mean_absolute_error(y_test, y_pred)\n",
    "rmse = np.sqrt(mean_squared_error(y_test, y_pred))\n",
    "mape = np.mean(np.abs((y_test - y_pred) / y_test)) * 100\n",
    "\n",
    "# Function to Compute SMAPE\n",
    "def smape(y_true, y_pred):\n",
    "    return 100 * np.mean(np.abs(y_pred - y_true) / ((np.abs(y_true) + np.abs(y_pred)) / 2))\n",
    "\n",
    "smape_score = smape(y_test, y_pred)\n",
    "\n",
    "print(f\"Validation Metrics:\")\n",
    "print(f\"Mean Absolute Error (MAE): {mae:.2f}%\")\n",
    "print(f\"Root Mean Square Error (RMSE): {rmse:.2f}%\")\n",
    "print(f\"Mean Absolute Percentage Error (MAPE): {mape:.2f}%\")\n",
    "print(f\"Symmetric Mean Absolute Percentage Error (SMAPE): {smape_score:.2f}%\")\n",
    "\n",
    "# Residual Analysis\n",
    "residuals = y_test - y_pred\n",
    "plt.figure(figsize=(12, 6))\n",
    "sns.histplot(residuals, bins=50, kde=True, color=\"purple\")\n",
    "plt.axvline(residuals.mean(), color='red', linestyle='dashed', label=f\"Mean Residual: {residuals.mean():.2f}\")\n",
    "plt.title(\"Residual Distribution (y_test - y_pred)\")\n",
    "plt.xlabel(\"Residual\")\n",
    "plt.ylabel(\"Frequency\")\n",
    "plt.legend()\n",
    "plt.grid()\n",
    "plt.show()\n",
    "\n",
    "# Rolling Error Analysis (50-step window)\n",
    "rolling_window = 50\n",
    "rolling_mae = np.convolve(np.abs(residuals), np.ones(rolling_window)/rolling_window, mode='valid')\n",
    "rolling_rmse = np.convolve(np.square(residuals), np.ones(rolling_window)/rolling_window, mode='valid')\n",
    "rolling_rmse = np.sqrt(rolling_rmse)\n",
    "\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.plot(y_test.index[rolling_window-1:], rolling_mae, label=\"Rolling MAE\", color='blue')\n",
    "plt.plot(y_test.index[rolling_window-1:], rolling_rmse, label=\"Rolling RMSE\", color='orange')\n",
    "plt.title(\"Rolling Error Analysis (MAE & RMSE Over Time)\")\n",
    "plt.xlabel(\"Time\")\n",
    "plt.ylabel(\"Error\")\n",
    "plt.legend()\n",
    "plt.grid()\n",
    "plt.show()\n",
    "\n",
    "# Extract trials and corresponding objective values\n",
    "trials = [t.number for t in study.trials if t.state == optuna.trial.TrialState.COMPLETE]\n",
    "values = [t.value for t in study.trials if t.state == optuna.trial.TrialState.COMPLETE]\n",
    "\n",
    "# Best value tracking\n",
    "best_values = [min(values[:i+1]) for i in range(len(values))]\n",
    "\n",
    "# Compute percentage improvement\n",
    "initial_value = values[0]  # First trial\n",
    "final_value = min(values)  # Best trial\n",
    "\n",
    "# Convergence Plot\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.plot(trials, values, marker='o', linestyle='-', color='b', label=\"MAE per trial\")\n",
    "plt.plot(trials, best_values, marker='o', linestyle='-', color='g', label=\"Best MAE found\")\n",
    "\n",
    "# Add a percentage bar\n",
    "plt.xlabel(\"Trial Number\")\n",
    "plt.ylabel(\"Objective Value (MAE)\")\n",
    "plt.title(\"Optuna Optimization History (Convergence Plot)\")\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()\n",
    "\n",
    "fig1 = optuna.visualization.plot_optimization_history(study)\n",
    "fig1.show()\n",
    "\n",
    "fig2 = optuna.visualization.plot_param_importances(study)\n",
    "fig2.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Plot Actual SOC (Train & Test) & Predictions (Only for Test)\n",
    "\n",
    "ax[0].plot(y_train.index, y_train, label=\"Actual SOC (Train)\", color='blue', linestyle=\"dashed\", alpha=0.7)  # Show only actual values for train\n",
    "ax[0].plot(y_test.index, y_test, label=\"Actual SOC (Test)\", color='black', linestyle=\"dashed\")\n",
    "ax[0].plot(y_test.index, y_pred, label=\"Predicted SOC (Test)\", color='red')\n",
    "\n",
    "ax[0].set_title(f\"Optimized XGBoost SOC Prediction vs Actual SOC (Train & Test)\")\n",
    "ax[0].set_ylabel(\"SOC Battery Level (%)\")\n",
    "ax[0].legend(loc=\"upper left\", bbox_to_anchor=(1, 1))  # Legend outside\n",
    "ax[0].grid(True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Identify Gaps Greater Than 1 Day in Training & Test Data\n",
    "gap_threshold = pd.Timedelta(days=1)\n",
    "\n",
    "# Find gaps in training data\n",
    "time_gaps_train = y_train.index.to_series().diff() > gap_threshold\n",
    "train_segment_indices = np.where(time_gaps_train)[0]\n",
    "\n",
    "# Find gaps in test data\n",
    "time_gaps_test = y_test.index.to_series().diff() > gap_threshold\n",
    "test_segment_indices = np.where(time_gaps_test)[0]\n",
    "\n",
    "# Copy actual values to introduce NaNs where gaps exist (preserving gaps only for actual SOC)\n",
    "y_train_gapfree = y_train.copy()\n",
    "y_test_gapfree = y_test.copy()\n",
    "\n",
    "y_train_gapfree.iloc[train_segment_indices] = np.nan\n",
    "y_test_gapfree.iloc[test_segment_indices] = np.nan\n",
    "\n",
    "# Clip predictions to a maximum of 100%\n",
    "y_pred_clipped = np.clip(y_pred, 0, 100)  # Ensures predictions stay between 0% and 100%\n",
    "predictions_clipped = np.clip(predictions, 0, 100)  # Multi-step forecast predictions clipped\n",
    "\n",
    "# Create subplots\n",
    "fig, ax = plt.subplots(2, 1, figsize=(12, 10), sharex=False)\n",
    "\n",
    "# Plot Actual SOC (Train & Test) with Gaps & Predictions (No Gaps, Clipped at 100%)\n",
    "ax[0].plot(y_train.index, y_train_gapfree, label=\"Actual SOC (Train)\", color='blue', alpha=0.7, linewidth=1)  # Gaps in train\n",
    "ax[0].plot(y_test.index, y_test_gapfree, label=\"Actual SOC (Test)\", color='blue', linestyle=\"dashed\", alpha=0.7, linewidth=1)  # Gaps in test\n",
    "ax[0].plot(y_test.index, y_pred_clipped, label=\"Predicted SOC (Test)\", color='red', linestyle=\"dashed\", alpha=0.7, linewidth=1)  # Predictions remain continuous but clipped\n",
    "\n",
    "ax[0].set_title(\"Optimized XGBoost SOC Prediction vs Actual SOC (Train & Test)\")\n",
    "ax[0].set_ylabel(\"SOC Battery Level (%)\")\n",
    "ax[0].legend(loc=\"upper left\", bbox_to_anchor=(1, 1))  # Legend outside\n",
    "ax[0].grid(True)\n",
    "\n",
    "# Multi-Step Forecasting & Safe SOC Threshold (Clipped at 100%)\n",
    "for i in range(future_steps):\n",
    "    ax[1].plot(y_test.index[:len(predictions_clipped[:, i])], predictions_clipped[:, i], linestyle=\"dotted\", linewidth=1, alpha=0.7, label=f\"Step {i+1} Forecast\")\n",
    "\n",
    "ax[1].plot(y_test.index[:len(dynamic_thresholds[:, i])], dynamic_thresholds[:, -1], linestyle=\"solid\", color='green', alpha=0.8, label=\"Safe SOC Threshold (Final Step)\")\n",
    "\n",
    "ax[1].set_title(\"Multi-Step SOC Forecast with Dynamic Safe Thresholds\")\n",
    "ax[1].set_xlabel(\"Time\")\n",
    "ax[1].set_ylabel(\"SOC Battery Level (%)\")\n",
    "ax[1].legend(loc=\"upper left\", bbox_to_anchor=(1, 1))  # Legend outside\n",
    "ax[1].grid(True)\n",
    "\n",
    "# Adjust layout\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 306,
   "metadata": {},
   "outputs": [],
   "source": [
    "# **Step 1: Retrain Model on Full Dataset**\n",
    "X_full = df_device[features]\n",
    "y_full = df_device['SOC_batt']\n",
    "\n",
    "xgb_model_full = xgb.XGBRegressor(**best_params)\n",
    "xgb_model_full.fit(X_full, y_full)  # Train on full dataset\n",
    "\n",
    "# **Step 2: Predict SOC for All Data**\n",
    "y_pred_full = xgb_model_full.predict(X_full)\n",
    "\n",
    "# **Step 3: Compute Dynamic Safe SOC Threshold for All Data**\n",
    "prior_mean = np.mean(y_full)\n",
    "prior_std = np.std(y_full)\n",
    "\n",
    "failure_prob_cache = stats.norm.cdf(10, loc=prior_mean, scale=prior_std)\n",
    "\n",
    "safe_soc_thresholds = np.zeros(len(y_full))\n",
    "\n",
    "for i in range(len(y_pred_full)):\n",
    "    depletion_factor = np.mean(np.diff(y_pred_full[max(0, i-5):i])) if i >= 5 else 0\n",
    "    prediction_uncertainty = np.std(y_pred_full[max(0, i-5):i]) if i >= 5 else 0\n",
    "\n",
    "    safe_soc = 30 + (10 * failure_prob_cache) + (5 * prediction_uncertainty)\n",
    "    safe_soc += (X_full['Prev_Charging'].iloc[i] * 5)\n",
    "    safe_soc -= (X_full['Prev_Solar_Current'].iloc[i] > 0) * 3\n",
    "    safe_soc = np.clip(safe_soc, 15, 45)\n",
    "    \n",
    "    safe_soc_thresholds[i] = safe_soc\n",
    "\n",
    "# **Step 4: Identify Gaps Greater Than 1 Day**\n",
    "gap_threshold = pd.Timedelta(days=1)\n",
    "time_gaps = df_device.index.to_series().diff() > gap_threshold  # Find where gaps are > 3 days\n",
    "segment_indices = np.where(time_gaps)[0]  # Indices where gaps exist\n",
    "\n",
    "# **Step 5: Plot Results with Unlinked Segments**\n",
    "plt.figure(figsize=(12, 6))\n",
    "\n",
    "# Track first segment to add legend only once\n",
    "first_segment = True\n",
    "\n",
    "# Loop through segments and plot separately\n",
    "start_idx = 0\n",
    "for idx in segment_indices:\n",
    "    if first_segment:\n",
    "        plt.plot(df_device.index[start_idx:idx], y_full[start_idx:idx], label=\"Actual SOC\", color='black', linestyle=\"dashed\")\n",
    "        plt.plot(df_device.index[start_idx:idx], y_pred_full[start_idx:idx], label=\"Predicted SOC\", color='red')\n",
    "        plt.plot(df_device.index[start_idx:idx], safe_soc_thresholds[start_idx:idx], linestyle=\"dashdot\", color='blue', label=\"Safe SOC Threshold\")\n",
    "        first_segment = False  # After first plot, disable legend labels\n",
    "    else:\n",
    "        plt.plot(df_device.index[start_idx:idx], y_full[start_idx:idx], color='black', linestyle=\"dashed\")\n",
    "        plt.plot(df_device.index[start_idx:idx], y_pred_full[start_idx:idx], color='red')\n",
    "        plt.plot(df_device.index[start_idx:idx], safe_soc_thresholds[start_idx:idx], linestyle=\"dashdot\", color='blue')\n",
    "\n",
    "    start_idx = idx  # Move start index to the next segment\n",
    "\n",
    "# Plot the last segment\n",
    "plt.plot(df_device.index[start_idx:], y_full[start_idx:], color='black', linestyle=\"dashed\")\n",
    "plt.plot(df_device.index[start_idx:], y_pred_full[start_idx:], color='red')\n",
    "plt.plot(df_device.index[start_idx:], safe_soc_thresholds[start_idx:], linestyle=\"dashdot\", color='blue')\n",
    "\n",
    "plt.xlabel(\"Time\")\n",
    "plt.ylabel(\"SOC Battery Level (%)\")\n",
    "plt.title(\"Deployed Model: SOC Prediction & Safe Threshold Over Time (Unlinked Gaps > 1 Day)\")\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Load and preprocess data for all devices\n",
    "df_threshold = df.copy()\n",
    "\n",
    "# Sort and prepare dataset\n",
    "df_threshold = df_threshold.sort_values(by=['deviceID', 'Timestamp'])\n",
    "df_threshold.set_index('Timestamp', inplace=True)\n",
    "\n",
    "# Step 2: Initialize List for Safe SOC Thresholds**\n",
    "all_safe_soc_thresholds = []\n",
    "\n",
    "# Step 3: Loop over each device\n",
    "for device_id in df_threshold['deviceID'].unique():\n",
    "    print(f\"\\n🔹 Processing Device ID: {device_id}\")\n",
    "\n",
    "    # **Extract data for the current device**\n",
    "    df_device = df_threshold[df_threshold['deviceID'] == device_id].copy()\n",
    "\n",
    "    # Convert SOC and related features to numeric\n",
    "    for col in ['SOC_batt', 'current_batt', 'solar_current', 'isCharginS', 'volatge_batt', 'isCharging']:\n",
    "        df_device[col] = pd.to_numeric(df_device[col], errors='coerce')\n",
    "\n",
    "    df_device.dropna(inplace=True)\n",
    "\n",
    "    # **Feature Engineering**\n",
    "    df_device['Hour'] = df_device.index.hour\n",
    "    df_device['Prev_SOC'] = df_device['SOC_batt'].shift(1)\n",
    "    df_device['Prev_Current'] = df_device['current_batt'].shift(1)\n",
    "    df_device['Prev_Solar_Current'] = df_device['solar_current'].shift(1)\n",
    "    df_device['Prev_Solar'] = df_device['isCharginS'].shift(1)\n",
    "    df_device['Prev_Voltage'] = df_device['volatge_batt'].shift(1)\n",
    "    df_device['Prev_Charging'] = df_device['isCharging'].shift(1)\n",
    "\n",
    "    # Compute rolling depletion rate\n",
    "    df_device['Depletion_Rate'] = df_device['SOC_batt'].diff().rolling(window=5).mean()\n",
    "    df_device['Depletion_Rate'].fillna(0, inplace=True)\n",
    "\n",
    "    df_device.dropna(inplace=True)\n",
    "\n",
    "    # Define input features and target\n",
    "    features = ['Hour', 'Prev_SOC', 'Prev_Current', 'Prev_Solar_Current', 'Prev_Solar', 'Prev_Voltage', 'Prev_Charging', 'Depletion_Rate']\n",
    "    X_full = df_device[features]\n",
    "    y_full = df_device['SOC_batt']\n",
    "\n",
    "    # Step 4: Train XGBoost model for this device\n",
    "    xgb_model_full = xgb.XGBRegressor(**best_params)\n",
    "    xgb_model_full.fit(X_full, y_full)\n",
    "\n",
    "    # Step 5: Predict SOC for all timestamps in this device\n",
    "    y_pred_full = xgb_model_full.predict(X_full)\n",
    "\n",
    "    # Step 6: Compute Dynamic Safe SOC Threshold\n",
    "    prior_mean = np.mean(y_full)\n",
    "    prior_std = np.std(y_full)\n",
    "    failure_prob_cache = stats.norm.cdf(10, loc=prior_mean, scale=prior_std)\n",
    "\n",
    "    safe_soc_thresholds = np.zeros(len(y_full))\n",
    "\n",
    "    for i in range(len(y_pred_full)):\n",
    "        depletion_factor = np.mean(np.diff(y_pred_full[max(0, i-5):i])) if i >= 5 else 0\n",
    "        prediction_uncertainty = np.std(y_pred_full[max(0, i-5):i]) if i >= 5 else 0\n",
    "\n",
    "        safe_soc = 50 + (10 * failure_prob_cache) + (5 * prediction_uncertainty)\n",
    "        safe_soc += (X_full['Prev_Charging'].iloc[i] * 5)\n",
    "        safe_soc -= (X_full['Prev_Solar_Current'].iloc[i] > 0) * 3\n",
    "        safe_soc = np.clip(safe_soc, 10, 70)\n",
    "\n",
    "        safe_soc_thresholds[i] = safe_soc\n",
    "\n",
    "    # Step 7: Store Safe SOC Thresholds for Merging\n",
    "    device_results = pd.DataFrame({\n",
    "        'deviceID': df_device['deviceID'].values,\n",
    "        'Timestamp': df_device.index,\n",
    "        'Safe_SOC_Threshold': safe_soc_thresholds\n",
    "    })\n",
    "    all_safe_soc_thresholds.append(device_results)\n",
    "\n",
    "    # Step 8: Identify Gaps Greater Than 1 Day\n",
    "    gap_threshold = pd.Timedelta(days=1)\n",
    "    time_gaps = df_device.index.to_series().diff() > gap_threshold\n",
    "    segment_indices = np.where(time_gaps)[0]\n",
    "\n",
    "    # Step 9: Plot Results for Each Device with Unlinked Segments\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    first_segment = True\n",
    "\n",
    "    start_idx = 0\n",
    "    for idx in segment_indices:\n",
    "        if first_segment:\n",
    "            plt.plot(df_device.index[start_idx:idx], y_full[start_idx:idx], label=\"Actual SOC\", color='black', linestyle=\"dashed\")\n",
    "            plt.plot(df_device.index[start_idx:idx], y_pred_full[start_idx:idx], label=\"Predicted SOC\", color='red')\n",
    "            plt.plot(df_device.index[start_idx:idx], safe_soc_thresholds[start_idx:idx], linestyle=\"dashdot\", color='blue', label=\"Safe SOC Threshold\")\n",
    "            first_segment = False\n",
    "        else:\n",
    "            plt.plot(df_device.index[start_idx:idx], y_full[start_idx:idx], color='black', linestyle=\"dashed\")\n",
    "            plt.plot(df_device.index[start_idx:idx], y_pred_full[start_idx:idx], color='red')\n",
    "            plt.plot(df_device.index[start_idx:idx], safe_soc_thresholds[start_idx:idx], linestyle=\"dashdot\", color='blue')\n",
    "\n",
    "        start_idx = idx  \n",
    "\n",
    "    plt.plot(df_device.index[start_idx:], y_full[start_idx:], color='black', linestyle=\"dashed\")\n",
    "    plt.plot(df_device.index[start_idx:], y_pred_full[start_idx:], color='red')\n",
    "    plt.plot(df_device.index[start_idx:], safe_soc_thresholds[start_idx:], linestyle=\"dashdot\", color='blue')\n",
    "\n",
    "    plt.xlabel(\"Time\")\n",
    "    plt.ylabel(\"SOC Battery Level (%)\")\n",
    "    plt.title(f\"Device {device_id}: SOC Prediction & Safe Threshold Over Time (Unlinked Gaps > 1 Day)\")\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    plt.show()\n",
    "\n",
    "# Step 10: Merge Computed Safe SOC Thresholds into `df_gdf`\n",
    "safe_soc_thresholds_df = pd.concat(all_safe_soc_thresholds)\n",
    "df_gdf = df_gdf.merge(safe_soc_thresholds_df, on=['deviceID', 'Timestamp'], how='left')\n",
    "\n",
    "# Step 11: Replace Static Threshold with Dynamic Safe SOC\n",
    "df_gdf['Depleted'] = df_gdf['SOC_batt'] < df_gdf['Safe_SOC_Threshold']  # Dynamic threshold check\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **SOC Depletion Modeling with Sensor OFF Strategies**\n",
    "\n",
    "## **Context and Objective**\n",
    "This implementation models the **State of Charge (SOC) depletion** of sensor-equipped vehicles while applying **dynamic sensor OFF strategies** based on pre-defined **time thresholds**. The goal is to assess the impact of temporarily turning **OFF** sensors to conserve energy, tracking **SOC savings**, and evaluating the difference in battery depletion rates under different sensor control policies.\n",
    "\n",
    "The core of this method involves:\n",
    "1. **Tracking the energy savings** from sensor OFF periods.\n",
    "2. **Modeling sensor activation behavior** based on pre-defined **time thresholds**.\n",
    "3. **Computing the modified SOC depletion** with stored energy savings.\n",
    "4. **Comparing the baseline SOC depletion with sensor control policies.**\n",
    "\n",
    "## **1. Data Preparation and Sorting**\n",
    "The dataset is first copied and indexed sequentially to ensure **chronological order**:\n",
    "\n",
    "$$\n",
    "\\text{Sort}(df, \\text{by}=[\\text{Timestamp}])\n",
    "$$\n",
    "\n",
    "This sorting is essential for maintaining **temporal consistency**, ensuring that energy tracking occurs in the correct **sequential order**.\n",
    "\n",
    "## **2. Definition of Sensor OFF Strategy**\n",
    "A sensor OFF threshold $ T_{\\text{threshold}} $ is defined, which determines how long a **grid cell** can remain **inactive** before reactivation is allowed. Given:\n",
    "\n",
    "$$\n",
    "T_{\\text{threshold}} = \\{ 12 \\text{ sec} \\}\n",
    "$$\n",
    "\n",
    "a sensor remains **OFF** if a vehicle has recently occupied the same **grid cell** within the threshold:\n",
    "\n",
    "$$\n",
    "\\text{Sensor\\_ON}_i =\n",
    "\\begin{cases} \n",
    "0, & \\text{if } (t_i - t_{\\text{last sensed}}) < T_{\\text{threshold}} \\\\\n",
    "1, & \\text{otherwise}\n",
    "\\end{cases}\n",
    "$$\n",
    "\n",
    "where:\n",
    "- $ t_i $ is the **current timestamp**.\n",
    "- $ t_{\\text{last sensed}} $ is the timestamp of the **last recorded sensor activation**.\n",
    "- $ T_{\\text{threshold}} $ is the **predefined time limit** for keeping the sensor OFF.\n",
    "\n",
    "## **3. Energy Storage Mechanism**\n",
    "During **sensor OFF periods**, the energy that would have been consumed is tracked using a **stored energy accumulation model**. The change in **SOC depletion rate** due to the sensor OFF mechanism is computed as:\n",
    "\n",
    "$$\n",
    "\\Delta \\text{SOC}_i = \\max(0, \\text{SOC}_{i-1} - \\text{SOC}_i)\n",
    "$$\n",
    "\n",
    "where:\n",
    "- $ \\text{SOC}_i $ is the **current SOC measurement**.\n",
    "- $ \\text{SOC}_{i-1} $ is the **previous SOC measurement**.\n",
    "\n",
    "Stored energy savings accumulate over time:\n",
    "\n",
    "$$\n",
    "\\text{Energy\\_Saved}_i = \\sum_{j=1}^{i} \\Delta \\text{SOC}_j, \\quad \\text{if Sensor\\_ON} = 0\n",
    "$$\n",
    "\n",
    "where:\n",
    "- $ \\text{Energy\\_Saved}_i $ represents the **total accumulated SOC savings**.\n",
    "- The accumulation occurs **only when the sensor is OFF**.\n",
    "\n",
    "## **4. Dynamic SOC Update with Energy Savings**\n",
    "The SOC for a given time step is **recomputed** using the stored energy:\n",
    "\n",
    "$$\n",
    "\\text{SOC\\_batt}_i' = \\text{SOC\\_batt}_i + \\text{Energy\\_Saved}_i\n",
    "$$\n",
    "\n",
    "where:\n",
    "- $ \\text{SOC\\_batt}_i' $ is the **corrected SOC value** incorporating energy savings.\n",
    "- $ \\text{SOC\\_batt}_i $ is the **original SOC value**.\n",
    "- $ \\text{Energy\\_Saved}_i $ represents the **cumulative stored SOC improvements**.\n",
    "\n",
    "To ensure **SOC does not exceed 100%**, the final corrected SOC values are clipped:\n",
    "\n",
    "$$\n",
    "\\text{SOC\\_batt}_i' = \\min(100, \\text{SOC\\_batt}_i + \\text{Energy\\_Saved}_i)\n",
    "$$\n",
    "\n",
    "## **5. Baseline and Threshold Comparisons**\n",
    "To assess the impact of sensor OFF strategies, SOC depletion is **compared across different thresholds**. The average daily SOC depletion per device is computed as:\n",
    "\n",
    "$$\n",
    "\\text{SOC\\_depletion} = \\frac{1}{N} \\sum_{i=1}^{N} \\text{SOC\\_batt}_i'\n",
    "$$\n",
    "\n",
    "where:\n",
    "- $ N $ is the number of **time steps in a day**.\n",
    "- $ \\text{SOC\\_batt}_i' $ represents the **SOC levels incorporating sensor OFF savings**.\n",
    "\n",
    "The **baseline depletion rate** without any sensor OFF strategy is also computed:\n",
    "\n",
    "$$\n",
    "\\text{Baseline\\_SOC} = \\frac{1}{N} \\sum_{i=1}^{N} \\text{SOC\\_batt}_i\n",
    "$$\n",
    "\n",
    "where no energy savings are incorporated.\n",
    "\n",
    "## **6. Expected Outcomes and Justification**\n",
    "This method enables:\n",
    "- **Quantification of energy savings** from sensor OFF strategies.\n",
    "- **Comparison of SOC depletion trends** across different sensor OFF thresholds.\n",
    "- **Validation of sensor optimization policies** to maximize battery lifespan.\n",
    "\n",
    "By implementing **grid-based sensing optimization**, the system **reduces unnecessary sensor activations**, ensuring **longer sensor endurance** while maintaining **effective data collection**.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 308,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_temp = df.copy().reset_index(drop=True)  # Ensure indices are sequential\n",
    "df_temp=df_temp.sort_values(by=['Timestamp']).reset_index(drop=True) \n",
    "\n",
    "# Define different time thresholds to compare\n",
    "time_thresholds = {\n",
    "    # \"3 sec\": 3,\n",
    "    \"360 sec\": 360\n",
    "}\n",
    "\n",
    "# Create a dictionary to store SOC and sensor states for each threshold\n",
    "soc_depletion_results = {}\n",
    "\n",
    "# Iterate over different time thresholds\n",
    "for label, TIME_THRESHOLD in time_thresholds.items():\n",
    "\n",
    "    # Track last sensed timestamp, and stored energy during OFF periods\n",
    "    last_sensed_time = {}\n",
    "    stored_energy = {}\n",
    "\n",
    "    # Previous date\n",
    "    prev_date=None\n",
    "    \n",
    "    for i in range(len(df_temp)):\n",
    "        row = df_temp.iloc[i]\n",
    "        grid_key = (row['Lat_Grid'], row['Log_Grid'])  # Unique grid identifier\n",
    "        current_time = row['Timestamp']\n",
    "        current_date = row['Date']\n",
    "        device= row['deviceID']\n",
    "\n",
    "        # Initialise inter-row differences when OFF\n",
    "        d_diff_prev=0 \n",
    "        \n",
    "        # Reset stored energy at the start of a new day\n",
    "        if prev_date is not None and current_date != prev_date:\n",
    "            stored_energy={}  # Reset stored energy for all grid cells\n",
    "            df_temp.loc[i:, f'Energy_Saved_{label}'] = 0  # Reset energy savings for the new day\n",
    "            print(f\"[RESET] Reset stored energy for new day: {current_date}\")\n",
    "\n",
    "        prev_date = current_date  # Update previous date tracker\n",
    "\n",
    "        if df_temp.loc[i, 'SOC_batt']>99:\n",
    "            stored_energy[grid_key]=0\n",
    "            df_temp.loc[i:, f'Energy_Saved_{label}']=0\n",
    "\n",
    "        # If the grid was sensed recently (within the threshold), turn OFF the sensor\n",
    "        if grid_key in last_sensed_time and (current_time - last_sensed_time[grid_key]).total_seconds() < TIME_THRESHOLD:\n",
    "            df_temp.at[i, f'Sensor_ON_{label}'] = False   \n",
    "\n",
    "            # Accumulate stored energy\n",
    "            if i > 0 and pd.notna(df_temp.iloc[i - 1]['SOC_batt']) and pd.notna(row['SOC_batt']):\n",
    "            \n",
    "                # Find the last preceding row for this device\n",
    "                if device == df_temp.iloc[i-1]['deviceID']:\n",
    "                    d_diff = max(0, df_temp.iloc[i - 1]['SOC_batt'] - row['SOC_batt']) #current inter-row difference\n",
    "                    diff = max(d_diff_prev, d_diff)\n",
    "                    stored_energy[grid_key] = df_temp.loc[i-1, f'Energy_Saved_{label}']\n",
    "                    stored_energy[grid_key] += diff\n",
    "                    \n",
    "                    if diff != 0: \n",
    "                        df_temp.loc[i:, f'Energy_Saved_{label}'] = stored_energy[grid_key]\n",
    "                    else:\n",
    "                        df_temp.loc[i:, f'Energy_Saved_{label}'] = df_temp.loc[i-1, f'Energy_Saved_{label}']\n",
    "\n",
    "                    d_diff_prev=diff\n",
    "                    print(f\"[OFF]: Accumulated {diff:.2f}% for device {device}. Total stored: {stored_energy[grid_key]:.2f}%\")\n",
    "\n",
    "                else:\n",
    "                    d_diff = max(0, df_temp.loc[df_temp.deviceID == device, :]['SOC_batt'].iloc[-1] - row['SOC_batt']) #current inter-row difference\n",
    "                    diff = max(d_diff_prev, d_diff)\n",
    "                    stored_energy[grid_key] = df_temp.loc[df_temp.deviceID == device, :][f'Energy_Saved_{label}'].iloc[-1]\n",
    "                    stored_energy[grid_key] += diff\n",
    "                    \n",
    "                    if diff != 0: \n",
    "                        df_temp.loc[i:, f'Energy_Saved_{label}'] = stored_energy[grid_key]\n",
    "                    else:\n",
    "                        df_temp.loc[i:, f'Energy_Saved_{label}'] = df_temp.loc[df_temp.deviceID == device, :][f'Energy_Saved_{label}'].iloc[-1]\n",
    "\n",
    "                    d_diff_prev=diff\n",
    "                    print(f\"CHANGE [OFF]: Accumulated {diff:.2f}% for device {device}. Total stored: {stored_energy[grid_key]:.2f}%\")\n",
    "\n",
    "        else:\n",
    "            # Update last sensed time when the sensor turns ON\n",
    "            last_sensed_time[grid_key] = current_time\n",
    "            df_temp.at[i, f'Sensor_ON_{label}'] = True\n",
    "            d_diff_prev=0\n",
    "\n",
    "            if device == df_temp.iloc[i-1]['deviceID']:\n",
    "\n",
    "                # Ensure stored_energy is initialized per grid cell without overwriting previous values\n",
    "                if grid_key not in stored_energy:\n",
    "                    if i > 0:\n",
    "                        #Carry forward the stored energy from the last known row\n",
    "                        stored_energy[grid_key] = df_temp.loc[i-1, f'Energy_Saved_{label}']\n",
    "                        df_temp.loc[i:, f'Energy_Saved_{label}'] = stored_energy[grid_key]\n",
    "                    elif i == 0:\n",
    "                        stored_energy[grid_key] = 0  # First iteration, no prior energy \n",
    "                        df_temp.loc[i:, f'Energy_Saved_{label}'] = stored_energy[grid_key]  \n",
    "                print(f\"[ON]: device {device}. Total stored: {stored_energy[grid_key]:.2f}%\")\n",
    "\n",
    "            else:\n",
    "                 # Ensure stored_energy is initialized per grid cell without overwriting previous values\n",
    "                if grid_key not in stored_energy:\n",
    "                    if i > 0:\n",
    "                        #Carry forward the stored energy from the last known row\n",
    "                        stored_energy[grid_key] = df_temp.loc[df_temp.deviceID == device, :][f'Energy_Saved_{label}'].iloc[-1]\n",
    "                        df_temp.loc[i:, f'Energy_Saved_{label}'] = stored_energy[grid_key]\n",
    "                    elif i == 0:\n",
    "                        stored_energy[grid_key] = 0  # First iteration, no prior energy \n",
    "                        df_temp.loc[i:, f'Energy_Saved_{label}'] = stored_energy[grid_key]                 \n",
    "                \n",
    "                print(f\"CHANGE [ON]: device {device}. Total stored: {stored_energy[grid_key]:.2f}%\")\n",
    "\n",
    "    \n",
    "    # Compute new SOC_batt with savings\n",
    "    df_temp[f'SOC_batt_{label}'] = df_temp['SOC_batt'] + df_temp[f'Energy_Saved_{label}']\n",
    "    df_temp[f'SOC_batt_{label}'] = df_temp[f'SOC_batt_{label}'].clip(upper=100)\n",
    "\n",
    "    # Compute SOC depletion for this threshold\n",
    "    daily_soc = df_temp.groupby(['Date', 'deviceID'])[f'SOC_batt_{label}'].mean()\n",
    "    soc_depletion_results[label] = daily_soc\n",
    "\n",
    "\n",
    "# Baseline: Compute SOC depletion without constraints\n",
    "soc_depletion_results[\"Baseline\"] = df_temp.groupby(['Date', 'deviceID'])['SOC_batt'].mean()\n",
    "\n",
    "# Convert results to a DataFrame for plotting\n",
    "soc_depletion_df = pd.DataFrame(soc_depletion_results)\n",
    "\n",
    "# Save the updated dataset with sensor states and energy savings for each threshold\n",
    "output_path = \"/workspace/data/updated_SOC_batt_with_energy_savings.xlsx\"\n",
    "df_temp.to_excel(output_path, index=False)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define line styles and transparency levels for each threshold\n",
    "line_styles = {\n",
    "    \"Baseline\": \"--\",\n",
    "    # \"3 sec\": \"--\",\n",
    "    \"360 sec\": \"-\"\n",
    "}\n",
    "\n",
    "# Transparency levels for each threshold\n",
    "alpha_values = {\n",
    "    \"Baseline\": 0.5,  # 70% transparent\n",
    "    # \"3 sec\": 0.7,  # 30% transparent\n",
    "    \"360 sec\": 1   #Fully visible\n",
    "}\n",
    "\n",
    "# Predefined colors for devices\n",
    "predefined_colors = ['#007FFF', '#DC143C', '#FF4500','#39FF14', '#800080']\n",
    "device_ids = set()\n",
    "\n",
    "for soc_series in soc_depletion_results.values():\n",
    "    device_ids.update(soc_series.index.get_level_values('deviceID').unique())\n",
    "\n",
    "# Create a color map using predefined colors\n",
    "color_map = {device_id: predefined_colors[i % len(predefined_colors)] for i, device_id in enumerate(sorted(device_ids))}\n",
    "\n",
    "# Plot SOC depletion for different devices and thresholds\n",
    "plt.figure(figsize=(12, 6))\n",
    "\n",
    "# Iterate over thresholds and plot per device\n",
    "for label, soc_series in soc_depletion_results.items():  # soc_series is a MultiIndexed Series\n",
    "    for device_id in soc_series.index.get_level_values('deviceID').unique():  # Get unique devices\n",
    "        device_data = soc_series[soc_series.index.get_level_values('deviceID') == device_id]\n",
    "        plt.plot(\n",
    "            device_data.index.get_level_values('Date'),  # X-axis: Dates\n",
    "            device_data.values,  # Y-axis: SOC values\n",
    "            linestyle=line_styles[label],\n",
    "            color=color_map[device_id],  # Use predefined color for the device\n",
    "            # marker='o',\n",
    "            # markersize=3,\n",
    "            alpha=alpha_values[label],  # Apply transparency per threshold\n",
    "            label=f\"Device {device_id} - {label}\"\n",
    "        )\n",
    "\n",
    "plt.xlabel('Date')\n",
    "plt.ylabel('Mean SOC (%)')\n",
    "plt.title('SOC Depletion Comparison Across Devices and Time Constraints')\n",
    "\n",
    "# Place the legend outside the plot\n",
    "plt.legend(\n",
    "    bbox_to_anchor=(1.05, 1),  # Place legend to the right of the plot\n",
    "    loc='upper left',          # Align legend to the top-left of the bounding box\n",
    "    borderaxespad=0.           # Reduce spacing between the legend and the plot\n",
    ")\n",
    "plt.grid(True)\n",
    "plt.xticks(rotation=45)\n",
    "\n",
    "# Adjust layout to make room for the legend\n",
    "plt.tight_layout()\n",
    "\n",
    "# Show plot\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "HEATMAPS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 310,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make copies for Baseline and Adaptive to avoid overwriting\n",
    "grid_gdf_baseline = grid_gdf.copy()\n",
    "grid_gdf_adaptive = grid_gdf.copy()\n",
    "\n",
    "### ---- BASELINE ----\n",
    "# Step 1: Compute Gamma Scaling on Raw Counts (Without Log Transformation)\n",
    "mean_count_baseline = grid_gdf_baseline['Count'].mean()\n",
    "std_count_baseline = grid_gdf_baseline['Count'].std()\n",
    "CV_baseline = std_count_baseline / mean_count_baseline if mean_count_baseline > 0 else 1\n",
    "baseline_gamma = 1 / (1 + CV_baseline)\n",
    "\n",
    "# Apply gamma scaling directly on raw counts\n",
    "grid_gdf_baseline['Scaled_Count_Baseline'] = (grid_gdf_baseline['Count'] + 1) ** baseline_gamma\n",
    "\n",
    "# Step 2: Normalize the Scaled Counts for Better Visual Comparison\n",
    "scaled_min_baseline = grid_gdf_baseline['Scaled_Count_Baseline'].min()\n",
    "scaled_max_baseline = grid_gdf_baseline['Scaled_Count_Baseline'].max()\n",
    "\n",
    "\n",
    "### ---- ADAPTIVE ----\n",
    "# Define the label used in your code (e.g., \"30 sec\")\n",
    "label = \"360 sec\"\n",
    "\n",
    "# Filter the dataset to include only rows where the sensor is ON\n",
    "adaptive_df = df_temp[df_temp[f'Sensor_ON_{label}'] == True]\n",
    "\n",
    "# Create a GeoDataFrame for the points in df\n",
    "adaptive_df_gdf = gpd.GeoDataFrame(adaptive_df, geometry=gpd.points_from_xy(adaptive_df['Log'], adaptive_df['Lat']), crs=\"EPSG:4326\")\n",
    "\n",
    "# Perform spatial join for Adaptive case\n",
    "joined = gpd.sjoin(adaptive_df_gdf, grid_gdf_adaptive, how=\"left\", predicate=\"within\")\n",
    "grid_gdf_adaptive['Count'] = joined.groupby(joined.index_right).size()\n",
    "grid_gdf_adaptive['Count'].fillna(0, inplace=True)\n",
    "\n",
    "# Step 1: Apply Gamma Scaling on Raw Counts (Without Log Transformation)\n",
    "mean_count_adaptive = grid_gdf_adaptive['Count'].mean()\n",
    "std_count_adaptive = grid_gdf_adaptive['Count'].std()\n",
    "CV_adaptive = std_count_adaptive / mean_count_adaptive if mean_count_adaptive > 0 else 1\n",
    "adaptive_gamma = 1 / (1 + CV_adaptive)\n",
    "\n",
    "# Apply gamma scaling directly on raw counts\n",
    "grid_gdf_adaptive['Scaled_Count_Adaptive'] = (grid_gdf_adaptive['Count'] + 1) ** adaptive_gamma\n",
    "\n",
    "# Step 2: Normalize the Scaled Counts for Better Visual Comparison\n",
    "scaled_min_adaptive = grid_gdf_adaptive['Scaled_Count_Adaptive'].min()\n",
    "scaled_max_adaptive = grid_gdf_adaptive['Scaled_Count_Adaptive'].max()\n",
    "\n",
    "\n",
    "# Global min and max for both datasets\n",
    "global_min = min(scaled_min_baseline, scaled_min_adaptive)\n",
    "global_max = max(scaled_max_baseline, scaled_max_adaptive)\n",
    "\n",
    "print(f\"Global Min: {global_min}, Global Max: {global_max}\")\n",
    "\n",
    "## ViZ\n",
    "grid_gdf_baseline['Normalized_Scaled_Baseline'] = (grid_gdf_baseline['Scaled_Count_Baseline'] - global_min) / (global_max - global_min)\n",
    "# Step 3: Apply Sigmoid Scaling ONLY for Visualization (Baseline)\n",
    "k = 10  # Adjust this value for better contrast enhancement\n",
    "grid_gdf_baseline['Visual_Baseline'] = 1 / (1 + np.exp(-k * (grid_gdf_baseline['Normalized_Scaled_Baseline'] - 0.5)))\n",
    "\n",
    "grid_gdf_adaptive['Normalized_Scaled_Adaptive'] = (grid_gdf_adaptive['Scaled_Count_Adaptive'] - global_min) / (global_max - global_min)\n",
    "# Step 3: Apply Sigmoid Scaling ONLY for Visualization (Adaptive)\n",
    "grid_gdf_adaptive['Visual_Adaptive'] = 1 / (1 + np.exp(-k * (grid_gdf_adaptive['Normalized_Scaled_Adaptive'] - 0.5)))\n",
    "\n",
    "\n",
    "### ---- COMMON VMIN COMPUTATION ----\n",
    "# Compute the common vmax for both Baseline and Adaptive visualizations\n",
    "common_vmin = min(\n",
    "    grid_gdf_baseline['Visual_Baseline'].min(),\n",
    "    grid_gdf_adaptive['Visual_Adaptive'].min()\n",
    ")\n",
    "\n",
    "print(f\"Computed common_vmin: {common_vmin:.4f}\")\n",
    "\n",
    "### ---- COMMON VMAX COMPUTATION ----\n",
    "\n",
    "# Generate quantile range\n",
    "# Generate quantile range\n",
    "quantile_range = np.linspace(0.90, 1, 1000)\n",
    "cv_differences = []\n",
    "\n",
    "for q in quantile_range:\n",
    "    baseline_q = grid_gdf_baseline['Visual_Baseline'].quantile(q)\n",
    "    adaptive_q = grid_gdf_adaptive['Visual_Adaptive'].quantile(q)\n",
    "    \n",
    "    cv_baseline = np.std(grid_gdf_baseline['Visual_Baseline'][grid_gdf_baseline['Visual_Baseline'] <= baseline_q]) / np.mean(grid_gdf_baseline['Visual_Baseline'][grid_gdf_baseline['Visual_Baseline'] <= baseline_q])\n",
    "    cv_adaptive = np.std(grid_gdf_adaptive['Visual_Adaptive'][grid_gdf_adaptive['Visual_Adaptive'] <= adaptive_q]) / np.mean(grid_gdf_adaptive['Visual_Adaptive'][grid_gdf_adaptive['Visual_Adaptive'] <= adaptive_q])\n",
    "    \n",
    "    cv_differences.append(abs(cv_baseline - cv_adaptive))\n",
    "\n",
    "# Filter out the initial flat part by thresholding\n",
    "threshold_value = 0.01  # Adjust this value if needed\n",
    "filtered_indices = np.where(np.array(cv_differences) > threshold_value)[0]\n",
    "\n",
    "# Apply Kneedle only if there's a significant increase\n",
    "if len(filtered_indices) > 0:\n",
    "    filtered_quantiles = quantile_range[filtered_indices]\n",
    "    filtered_cv_differences = np.array(cv_differences)[filtered_indices]\n",
    "\n",
    "    # Use kneed to find the optimal quantile\n",
    "    kneedle = KneeLocator(filtered_quantiles, filtered_cv_differences, curve='convex', direction='increasing')\n",
    "    optimal_quantile = round(kneedle.elbow, 3)  # Display to three decimals\n",
    "else:\n",
    "    # Fallback if no significant increase is detected\n",
    "    optimal_quantile = 0.990  # A sensible high quantile to use\n",
    "\n",
    "# Plotting the improvement trend\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.plot(quantile_range, cv_differences, label='CV Difference')\n",
    "plt.axvline(optimal_quantile, color='red', linestyle='--', label=f'Optimal Quantile = {optimal_quantile:.3f}')\n",
    "plt.title('CV Difference vs Quantile Cutoff (Using Kneedle on Filtered Data)')\n",
    "plt.xlabel('Quantile Cutoff')\n",
    "plt.ylabel('CV Difference')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()\n",
    "\n",
    "print(f\"Optimal Quantile Found: {optimal_quantile:.3f}\")\n",
    "\n",
    "\n",
    "common_vmax = min(\n",
    "    grid_gdf_baseline['Visual_Baseline'].quantile(optimal_quantile),\n",
    "    grid_gdf_adaptive['Visual_Adaptive'].quantile(optimal_quantile)\n",
    ")\n",
    "\n",
    "print(f\"Computed common_vmin: {common_vmax:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Baseline Optimal Gamma: {baseline_gamma:.4f}\")\n",
    "\n",
    "# Define a colormap\n",
    "colormap = cm.LinearColormap(\n",
    "    colors=['darkblue', 'cyan', 'yellow', 'orangered'],\n",
    "    vmin=common_vmin,\n",
    "    vmax=common_vmax\n",
    ")\n",
    "\n",
    "# Create folium map with a dark basemap\n",
    "m = folium.Map(location=[df['Lat'].mean(), df['Log'].mean()], zoom_start=12, tiles='Cartodb dark_matter')\n",
    "\n",
    "# Add GeoJSON overlay\n",
    "geojson = folium.GeoJson(\n",
    "    grid_gdf_baseline,\n",
    "    style_function=lambda feature: {\n",
    "        'fillColor': colormap(feature['properties']['Visual_Baseline']),\n",
    "        'weight': 0.0,\n",
    "        'fillOpacity': 0.4\n",
    "    }\n",
    ").add_to(m)\n",
    "\n",
    "colormap.caption = \"Scaled Count Intensity (Baseline)\"\n",
    "colormap.add_to(m)  # Attach to the map\n",
    "\n",
    "# Show map\n",
    "m"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 312,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute baseline statistics\n",
    "baseline_counts = df_temp.groupby(['Lat_Grid', 'Log_Grid']).size()\n",
    "mean_count_baseline = baseline_counts.mean()\n",
    "std_count_baseline = baseline_counts.std()\n",
    "cv_baseline = std_count_baseline / mean_count_baseline\n",
    "\n",
    "print(f\"Baseline Statistics:\")\n",
    "print(f\"Mean Count: {mean_count_baseline:.4f}\")\n",
    "print(f\"Standard Deviation: {std_count_baseline:.4f}\")\n",
    "print(f\"CV (Baseline): {cv_baseline:.4f}\")\n",
    "\n",
    "# Define the label used in your code (e.g., \"30 sec\")\n",
    "label = \"360 sec\"\n",
    "\n",
    "# Filter the dataset to include only rows where the sensor is ON\n",
    "adaptive_df = df_temp[df_temp[f'Sensor_ON_{label}'] == True]\n",
    "\n",
    "# Compute adaptive statistics\n",
    "adaptive_counts = adaptive_df.groupby(['Lat_Grid', 'Log_Grid']).size()\n",
    "mean_count_adaptive = adaptive_counts.mean()\n",
    "std_count_adaptive = adaptive_counts.std()\n",
    "cv_adaptive = std_count_adaptive / mean_count_adaptive\n",
    "\n",
    "print(f\"\\n Adaptive Sensing Statistics ({label}):\")\n",
    "print(f\"Mean Count: {mean_count_adaptive:.4f}\")\n",
    "print(f\"Standard Deviation: {std_count_adaptive:.4f}\")\n",
    "print(f\"CV (Adaptive): {cv_adaptive:.4f}\")\n",
    "\n",
    "# Calculate improvement in uniformity\n",
    "delta_cv = cv_baseline - cv_adaptive\n",
    "percentage_improvement = (delta_cv / cv_baseline) * 100\n",
    "\n",
    "print(f\"\\n Improvement in Spatial Uniformity (ΔCV): {delta_cv:.4f}\")\n",
    "print(f\" Percentage Improvement in Uniformity: {percentage_improvement:.2f}%\")\n",
    "\n",
    "# Plot histogram of counts for baseline and adaptive scenarios\n",
    "plt.figure(figsize=(12, 6))\n",
    "\n",
    "plt.hist(baseline_counts, bins=50, alpha=0.5, label='Baseline (All Measurements)')\n",
    "plt.hist(adaptive_counts, bins=50, alpha=0.5, label=f'Adaptive Sensing ({label})')\n",
    "\n",
    "plt.title('Distribution of Measurements per Grid Cell')\n",
    "plt.xlabel('Number of Measurements per Grid Cell')\n",
    "plt.ylabel('Frequency')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Adaptive Optimal Gamma: {adaptive_gamma:.4f}\")\n",
    "\n",
    "# Define a colormap\n",
    "colormap = cm.LinearColormap(\n",
    "    colors=['darkblue', 'cyan', 'yellow', 'orangered'],\n",
    "    vmin=common_vmin,\n",
    "    vmax=common_vmax\n",
    ")\n",
    "\n",
    "# Create folium map with a dark basemap\n",
    "m = folium.Map(location=[df['Lat'].mean(), df['Log'].mean()], zoom_start=12, tiles='Cartodb dark_matter')\n",
    "\n",
    "# Add GeoJSON overlay for the adaptive map\n",
    "geojson = folium.GeoJson(\n",
    "    grid_gdf_adaptive,\n",
    "    style_function=lambda feature: {\n",
    "        'fillColor': colormap(feature['properties']['Visual_Adaptive']),\n",
    "        'weight': 0.0,\n",
    "        'fillOpacity': 0.4\n",
    "    }\n",
    ").add_to(m)\n",
    "\n",
    "# Add the color legend\n",
    "colormap.caption = \"Count Intensity (Adaptive)\"\n",
    "colormap.add_to(m)\n",
    "\n",
    "# Show map\n",
    "m"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 314,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Ensure Both Visual Maps Are Normalized Between 0 and 1\n",
    "visual_baseline_min = grid_gdf_baseline['Visual_Baseline'].min()\n",
    "visual_baseline_max = grid_gdf_baseline['Visual_Baseline'].max()\n",
    "grid_gdf_baseline['Normalized_Visual_Baseline'] = (grid_gdf_baseline['Visual_Baseline'] - visual_baseline_min) / (visual_baseline_max - visual_baseline_min)\n",
    "\n",
    "visual_adaptive_min = grid_gdf_adaptive['Visual_Adaptive'].min()\n",
    "visual_adaptive_max = grid_gdf_adaptive['Visual_Adaptive'].max()\n",
    "grid_gdf_adaptive['Normalized_Visual_Adaptive'] = (grid_gdf_adaptive['Visual_Adaptive'] - visual_adaptive_min) / (visual_adaptive_max - visual_adaptive_min)\n",
    "\n",
    "# Step 2: Calculate Relative Improvement (Enhanced Difference Calculation)\n",
    "grid_gdf_adaptive['Difference'] = grid_gdf_adaptive['Normalized_Visual_Adaptive'] - grid_gdf_baseline['Normalized_Visual_Baseline']\n",
    "grid_gdf_adaptive['Difference_Sign'] = np.sign(grid_gdf_adaptive['Difference'])\n",
    "\n",
    "# Sigmoid Scaling (Adjust k for more/less aggressive enhancement)\n",
    "# k = 30  # Increase for stronger contrast, decrease for smoother visualization\n",
    "grid_gdf_adaptive['Enhanced_Difference'] = grid_gdf_adaptive['Difference_Sign'] * (1 / (1 + np.exp(-k * grid_gdf_adaptive['Difference'])) - 0.5)\n",
    "\n",
    "# Step 4: Normalization of Enhanced Differences for Optimal Visualization\n",
    "enhanced_diff_min = grid_gdf_adaptive['Enhanced_Difference'].min()\n",
    "enhanced_diff_max = grid_gdf_adaptive['Enhanced_Difference'].max()\n",
    "\n",
    "# Normalizing to range [0, 1] for Blue intensities\n",
    "grid_gdf_adaptive['Enhanced_Difference_Vis'] = (grid_gdf_adaptive['Enhanced_Difference'] - enhanced_diff_min) / (enhanced_diff_max - enhanced_diff_min)\n",
    "\n",
    "# Step 5: Define a More Sensitive Colormap with Strong Contrast (Optimized Blue Scale)\n",
    "colormap = cm.LinearColormap(\n",
    "    colors=['white','blue'],  # Only Blue to highlight improvements\n",
    "    vmin=0,  # Minimum of the normalized data\n",
    "    vmax=1   # Maximum of the normalized data\n",
    ")\n",
    "\n",
    "# Step 6: Create folium map to visualize differences\n",
    "m = folium.Map(location=[df['Lat'].mean(), df['Log'].mean()], zoom_start=12, tiles='Cartodb dark_matter')\n",
    "\n",
    "# Add GeoJSON overlay for the difference map\n",
    "geojson = folium.GeoJson(\n",
    "    grid_gdf_adaptive,\n",
    "    style_function=lambda feature: {\n",
    "        'fillColor': colormap(feature['properties']['Enhanced_Difference_Vis']),\n",
    "        'weight': 0.0,\n",
    "        'fillOpacity': 0.4\n",
    "    }\n",
    ").add_to(m)\n",
    "\n",
    "# Add the color legend\n",
    "colormap.caption = \"Improvement Map (Uniformity Improvement in Blue)\"\n",
    "colormap.add_to(m)\n",
    "\n",
    "# Show map\n",
    "m\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 317,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "from shapely.geometry import Polygon\n",
    "import geopandas as gpd\n",
    "\n",
    "# Define simulation parameters\n",
    "num_simulations = 100  # Number of Monte Carlo simulations per sensor count\n",
    "sensor_counts = [1, 2, 3, 4, 5, 10, 15, 20, 30]  # Different numbers of sensors to test\n",
    "grid_size = 40  # Grid size in meters\n",
    "\n",
    "# Prepare storage for results\n",
    "coverage_results = {n: [] for n in sensor_counts}\n",
    "\n",
    "# Generate grid (reusing previous code)\n",
    "min_lat = df['Lat'].min()\n",
    "max_lat = df['Lat'].max()\n",
    "min_lon = df['Log'].min()\n",
    "max_lon = df['Log'].max()\n",
    "\n",
    "lat_resolution = grid_size / 111320\n",
    "lon_resolution_at_lat = lambda lat: grid_size / (111320 * np.cos(np.radians(lat)))\n",
    "\n",
    "grid = []\n",
    "lat = min_lat\n",
    "while lat < max_lat:\n",
    "    lon = min_lon\n",
    "    while lon < max_lon:\n",
    "        lon_res = lon_resolution_at_lat(lat)\n",
    "        grid.append(Polygon([\n",
    "            (lon, lat),\n",
    "            (lon + lon_res, lat),\n",
    "            (lon + lon_res, lat + lat_resolution),\n",
    "            (lon, lat + lat_resolution)\n",
    "        ]))\n",
    "        lon += lon_res\n",
    "    lat += lat_resolution\n",
    "\n",
    "# Create a GeoDataFrame for the grid\n",
    "grid_gdf = gpd.GeoDataFrame({'geometry': grid, 'Count': 0}, crs=\"EPSG:4326\")\n",
    "\n",
    "# Generate a list of unique deviceIDs\n",
    "unique_devices = df['deviceID'].unique()\n",
    "\n",
    "# Run Monte Carlo simulations for each sensor count\n",
    "for n in sensor_counts:\n",
    "    for _ in range(num_simulations):\n",
    "        # Randomly sample 'n' sensors from the dataset\n",
    "        sampled_devices = random.sample(list(unique_devices), min(n, len(unique_devices)))\n",
    "        df_sample = df[df['deviceID'].isin(sampled_devices)]\n",
    "        \n",
    "        # Create a GeoDataFrame for the sampled points\n",
    "        sampled_gdf = gpd.GeoDataFrame(df_sample, geometry=gpd.points_from_xy(df_sample['Log'], df_sample['Lat']), crs=\"EPSG:4326\")\n",
    "        \n",
    "        # Perform spatial join\n",
    "        joined = gpd.sjoin(sampled_gdf, grid_gdf, how=\"left\", predicate=\"within\")\n",
    "        \n",
    "        # Count unique cells covered\n",
    "        unique_cells_covered = len(joined['index_right'].unique())\n",
    "        \n",
    "        # Record coverage\n",
    "        coverage_results[n].append(unique_cells_covered)\n",
    "\n",
    "# Calculate average coverage and standard deviation for each sensor count\n",
    "average_coverage = {n: np.mean(coverage_results[n]) for n in sensor_counts}\n",
    "std_coverage = {n: np.std(coverage_results[n]) for n in sensor_counts}\n",
    "\n",
    "# Plot the results\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.errorbar(sensor_counts, list(average_coverage.values()), yerr=list(std_coverage.values()), fmt='o-', capsize=5)\n",
    "plt.title(\"Coverage vs. Number of Sensors (Monte Carlo Simulation)\")\n",
    "plt.xlabel(\"Number of Sensors\")\n",
    "plt.ylabel(\"Average Unique Cells Covered\")\n",
    "plt.grid(True)\n",
    "plt.show()\n",
    "\n",
    "# Display the coverage results for each sensor count\n",
    "results_df = pd.DataFrame({\n",
    "    \"Number of Sensors\": sensor_counts,\n",
    "    \"Average Coverage\": list(average_coverage.values()),\n",
    "    \"Standard Deviation\": list(std_coverage.values())\n",
    "})\n",
    "\n",
    "results_df\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Markov-Based Spatial Transition Modeling for Vehicle Movement and Sensor Depletion Analysis**\n",
    "\n",
    "This methodology enables **data-driven mobility prediction** and **battery depletion analysis** using **Markovian dynamics**.\n",
    "\n",
    "## **1. Assigning Vehicles to Grid Cells Using Polygon Containment**\n",
    "Given a dataset of GPS points (`Lat`, `Log`), we spatially bin the data into a **120m x 120m** grid using polygon containment:\n",
    "\n",
    "- Convert each GPS point into a **GeoDataFrame** (`df_gdf`).\n",
    "- Each point is assigned to its corresponding grid cell using the spatial containment function:\n",
    "\n",
    "  $$\n",
    "  G(i) = \\arg\\max_j \\mathbb{1}(p_i \\in P_j)\n",
    "  $$\n",
    "\n",
    "  where:\n",
    "  - $ p_i $ is the point geometry of record $ i $,\n",
    "  - $ P_j $ is the polygon geometry of grid cell $ j $,\n",
    "  - $ \\mathbb{1}(p_i \\in P_j) $ is an indicator function that is **1** if $ p_i $ is inside $ P_j $, else **0**.\n",
    "\n",
    "- Any points that **do not match** a grid cell are treated as **outliers** and removed.\n",
    "\n",
    "## **2. Temporal Sorting for Transition Analysis**\n",
    "To analyze transitions between grid cells, the dataset is **sorted** by:\n",
    "\n",
    "$$\n",
    "\\text{Sort}(df, \\text{by}=[\\text{deviceID}, \\text{Timestamp}])\n",
    "$$\n",
    "\n",
    "where:\n",
    "- `deviceID` ensures sorting is done **per vehicle**,\n",
    "- `Timestamp` orders data points **chronologically**.\n",
    "\n",
    "## **3. Identifying Sensor Depletion Events**\n",
    "A **battery depletion threshold** is defined as:\n",
    "\n",
    "$$\n",
    "SOC_{\\text{batt}} < 50\\%\n",
    "$$\n",
    "\n",
    "where **State of Charge (SOC)** is the battery’s remaining capacity. We create a binary indicator:\n",
    "\n",
    "$$\n",
    "D_i = \n",
    "\\begin{cases} \n",
    "1, & SOC_{\\text{batt}, i} < 50\\% \\\\\n",
    "0, & \\text{otherwise}\n",
    "\\end{cases}\n",
    "$$\n",
    "\n",
    "- **Pre-Depletion Data**: $ D_i = 0 $ (battery level above threshold).\n",
    "- **Post-Depletion Data**: $ D_i = 1 $ (battery level below threshold).\n",
    "\n",
    "## **4. Constructing Grid Cell Transitions**\n",
    "To model **spatial movement**, we define a transition as:\n",
    "\n",
    "$$\n",
    "T_i = (G_i, G_{i+1})\n",
    "$$\n",
    "\n",
    "where:\n",
    "- $ G_i $ is the grid cell at time $ t_i $,\n",
    "- $ G_{i+1} $ is the **next** grid cell at $ t_{i+1} $.\n",
    "\n",
    "We compute the **state transitions** for each vehicle:\n",
    "\n",
    "$$\n",
    "\\text{Next\\_Grid\\_Cell} = G_{i+1} = \\text{shift}(G_i, -1)\n",
    "$$\n",
    "\n",
    "Dropping the last row for each vehicle ensures only **valid transitions** are included.\n",
    "\n",
    "## **5. Constructing the Markov Transition Matrix**\n",
    "A **first-order Markov model** is constructed, where each transition probability is estimated as:\n",
    "\n",
    "$$\n",
    "P(G_{i+1} | G_i) = \\frac{N(G_i \\to G_{i+1})}{\\sum_{G_j} N(G_i \\to G_j)}\n",
    "$$\n",
    "\n",
    "where:\n",
    "- $ N(G_i \\to G_{i+1}) $ is the **count** of observed transitions from $ G_i $ to $ G_{i+1} $,\n",
    "- The denominator sums over **all possible** next states.\n",
    "\n",
    "The **transition matrix** $ P $ is structured as:\n",
    "\n",
    "$$\n",
    "P = \\begin{bmatrix}\n",
    "P(G_1 | G_1) & P(G_2 | G_1) & \\dots & P(G_n | G_1) \\\\\n",
    "P(G_1 | G_2) & P(G_2 | G_2) & \\dots & P(G_n | G_2) \\\\\n",
    "\\vdots & \\vdots & \\ddots & \\vdots \\\\\n",
    "P(G_1 | G_n) & P(G_2 | G_n) & \\dots & P(G_n | G_n)\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "This is a **stochastic matrix**, where each row sums to **1**:\n",
    "\n",
    "$$\n",
    "\\sum_{G_{i+1}} P(G_{i+1} | G_i) = 1, \\quad \\forall G_i\n",
    "$$\n",
    "\n",
    "## **6. Predicting the Most Likely Next Grid Cell**\n",
    "For a given grid cell $ G_i $, the predicted **next location** is:\n",
    "\n",
    "$$\n",
    "G_{\\text{predicted}} = \\arg\\max_{G_{i+1}} P(G_{i+1} | G_i)\n",
    "$$\n",
    "\n",
    "This follows a **greedy decision rule**, selecting the most probable transition based on historical data.\n",
    "\n",
    "## **7. Validation Against Post-Depletion Data**\n",
    "For vehicles that **experienced battery depletion**:\n",
    "\n",
    "- The predicted transition is compared to the **actual** next grid cell.\n",
    "- A **correct prediction** is counted if:\n",
    "\n",
    "  $$\n",
    "  G_{\\text{predicted}} = G_{\\text{actual}}\n",
    "  $$\n",
    "\n",
    "- The **prediction accuracy** is computed as:\n",
    "\n",
    "  $$\n",
    "  \\text{Accuracy} = \\frac{\\sum \\mathbb{1}(G_{\\text{predicted}} = G_{\\text{actual}})}{N_{\\text{post-depletion}}}\n",
    "  $$\n",
    "\n",
    "where $ N_{\\text{post-depletion}} $ is the total number of post-depletion data points.\n",
    "\n",
    "## **8. Output and Insights**\n",
    "- The **transition matrix** is saved for further analysis.\n",
    "- The **post-depletion validation results** are stored.\n",
    "- The **top 10 most likely transitions** are displayed, providing insight into dominant movement patterns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## 1ST ORDER MC\n",
    "\n",
    "# Step 1: Assign Vehicles to Grid Cells Using Polygon Containment\n",
    "#df_gdf = gpd.GeoDataFrame(df, geometry=gpd.points_from_xy(df['Log'], df['Lat']), crs=\"EPSG:4326\")\n",
    "\n",
    "# Function to find which grid cell a point belongs to\n",
    "def find_grid_cell(point, grid_gdf):\n",
    "    match = grid_gdf.contains(point.geometry)\n",
    "    if match.any():\n",
    "        return match.idxmax()  # Return index of matching grid cell\n",
    "    else:\n",
    "        return None  # No match found\n",
    "\n",
    "# Apply function to assign each GPS point to a grid cell\n",
    "df_gdf['Grid_Cell'] = df_gdf.apply(lambda row: find_grid_cell(row, grid_gdf), axis=1)\n",
    "\n",
    "# Drop rows where no grid cell was matched (outliers)\n",
    "df_gdf = df_gdf.dropna(subset=['Grid_Cell'])\n",
    "\n",
    "# Step 2: Sort by deviceID and Timestamp for Transition Analysis\n",
    "df_gdf = df_gdf.sort_values(by=['deviceID', 'Timestamp'])\n",
    "\n",
    "# Step 3: Identify Sensor Depletion (SOC_batt < 30)\n",
    "df_gdf['Depleted'] = df_gdf['SOC_batt'] < df_gdf['Safe_SOC_Threshold']\n",
    "\n",
    "# Step 4: Separate Pre- and Post-Depletion Data\n",
    "df_pre_depletion = df_gdf[~df_gdf['Depleted']].copy()\n",
    "df_post_depletion = df_gdf[df_gdf['Depleted']].copy()\n",
    "\n",
    "# Step 5: Create Transitions (from one grid cell to the next)\n",
    "df_pre_depletion['Next_Grid_Cell'] = df_pre_depletion.groupby('deviceID')['Grid_Cell'].shift(-1)\n",
    "\n",
    "# Drop last row per vehicle (no next transition available)\n",
    "df_transitions = df_pre_depletion.dropna(subset=['Next_Grid_Cell'])\n",
    "\n",
    "# Step 6: Build the Markov Transition Matrix\n",
    "transition_counts = (\n",
    "    df_transitions.groupby(['Grid_Cell', 'Next_Grid_Cell'])\n",
    "    .size()\n",
    "    .unstack(fill_value=0)\n",
    ")\n",
    "transition_probabilities = transition_counts.div(transition_counts.sum(axis=1), axis=0)  # Normalize to probabilities\n",
    "\n",
    "# Step 7: Define a Function to Predict the Next Grid Cell\n",
    "def predict_next_grid(current_grid, transition_matrix):\n",
    "    if current_grid in transition_matrix.index:\n",
    "        return transition_matrix.loc[current_grid].idxmax()  # Most likely transition\n",
    "    else:\n",
    "        return None  # No transition data available\n",
    "\n",
    "# Step 8: Validate Predictions Using Post-Depletion Data\n",
    "df_post_depletion['Predicted_Grid_Cell'] = df_post_depletion['Grid_Cell'].apply(\n",
    "    lambda x: predict_next_grid(x, transition_probabilities)\n",
    ")\n",
    "\n",
    "# Step 9: Measure Prediction Accuracy\n",
    "df_post_depletion['Correct_Prediction'] = df_post_depletion['Predicted_Grid_Cell'] == df_post_depletion['Grid_Cell']\n",
    "accuracy = df_post_depletion['Correct_Prediction'].mean()\n",
    "\n",
    "print(f\"Prediction Accuracy: {accuracy:.2%}\")\n",
    "\n",
    "# # Step 10: Save the Transition Matrix and Post-Depletion Validation\n",
    "# output_path_transition_matrix = \"/Users/mayar/Desktop/MIT/Research_Fellow/ENERGY_SENSING/DATA/Transition_Matrix.xlsx\"\n",
    "# transition_probabilities.to_excel(output_path_transition_matrix)\n",
    "\n",
    "# output_path_post_depletion = \"/Users/mayar/Desktop/MIT/Research_Fellow/ENERGY_SENSING/DATA/Post_Depletion_Validation.xlsx\"\n",
    "# df_post_depletion.to_excel(output_path_post_depletion, index=False)\n",
    "\n",
    "# # Step 11: Analyze and Print the Top 10 Most Likely Transitions\n",
    "# most_likely_transitions = (\n",
    "#     transition_probabilities.stack()\n",
    "#     .reset_index()\n",
    "#     .rename(columns={0: 'Probability', 'level_0': 'From_Grid', 'level_1': 'To_Grid'})\n",
    "#     .sort_values(by='Probability', ascending=False)\n",
    "# )\n",
    "\n",
    "# print(\"Top 10 Most Likely Transitions:\")\n",
    "# print(most_likely_transitions.head(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# **🔹 Use Spatial Indexing for Fast Grid Cell Lookup**\n",
    "grid_sindex = grid_gdf.sindex\n",
    "\n",
    "def find_grid_cell(point):\n",
    "    possible_matches = list(grid_sindex.intersection(point.bounds))  # ✅ FIXED: Use `.bounds`\n",
    "    for match in possible_matches:\n",
    "        if grid_gdf.iloc[match].geometry.contains(point):  # ✅ FIXED: Use `point` directly\n",
    "            return grid_gdf.index[match]  # Return grid cell index\n",
    "    return None\n",
    "\n",
    "# **🔹 Apply Fast Lookup in Parallel**\n",
    "df_gdf['Grid_Cell'] = df_gdf['geometry'].map(find_grid_cell)  # ✅ FIXED: Ensure correct `.map()` usage\n",
    "\n",
    "# Drop rows where no grid cell was matched\n",
    "df_gdf.dropna(subset=['Grid_Cell'], inplace=True)\n",
    "\n",
    "# Step 2: Sort by deviceID and Timestamp for Transition Analysis\n",
    "df_gdf.sort_values(by=['deviceID', 'Timestamp'], inplace=True)\n",
    "\n",
    "# Step 3: Identify Sensor Depletion \n",
    "depletion_threshold = df_gdf['Safe_SOC_Threshold']\n",
    "df_gdf['Depleted'] = df_gdf['SOC_batt'] < depletion_threshold\n",
    "\n",
    "# Step 4: Separate Pre- and Post-Depletion Data\n",
    "df_pre_depletion = df_gdf[~df_gdf['Depleted']].copy()\n",
    "df_post_depletion = df_gdf[df_gdf['Depleted']].copy()\n",
    "\n",
    "# Step 5: Compute Transitions Efficiently\n",
    "df_pre_depletion['Next_Grid_Cell'] = df_pre_depletion.groupby('deviceID')['Grid_Cell'].shift(-1)\n",
    "df_transitions = df_pre_depletion.dropna(subset=['Next_Grid_Cell'])\n",
    "\n",
    "# **🔹 Fast Markov Transition Matrix Computation (Numba-Optimized)**\n",
    "@numba.njit(parallel=True)\n",
    "def compute_transition_matrix(transitions):\n",
    "    unique_states = np.unique(transitions)  # Extract unique states\n",
    "    num_states = len(unique_states)\n",
    "    \n",
    "    # **Numba-Compatible Mapping (Replace Dictionary)**\n",
    "    state_map = {state: i for i, state in enumerate(unique_states)}  # Index mapping\n",
    "    \n",
    "    # **Initialize Transition Matrix**\n",
    "    matrix = np.zeros((num_states, num_states), dtype=np.float32)\n",
    "\n",
    "    # **Compute State Transitions in Parallel**\n",
    "    for i in numba.prange(len(transitions) - 1):\n",
    "        if transitions[i] in state_map and transitions[i + 1] in state_map:\n",
    "            from_idx = state_map[transitions[i]]\n",
    "            to_idx = state_map[transitions[i + 1]]\n",
    "            matrix[from_idx, to_idx] += 1  # Increment count\n",
    "\n",
    "    # **Normalize Matrix (Convert to Probabilities)**\n",
    "    row_sums = matrix.sum(axis=1)\n",
    "    for i in range(num_states):\n",
    "        if row_sums[i] > 0:\n",
    "            matrix[i, :] /= row_sums[i]\n",
    "\n",
    "    return matrix, unique_states  # Return transition matrix & corresponding states\n",
    "\n",
    "# **🔹 Convert transitions to NumPy for Fast Processing**\n",
    "transitions_np = df_transitions[['Grid_Cell', 'Next_Grid_Cell']].to_numpy().flatten()\n",
    "\n",
    "# **🚀 Compute Transition Matrix Using Numba**\n",
    "transition_matrix, state_list = compute_transition_matrix(transitions_np)\n",
    "\n",
    "# **Convert Back to Pandas DataFrame**\n",
    "transition_df = pd.DataFrame(transition_matrix, index=state_list, columns=state_list)\n",
    "\n",
    "# **🔹 Step 7: Optimized Prediction Function**\n",
    "def predict_next_grid_batch(grid_cells, transition_df):\n",
    "    return [transition_df.loc[cell].idxmax() if cell in transition_df.index else None for cell in grid_cells]\n",
    "\n",
    "# **Parallel Prediction**\n",
    "df_post_depletion['Predicted_Grid_Cell'] = predict_next_grid_batch(df_post_depletion['Grid_Cell'], transition_df)\n",
    "\n",
    "# **Step 8: Measure Prediction Accuracy**\n",
    "df_post_depletion['Correct_Prediction'] = df_post_depletion['Predicted_Grid_Cell'] == df_post_depletion['Grid_Cell']\n",
    "accuracy = df_post_depletion['Correct_Prediction'].mean()\n",
    "\n",
    "print(f\"Optimized Prediction Accuracy: {accuracy:.2%}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# **Step 1: Convert GPS Data to Geospatial Format**\n",
    "df_gdf = gpd.GeoDataFrame(df, geometry=gpd.points_from_xy(df['Log'], df['Lat']), crs=\"EPSG:4326\")\n",
    "\n",
    "# **🔹 Use Spatial Indexing for Fast Grid Cell Lookup**\n",
    "grid_sindex = grid_gdf.sindex\n",
    "\n",
    "def find_grid_cell(point):\n",
    "    possible_matches = list(grid_sindex.intersection(point.bounds))\n",
    "    for match in possible_matches:\n",
    "        if grid_gdf.iloc[match].geometry.contains(point):\n",
    "            return grid_gdf.index[match]  # Return grid cell index\n",
    "    return None\n",
    "\n",
    "# **🔹 Apply Fast Lookup in Parallel**\n",
    "df_gdf['Grid_Cell'] = df_gdf['geometry'].map(find_grid_cell)\n",
    "df_gdf.dropna(subset=['Grid_Cell'], inplace=True)\n",
    "\n",
    "# **Step 2: Merge Computed Safe SOC Thresholds**\n",
    "df_gdf = df_gdf.merge(safe_soc_thresholds_df, on=['deviceID', 'Timestamp'], how='left')\n",
    "\n",
    "# **Step 3: Sort by deviceID and Timestamp for Transition Analysis**\n",
    "df_gdf.sort_values(by=['deviceID', 'Timestamp'], inplace=True)\n",
    "\n",
    "# **Step 4: Identify Sensor Depletion Using Dynamic Safe SOC**\n",
    "df_gdf['Depleted'] = df_gdf['SOC_batt'] < df_gdf['Safe_SOC_Threshold']  # ✅ Replaced static threshold\n",
    "\n",
    "# **Step 5: Separate Pre- and Post-Depletion Data**\n",
    "df_pre_depletion = df_gdf[~df_gdf['Depleted']].copy()\n",
    "df_post_depletion = df_gdf[df_gdf['Depleted']].copy()\n",
    "\n",
    "# **Step 6: Compute Next Unique Grid Cell for the Entire Dataset**\n",
    "def find_next_different(series):\n",
    "    \"\"\"Finds the next different grid cell for each row within a deviceID group.\"\"\"\n",
    "    next_values = series.to_numpy()\n",
    "    result = np.full_like(next_values, fill_value=np.nan, dtype=np.float64)\n",
    "\n",
    "    for i in range(len(next_values) - 1):  \n",
    "        for j in range(i + 1, len(next_values)):  # Look ahead to find the next different value\n",
    "            if next_values[j] != next_values[i]:  \n",
    "                result[i] = next_values[j]  # Assign the first different value found\n",
    "                break  \n",
    "\n",
    "    return pd.Series(result, index=series.index)  # Ensure index remains unchanged\n",
    "\n",
    "# ✅ Apply Next Unique Grid Cell to the Entire Dataset (NOT JUST df_pre_depletion)\n",
    "df_gdf['Next_Grid_Cell'] = df_gdf.groupby('deviceID')['Grid_Cell'].transform(find_next_different)\n",
    "df_transitions = df_gdf.dropna(subset=['Next_Grid_Cell'])  # Train on full dataset\n",
    "\n",
    "# **🔹 Fast Markov Transition Matrix Computation (Numba-Optimized)**\n",
    "@numba.njit(parallel=True)\n",
    "def compute_transition_matrix(transitions):\n",
    "    unique_states = np.unique(transitions)  \n",
    "    num_states = len(unique_states)\n",
    "\n",
    "    # **Numba-Compatible Mapping**\n",
    "    state_map = {state: i for i, state in enumerate(unique_states)}\n",
    "\n",
    "    # **Initialize Transition Matrix**\n",
    "    matrix = np.zeros((num_states, num_states), dtype=np.float32)\n",
    "\n",
    "    # **Compute State Transitions in Parallel**\n",
    "    for i in numba.prange(len(transitions) - 1):\n",
    "        if transitions[i] in state_map and transitions[i + 1] in state_map:\n",
    "            from_idx = state_map[transitions[i]]\n",
    "            to_idx = state_map[transitions[i + 1]]\n",
    "            matrix[from_idx, to_idx] += 1  # Increment count\n",
    "\n",
    "    # **Normalize Matrix (Convert to Probabilities)**\n",
    "    row_sums = matrix.sum(axis=1)\n",
    "    for i in range(num_states):\n",
    "        if row_sums[i] > 0:\n",
    "            matrix[i, :] /= row_sums[i]\n",
    "\n",
    "    return matrix, unique_states  \n",
    "\n",
    "# **🔹 Convert transitions to NumPy for Fast Processing**\n",
    "transitions_np = df_transitions[['Grid_Cell', 'Next_Grid_Cell']].to_numpy().flatten()\n",
    "\n",
    "# **🚀 Compute Transition Matrix Using Numba**\n",
    "transition_matrix, state_list = compute_transition_matrix(transitions_np)\n",
    "\n",
    "# **Convert Back to Pandas DataFrame**\n",
    "transition_df = pd.DataFrame(transition_matrix, index=state_list, columns=state_list)\n",
    "\n",
    "# **🔹 Step 7: Optimized Prediction Function**\n",
    "def predict_next_grid_batch(grid_cells, transition_df):\n",
    "    return [transition_df.loc[cell].idxmax() if cell in transition_df.index else None for cell in grid_cells]\n",
    "\n",
    "# **Use `Next_Grid_Cell` from Full Dataset for Predictions**\n",
    "df_post_depletion['Next_Grid_Cell'] = df_post_depletion.groupby('deviceID')['Grid_Cell'].transform(find_next_different)\n",
    "\n",
    "# **Parallel Prediction**\n",
    "df_post_depletion['Predicted_Grid_Cell'] = predict_next_grid_batch(df_post_depletion['Next_Grid_Cell'], transition_df)\n",
    "\n",
    "# ✅ Reorder Columns to Place Next_Grid_Cell After Depleted\n",
    "column_order = df_post_depletion.columns.tolist()\n",
    "column_order.remove('Next_Grid_Cell')  # Remove if exists\n",
    "depleted_index = column_order.index('Depleted')\n",
    "column_order.insert(depleted_index + 1, 'Next_Grid_Cell')  # Insert after Depleted\n",
    "df_post_depletion = df_post_depletion[column_order]  # Apply new column order\n",
    "\n",
    "# **Step 8: Measure Prediction Accuracy**\n",
    "df_post_depletion['Correct_Prediction'] = (\n",
    "    df_post_depletion['Predicted_Grid_Cell'].fillna(\"\").astype(str)\n",
    "    == df_post_depletion['Grid_Cell'].fillna(\"\").astype(str)\n",
    ")\n",
    "accuracy = df_post_depletion['Correct_Prediction'].mean()\n",
    "\n",
    "print(f\"Optimized Prediction Accuracy: {accuracy:.2%}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save DataFrame to Excel file\n",
    "df_post_depletion.to_excel(\"/workspace/data/df_post_depletion_1.xlsx\", index=False)\n",
    "df_pre_depletion.to_excel(\"/workspace/data/df_pre_depletion_1.xlsx\", index=False)\n",
    "\n",
    "# Return file path for download\n",
    "file_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# **Step 1: Convert GPS Data to Geospatial Format**\n",
    "df_gdf = gpd.GeoDataFrame(df, geometry=gpd.points_from_xy(df['Log'], df['Lat']), crs=\"EPSG:4326\")\n",
    "\n",
    "# **Step 2: Assign Vehicles to Grid Cells Using Spatial Index**\n",
    "grid_sindex = grid_gdf.sindex  \n",
    "\n",
    "def find_grid_cell(point):\n",
    "    \"\"\"Find the grid cell containing the given point.\"\"\"\n",
    "    possible_matches = list(grid_sindex.intersection(point.bounds))\n",
    "    for match in possible_matches:\n",
    "        if grid_gdf.iloc[match].geometry.contains(point):\n",
    "            return grid_gdf.index[match]  \n",
    "    return None  \n",
    "\n",
    "df_gdf['Grid_Cell'] = df_gdf['geometry'].map(find_grid_cell)\n",
    "df_gdf.dropna(subset=['Grid_Cell'], inplace=True)\n",
    "\n",
    "# **Step 3: Merge Safe SOC Thresholds**\n",
    "df_gdf = df_gdf.merge(safe_soc_thresholds_df, on=['deviceID', 'Timestamp'], how='left')\n",
    "\n",
    "# **Step 4: Sort and Identify Sensor Depletion**\n",
    "df_gdf.sort_values(by=['deviceID', 'Timestamp'], inplace=True)\n",
    "df_gdf['Depleted'] = df_gdf['SOC_batt'] < df_gdf['Safe_SOC_Threshold']\n",
    "\n",
    "# **Step 5: Separate Pre- and Post-Depletion Data**\n",
    "df_pre_depletion = df_gdf[~df_gdf['Depleted']].copy()\n",
    "df_post_depletion = df_gdf[df_gdf['Depleted']].copy()\n",
    "\n",
    "# **Step 6: Compute Transition Matrices**\n",
    "max_transitions_per_device = df_pre_depletion.groupby('deviceID')['Grid_Cell'].count().min()\n",
    "MAX_ORDER = max(1, min(max_transitions_per_device - 1, 3))  \n",
    "print(f\"🔹 Automatically Set MAX_ORDER = {MAX_ORDER}\")\n",
    "\n",
    "def find_next_different(series):\n",
    "    \"\"\"Finds the next different grid cell for each row within a deviceID group.\"\"\"\n",
    "    next_values = series.to_numpy()  # Convert to NumPy array for efficiency\n",
    "    result = np.full_like(next_values, fill_value=np.nan, dtype=np.float64)  # Initialize result\n",
    "\n",
    "    # Loop through each value in the series\n",
    "    for i in range(len(next_values) - 1):  \n",
    "        for j in range(i + 1, len(next_values)):  # Look ahead to find the next different value\n",
    "            if next_values[j] != next_values[i]:  \n",
    "                result[i] = next_values[j]  # Assign the first different value found\n",
    "                break  \n",
    "\n",
    "    return pd.Series(result, index=series.index)  # Ensure index remains unchanged\n",
    "\n",
    "# ✅ Apply it safely without breaking the DataFrame index\n",
    "df_pre_depletion['Next_Grid_Cell'] = df_pre_depletion.groupby('deviceID')['Grid_Cell'].transform(find_next_different)\n",
    "\n",
    "def find_previous_different(series):\n",
    "    \"\"\"Finds the last different grid cell for each row within a deviceID group.\"\"\"\n",
    "    prev_values = series.to_numpy()  # Convert to NumPy array for efficiency\n",
    "    result = np.full_like(prev_values, fill_value=np.nan, dtype=np.float64)  # Initialize result\n",
    "\n",
    "    for i in range(1, len(prev_values)):  \n",
    "        for j in range(i - 1, -1, -1):  # Look backward to find the last different value\n",
    "            if prev_values[j] != prev_values[i]:  \n",
    "                result[i] = prev_values[j]  # Assign the first different value found\n",
    "                break  \n",
    "\n",
    "    return pd.Series(result, index=series.index)\n",
    "\n",
    "# ✅ Apply for the first order\n",
    "df_pre_depletion['Prev_Grid_Cell_1'] = df_pre_depletion.groupby('deviceID')['Grid_Cell'].transform(find_previous_different)\n",
    "\n",
    "# ✅ Build higher orders iteratively, stopping when history runs out\n",
    "for order in range(2, MAX_ORDER + 1):\n",
    "    prev_col = f'Prev_Grid_Cell_{order - 1}'  # Previous order column\n",
    "    new_col = f'Prev_Grid_Cell_{order}'\n",
    "\n",
    "    # Only compute next order for rows where the previous order is NOT NaN\n",
    "    df_pre_depletion[new_col] = df_pre_depletion.groupby('deviceID')[prev_col].transform(\n",
    "        lambda x: find_previous_different(x) if x.notna().any() else np.nan\n",
    "    )\n",
    "\n",
    "# **Step 7: Compute Transition Matrices for Each Order with Laplace Smoothing**\n",
    "alpha = 0.01  \n",
    "transition_probabilities = {}\n",
    "\n",
    "# ✅ Ensure First-Order Matches Standalone Model\n",
    "transition_probabilities[1] = transition_df.copy()  # Directly use standalone first-order model\n",
    "\n",
    "# ✅ Compute Higher-Order Matrices\n",
    "for order in range(2, MAX_ORDER + 1):\n",
    "    prev_cols = [f'Prev_Grid_Cell_{i}' for i in range(order, 0, -1)]\n",
    "    required_cols = prev_cols + ['Grid_Cell', 'Next_Grid_Cell']\n",
    "    df_transitions = df_pre_depletion.dropna(subset=required_cols)\n",
    "\n",
    "    if not df_transitions.empty:\n",
    "        transition_counts = df_transitions.groupby(required_cols).size().unstack(fill_value=0)\n",
    "        row_sums = transition_counts.sum(axis=1)\n",
    "        transition_probabilities[order] = (transition_counts).div(row_sums, axis=0).fillna(0)\n",
    "\n",
    "# **Step 8: Define Hybrid N-th Order Prediction Function**\n",
    "def predict_next_grid_n_order(prev_grids, current_grid, transition_probabilities):\n",
    "    \"\"\"Predict the next grid cell using the highest available Markov order, gradually falling back.\"\"\"\n",
    "    print(f\"🔍 Predicting for {current_grid} with history: {prev_grids}\")\n",
    "\n",
    "    # Try the highest available order first\n",
    "    for order in range(len(prev_grids), 0, -1):  \n",
    "        key = (current_grid,) if order == 1 else tuple(prev_grids[-order:]) + (current_grid,)\n",
    "\n",
    "        if order in transition_probabilities and key in transition_probabilities[order].index:\n",
    "            print(f\"✅ Used Order {order} for {current_grid} (Key: {key})\")\n",
    "            return transition_probabilities[order].loc[key].idxmax()\n",
    "\n",
    "    # Fall back to first-order correctly\n",
    "    if 1 in transition_probabilities and current_grid in transition_probabilities[1].index:\n",
    "        print(f\"⚠️ Falling back to Order 1 for {current_grid} (Key: ({current_grid},))\")\n",
    "        return transition_probabilities[1].loc[current_grid].idxmax()\n",
    "\n",
    "    # If everything fails, use most frequent transition\n",
    "    if 1 in transition_probabilities and not transition_probabilities[1].empty:\n",
    "        most_common_transition = transition_probabilities[1].sum(axis=1).idxmax()\n",
    "        print(f\"🔹 Choosing most frequent transition for {current_grid}: {most_common_transition}\")\n",
    "        return most_common_transition\n",
    "\n",
    "    print(f\"🚨 No prediction found for {current_grid}. Returning Unknown.\")\n",
    "    return \"Unknown\"\n",
    "\n",
    "# **Step 9: Apply Hybrid N-th Order Prediction to Post-Depletion Data**\n",
    "prev_grid_columns = [f'Prev_Grid_Cell_{order}' for order in range(1, MAX_ORDER + 1)]\n",
    "# **Step 9: Apply Hybrid N-th Order Prediction to Post-Depletion Data**\n",
    "prev_grid_columns = [f'Prev_Grid_Cell_{order}' for order in range(1, MAX_ORDER + 1)]\n",
    "\n",
    "# ✅ Apply first order correctly\n",
    "df_post_depletion['Prev_Grid_Cell_1'] = df_post_depletion.groupby('deviceID')['Grid_Cell'].transform(find_previous_different)\n",
    "\n",
    "# ✅ Apply higher orders while ensuring correct stopping\n",
    "for order in range(2, MAX_ORDER + 1):\n",
    "    prev_col = f'Prev_Grid_Cell_{order - 1}'  # Previous order column\n",
    "    new_col = f'Prev_Grid_Cell_{order}'\n",
    "\n",
    "    # Only compute for rows where the previous order is NOT NaN\n",
    "    df_post_depletion[new_col] = df_post_depletion.groupby('deviceID')[prev_col].transform(\n",
    "        lambda x: find_previous_different(x) if x.notna().any() else np.nan\n",
    "    )\n",
    "\n",
    "def safe_predict(row):\n",
    "    prev_grids = [row[col] for col in prev_grid_columns if pd.notna(row[col])]\n",
    "\n",
    "    if not prev_grids:  \n",
    "        return np.nan\n",
    "\n",
    "    predicted = predict_next_grid_n_order(prev_grids, row['Grid_Cell'], transition_probabilities)\n",
    "    return predicted if isinstance(predicted, (int, str, float)) else \"Unknown\"\n",
    "\n",
    "# ✅ Debugging Function\n",
    "def debug_prediction(prev_grids, current_grid, transition_probabilities):\n",
    "    for order in range(len(prev_grids), 0, -1):\n",
    "        key = (current_grid,) if order == 1 else tuple(prev_grids[-order:]) + (current_grid,)\n",
    "\n",
    "        if order in transition_probabilities and key in transition_probabilities[order].index:\n",
    "            print(f\"✅ Order {order} used for {current_grid} (Key: {key})\")\n",
    "            return transition_probabilities[order].loc[key].idxmax()\n",
    "\n",
    "    print(f\"⚠️ No prediction found for {current_grid} with {prev_grids}. Falling back...\")\n",
    "    return None\n",
    "\n",
    "# **Apply Prediction**\n",
    "df_post_depletion['Predicted_Grid_Cell_Hybrid'] = df_post_depletion.apply(\n",
    "    lambda row: safe_predict(row), axis=1\n",
    ")\n",
    "df_post_depletion['Predicted_Grid_Cell_Hybrid'].fillna(\"Unknown\", inplace=True)\n",
    "\n",
    "# **Step 10: Measure Prediction Accuracy**\n",
    "df_post_depletion['Correct_Prediction_Hybrid'] = df_post_depletion['Predicted_Grid_Cell_Hybrid'].eq(df_post_depletion['Grid_Cell'])\n",
    "accuracy_hybrid = df_post_depletion['Correct_Prediction_Hybrid'].mean()\n",
    "\n",
    "print(f\"Hybrid N-th Order Markov Chain Prediction Accuracy: {accuracy_hybrid:.2%}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save DataFrame to Excel file\n",
    "df_post_depletion.to_excel(\"/workspace/data/df_post_depletion.xlsx\", index=False)\n",
    "df_pre_depletion.to_excel(\"/workspace/data/df_pre_depletion.xlsx\", index=False)\n",
    "\n",
    "# Return file path for download\n",
    "file_path\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check how many transitions exist at each order\n",
    "for order in transition_probabilities:\n",
    "    print(f\"📊 Order {order}: {len(transition_probabilities[order])} transitions recorded\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Hybrid N-th Order Markov Chain Prediction Accuracy: {accuracy_hybrid:.2%}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract entropy values\n",
    "orders = list(transition_probabilities.keys())\n",
    "entropy_values = []\n",
    "\n",
    "for order in orders:\n",
    "    probs = transition_probabilities[order]\n",
    "    entropy = -np.nansum(probs * np.log2(probs))\n",
    "    entropy_values.append(entropy)\n",
    "    print(f\"🔹 Entropy (Order {order}): {entropy:.4f}\")\n",
    "\n",
    "# Convert orders and entropy values to numpy arrays\n",
    "orders_array = np.array(orders)\n",
    "entropy_array = np.array(entropy_values)\n",
    "\n",
    "# Normalize the values for better numerical stability\n",
    "orders_norm = (orders_array - orders_array.min()) / (orders_array.max() - orders_array.min())\n",
    "entropy_norm = (entropy_array - entropy_array.min()) / (entropy_array.max() - entropy_array.min())\n",
    "\n",
    "# Define the line from the first to the last point\n",
    "line_vec = np.array([orders_norm[-1] - orders_norm[0], entropy_norm[-1] - entropy_norm[0]])\n",
    "line_vec_norm = line_vec / np.linalg.norm(line_vec)\n",
    "\n",
    "# Compute distances of each point from the line\n",
    "distances = []\n",
    "for i in range(len(orders_norm)):\n",
    "    point_vec = np.array([orders_norm[i] - orders_norm[0], entropy_norm[i] - entropy_norm[0]])\n",
    "    proj = np.dot(point_vec, line_vec_norm) * line_vec_norm\n",
    "    dist_vec = point_vec - proj\n",
    "    distances.append(np.linalg.norm(dist_vec))\n",
    "\n",
    "# Find the elbow point as the max distance from the line\n",
    "elbow_index = np.argmax(distances)\n",
    "elbow_order = orders[elbow_index]\n",
    "\n",
    "# Plot entropy vs. Markov order with elbow point\n",
    "plt.figure(figsize=(8, 5))\n",
    "plt.plot(orders, entropy_values, marker='o', linestyle='-', color='b', label=\"Entropy\")\n",
    "plt.axvline(x=elbow_order, color='r', linestyle='--', label=f'Elbow at N={elbow_order}')\n",
    "plt.xlabel(\"Markov Chain Order (N)\")\n",
    "plt.ylabel(\"Entropy\")\n",
    "plt.title(\"Entropy vs. Markov Order with Corrected Elbow Point\")\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()\n",
    "\n",
    "# Return the detected elbow order\n",
    "print(f\"🔹 Optimal order is {elbow_order}\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import scipy.stats as stats\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import mutual_info_score\n",
    "\n",
    "# **Step 1: Compute Mutual Information (MI)**\n",
    "def compute_mutual_information(df, prev_grid_col, next_grid_col='Next_Grid_Cell'):\n",
    "    valid_df = df.dropna(subset=[prev_grid_col, next_grid_col])\n",
    "    return mutual_info_score(valid_df[prev_grid_col], valid_df[next_grid_col])\n",
    "\n",
    "# **Step 2: Compute Conditional Entropy (Fixed)**\n",
    "def compute_conditional_entropy(df, prev_grid_col, next_grid_col='Next_Grid_Cell'):\n",
    "    contingency_table = pd.crosstab(df[prev_grid_col], df[next_grid_col])\n",
    "    probs = contingency_table.div(contingency_table.sum(axis=1), axis=0)\n",
    "\n",
    "    # 🔹 Fix: Replace zero values with NaN to avoid log(0) issues\n",
    "    probs = probs.replace(0, np.nan)\n",
    "    \n",
    "    entropy = -np.nansum(probs * np.log2(probs))  # Ignore NaN values in computation\n",
    "    return entropy\n",
    "\n",
    "# **Step 3: Chi-Square Test for Independence**\n",
    "def chi_square_test(df, prev_grid_col, next_grid_col='Next_Grid_Cell'):\n",
    "    contingency_table = pd.crosstab(df[prev_grid_col], df[next_grid_col])\n",
    "    chi2_stat, p_value, _, _ = stats.chi2_contingency(contingency_table)\n",
    "    return chi2_stat, p_value\n",
    "\n",
    "# Assuming df_pre_depletion is already defined in the environment\n",
    "orders = list(range(1, 31))\n",
    "\n",
    "# Initialize dictionaries to store values\n",
    "mi_scores = {}\n",
    "conditional_entropy_scores = {}\n",
    "chi_square_results = {}\n",
    "\n",
    "for order in orders:\n",
    "    prev_col = f'Prev_Grid_Cell_{order}'\n",
    "\n",
    "    mi_scores[order] = compute_mutual_information(df_pre_depletion, prev_col)\n",
    "    conditional_entropy_scores[order] = compute_conditional_entropy(df_pre_depletion, prev_col)\n",
    "    chi2_stat, p_value = chi_square_test(df_pre_depletion, prev_col)\n",
    "    chi_square_results[order] = (chi2_stat, p_value)\n",
    "\n",
    "# Convert results into lists for plotting\n",
    "mi_values = list(mi_scores.values())\n",
    "conditional_entropy_values = list(conditional_entropy_scores.values())\n",
    "chi_square_values = [chi_square_results[o][0] for o in orders]\n",
    "\n",
    "# **Step 5: Plot the Results with Dual Y-Axis**\n",
    "fig, ax1 = plt.subplots(figsize=(10, 5))\n",
    "\n",
    "# Plot Mutual Information on secondary Y-axis\n",
    "ax2 = ax1.twinx()\n",
    "ax2.plot(orders, mi_values, marker='o', linestyle='-', color='b', label=\"Mutual Information\")\n",
    "ax2.set_ylabel(\"Mutual Information\", color='b')\n",
    "ax2.tick_params(axis='y', labelcolor='b')\n",
    "\n",
    "# Plot Conditional Entropy and Chi-Square Statistic on primary Y-axis\n",
    "ax1.plot(orders, chi_square_values, marker='^', linestyle='-', color='r', label=\"Chi-Square Statistic\")\n",
    "\n",
    "ax1.set_xlabel(\"Markov Chain Order (N)\")\n",
    "ax1.set_ylabel(\"Score (Entropy & Chi-Square)\", color='r')\n",
    "ax1.tick_params(axis='y', labelcolor='r')\n",
    "\n",
    "# Tertiary Y-Axis (Conditional Entropy)\n",
    "ax3 = ax1.twinx()\n",
    "ax3.spines['right'].set_position(('outward', 60))  # Move third axis outward\n",
    "ax3.plot(orders, conditional_entropy_values, marker='s', linestyle='-', color='g', label=\"Conditional Entropy\")\n",
    "ax3.set_ylabel(\"Conditional Entropy\", color='g')\n",
    "ax3.tick_params(axis='y', labelcolor='g')\n",
    "\n",
    "# Title and Legend\n",
    "fig.suptitle(\"MI, Conditional Entropy, and Chi-Square vs. Markov Order\")\n",
    "fig.legend(loc=\"upper right\", bbox_to_anchor=(1,1), bbox_transform=ax1.transAxes)\n",
    "\n",
    "plt.grid(True)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **GPU-Accelerated Q-Learning for Grid-Based Mobility Prediction**\n",
    "\n",
    "This implementation leverages **GPU-accelerated computing** for **spatial binning, sensor depletion analysis, and reinforcement learning-based mobility prediction**. The core objective is to **train an agent to predict the next grid cell occupied by a vehicle after battery depletion**, based on historical transitions.\n",
    "\n",
    "## **1. Data Loading and Conversion to cuDF for GPU Acceleration**\n",
    "To facilitate **large-scale spatiotemporal data processing**, the dataset is converted into a **cuDF DataFrame**, enabling computations on NVIDIA GPUs via **RAPIDS cuDF**. \n",
    "\n",
    "Given an original DataFrame **$df$** containing numerical and categorical attributes, each numerical column is converted to **floating-point representation** while categorical variables ($deviceID$ and $Date$) remain as strings. The transformation ensures efficient memory alignment for GPU operations.\n",
    "\n",
    "## **2. Grid Binning Using cuSpatial**\n",
    "A **spatial discretization strategy** is applied to **map GPS coordinates into a structured 120m × 120m grid**. The Earth's curvature necessitates an **adaptive longitude resolution**, computed dynamically based on latitude.\n",
    "\n",
    "### **Latitude and Longitude Transformation**\n",
    "For a given latitude $ \\text{Lat}_i $ and longitude $ \\text{Log}_i $, the coordinates are mapped into grid indices as follows:\n",
    "\n",
    "$$\n",
    "\\text{Grid}_X = \\left\\lfloor \\frac{(\\text{Log}_i - \\text{Log}_{\\min}) \\cdot \\pi / 180 \\cdot R_E \\cdot \\cos(\\text{Lat}_i)}{\\text{grid size}} \\right\\rfloor\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\text{Grid}_Y = \\left\\lfloor \\frac{(\\text{Lat}_i - \\text{Lat}_{\\min}) \\cdot \\pi / 180 \\cdot R_E}{\\text{grid size}} \\right\\rfloor\n",
    "$$\n",
    "\n",
    "where:\n",
    "- $ R_E = 6371000 $ m is Earth's radius,\n",
    "- The **longitude transformation** incorporates the **cosine of latitude** to account for Earth's curvature,\n",
    "- The floor function ensures that values are discretized into **integer grid indices**.\n",
    "\n",
    "Each spatial point is **hashed into a grid cell identifier**:\n",
    "\n",
    "$$\n",
    "\\text{Grid\\_Cell}_i = (\\text{Grid}_X, \\text{Grid}_Y)\n",
    "$$\n",
    "\n",
    "## **3. Sensor Depletion Detection**\n",
    "A **binary depletion flag** is assigned to each vehicle record:\n",
    "\n",
    "$$\n",
    "D_i =\n",
    "\\begin{cases} \n",
    "1, & \\text{if } SOC_{\\text{batt}, i} < 20\\% \\\\\n",
    "0, & \\text{otherwise}\n",
    "\\end{cases}\n",
    "$$\n",
    "\n",
    "where **$ SOC_{\\text{batt}} $** is the state of charge. The dataset is split into:\n",
    "- **Pre-depletion** data **$P$**, containing normal mobility patterns.\n",
    "- **Post-depletion** data **$Q$**, capturing movement after battery exhaustion.\n",
    "\n",
    "The next grid cell for each pre-depletion record is assigned using a **time-ordered shift operation**:\n",
    "\n",
    "$$\n",
    "G_{i+1} = \\text{shift}(G_i, -1)\n",
    "$$\n",
    "\n",
    "ensuring that transitions are correctly captured.\n",
    "\n",
    "## **4. Reinforcement Learning (Q-Learning) for Mobility Prediction**\n",
    "\n",
    "The goal of this reinforcement learning (RL) framework is to train an agent that learns **optimal movement patterns** based on past trajectories and predicts the most probable **next grid cell** a vehicle will occupy after battery depletion. The agent is trained using **Q-learning**, a model-free reinforcement learning algorithm that iteratively updates **Q-values** representing the expected reward for selecting an action (i.e., moving to a new grid cell) from a given state.\n",
    "\n",
    "### **State and Action Representation**\n",
    "- The **state space** consists of all possible **grid cells** $ s \\in S $, where each grid cell is a **120m × 120m spatial unit** indexed as $(X, Y)$.  \n",
    "- The **action space** consists of transitions to **neighboring grid cells**, corresponding to potential movements between states.  \n",
    "\n",
    "Each transition is extracted from **pre-depletion data**, where each record consists of:\n",
    "1. **Current grid cell** $ G_i $\n",
    "2. **Next observed grid cell** $ G_{i+1} $\n",
    "3. **State of charge (SOC)** and depletion flag\n",
    "\n",
    "For each **state-action pair** $(s, a)$, we maintain a **Q-value** $ Q(s, a) $, which represents the estimated cumulative reward expected when selecting action $ a $ from state $ s $.\n",
    "\n",
    "### **Q-Value Update Rule**\n",
    "The agent updates its **Q-values** iteratively using the Bellman equation:\n",
    "\n",
    "$$\n",
    "Q(s, a) = (1 - \\alpha) Q(s, a) + \\alpha \\left( r + \\gamma \\max_{a'} Q(s', a') \\right)\n",
    "$$\n",
    "\n",
    "where:\n",
    "- **$ \\alpha $ (learning rate)** determines how much the newly acquired information overrides the existing Q-value.\n",
    "- **$ \\gamma $ (discount factor)** determines the importance of future rewards.\n",
    "- **$ r $ (reward function)** assigns a numerical value to each transition, encouraging movement patterns that match realistic trajectories.\n",
    "- **$ \\max_{a'} Q(s', a') $** represents the highest Q-value of possible actions in the next state $ s' $, guiding the agent toward high-reward decisions.\n",
    "\n",
    "### **Reward Function**\n",
    "To ensure realistic movement, the **reward function** incorporates both **empirical transition frequency** and **spatial coherence**:\n",
    "\n",
    "$$\n",
    "r = \\log (N(s, a) + 1) + 0.01 + 0.5 \\cdot e^{-\\frac{||s - a||}{\\text{grid size}}}\n",
    "$$\n",
    "\n",
    "where:\n",
    "- $ N(s, a) $ is the number of observed transitions from $ s $ to $ a $ in the dataset.\n",
    "- The **logarithmic term** prevents highly frequent transitions from dominating the learning process.\n",
    "- The **exponential decay term** penalizes large jumps, encouraging spatially coherent movement.\n",
    "\n",
    "### **Exploration vs. Exploitation Policy**\n",
    "The agent follows an **$ \\epsilon $-greedy policy**, balancing exploration (random movement selection) and exploitation (selecting the best known action):\n",
    "\n",
    "$$\n",
    "a =\n",
    "\\begin{cases} \n",
    "\\text{random action}, & \\text{with probability } \\epsilon \\\\\n",
    "\\arg\\max_{a} Q(s, a), & \\text{otherwise}\n",
    "\\end{cases}\n",
    "$$\n",
    "\n",
    "where **$ \\epsilon $** is the exploration probability, which **decays over time** to prioritize exploitation:\n",
    "\n",
    "$$\n",
    "\\epsilon = \\max(0.05, \\epsilon \\cdot \\text{decay factor})\n",
    "$$\n",
    "\n",
    "### **Batch Training Strategy**\n",
    "Instead of updating Q-values one transition at a time, **mini-batch updates** are applied using **vectorized operations**:\n",
    "- A batch size of **1000 transitions** is selected per iteration.\n",
    "- Transitions are processed in parallel using **GPU acceleration**.\n",
    "- Each batch computes transition counts and updates the **Q-table** accordingly.\n",
    "\n",
    "## **5. Hyperparameter Optimization Using Bayesian Search**\n",
    "To optimize **Q-learning performance**, we conduct **Bayesian optimization** over the following hyperparameters:\n",
    "- **$ \\alpha $ (learning rate) in $ [0.1, 0.5] $**: Controls how aggressively Q-values are updated.\n",
    "- **$ \\gamma $ (discount factor) in $ [0.5, 0.9] $**: Adjusts how much future rewards impact current decisions.\n",
    "- **$ \\epsilon $ (exploration probability) in $ [0.1, 0.3] $**: Governs the randomness of action selection.\n",
    "- **$ \\epsilon_{\\text{decay}} $ in $ [0.98, 0.999] $**: Ensures a smooth transition from exploration to exploitation.\n",
    "- **Number of training iterations in $ [2,10] $**: Determines how long the agent learns from past data.\n",
    "\n",
    "The optimization process **minimizes the loss function**:\n",
    "\n",
    "$$\n",
    "\\mathcal{L} = -\\text{Accuracy}\n",
    "$$\n",
    "\n",
    "where accuracy is computed as:\n",
    "\n",
    "$$\n",
    "\\text{Accuracy} = \\frac{\\sum \\mathbb{1} (G_{\\text{predicted}} = G_{\\text{actual}})}{N_{\\text{post-depletion}}}\n",
    "$$\n",
    "\n",
    "Bayesian search uses **tree-structured Parzen estimators (TPE)** to efficiently explore the hyperparameter space.\n",
    "\n",
    "## **6. RL-Based Mobility Prediction**\n",
    "Once training is complete, the **Q-table is used for inference** to predict the most likely **next grid cell** after depletion:\n",
    "\n",
    "$$\n",
    "G_{\\text{predicted}} = \\arg\\max_{a} Q(G_{\\text{current}}, a)\n",
    "$$\n",
    "\n",
    "A final evaluation step compares the predicted post-depletion locations with the actual recorded locations. The **model terminates training** if:\n",
    "\n",
    "$$\n",
    "\\text{Accuracy} \\geq 85\\%\n",
    "$$\n",
    "\n",
    "ensuring that the agent achieves a **sufficiently high prediction accuracy**.\n",
    "\n",
    "## **7. Conclusion**\n",
    "This approach leverages:\n",
    "- **Q-learning with batch training**, enabling efficient large-scale learning.\n",
    "- **GPU acceleration via RAPIDS cuDF**, significantly reducing training time.\n",
    "- **Bayesian hyperparameter tuning**, optimizing Q-learning efficiency.\n",
    "- **Adaptive exploration-exploitation balance**, refining the model over iterations.\n",
    "\n",
    "The final model provides **high-accuracy mobility predictions**, supporting **real-time sensor deployment planning and energy-aware urban mobility analysis**.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The below is the final RL framework"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import cudf\n",
    "import cupy as cp\n",
    "import cuspatial\n",
    "import random\n",
    "from hyperopt import fmin, tpe, hp, STATUS_OK, Trials\n",
    "import shapely.geometry\n",
    "import geopandas as gpd\n",
    "\n",
    "# ==========================\n",
    "# 🔹 Load and Convert Data to cuDF\n",
    "# ==========================\n",
    "dfcopied = df.copy()\n",
    "\n",
    "# Convert numeric columns\n",
    "for col in dfcopied.columns:\n",
    "    if dfcopied[col].dtype == 'object' and col not in ['deviceID', 'Date']:  \n",
    "        dfcopied[col] = pd.to_numeric(dfcopied[col], errors='coerce')\n",
    "\n",
    "dfcopied['deviceID'] = dfcopied['deviceID'].astype(str)  \n",
    "dfcopied['Date'] = dfcopied['Date'].astype(str)  \n",
    "\n",
    "# Convert to cuDF for GPU processing\n",
    "df_gdf = cudf.DataFrame(dfcopied)\n",
    "\n",
    "# Ensure deviceID and Date remain strings\n",
    "df_gdf['deviceID'] = df_gdf['deviceID'].astype('str')\n",
    "df_gdf['Date'] = df_gdf['Date'].astype('str')\n",
    "\n",
    "# ==========================\n",
    "# 🔹 Define Grid Binning Using cuSpatial\n",
    "# ==========================\n",
    "# Define spatial bounding box\n",
    "min_x, max_x = df_gdf['Log'].min(), df_gdf['Log'].max()\n",
    "min_y, max_y = df_gdf['Lat'].min(), df_gdf['Lat'].max()\n",
    "\n",
    "EARTH_RADIUS = 6371000  # Earth radius in meters\n",
    "\n",
    "# Convert lat/lon degrees to meters using Haversine formula approximation\n",
    "df_gdf['Grid_X'] = ((df_gdf['Log'] - min_x) * (np.pi/180) * EARTH_RADIUS * np.cos(np.radians(df_gdf['Lat']))).astype(int) // grid_size\n",
    "df_gdf['Grid_Y'] = ((df_gdf['Lat'] - min_y) * (np.pi/180) * EARTH_RADIUS).astype(int) // grid_size\n",
    "\n",
    "df_gdf['Grid_Cell'] = df_gdf['Grid_X'].astype(str) + \"_\" + df_gdf['Grid_Y'].astype(str)\n",
    "\n",
    "# Sort by deviceID and Timestamp for Transition Analysis\n",
    "df_gdf = df_gdf.sort_values(by=['deviceID', 'Timestamp'])\n",
    "\n",
    "# ==========================\n",
    "# 🔹 Identify Sensor Depletion\n",
    "# ==========================\n",
    "depletion_threshold = 20\n",
    "df_gdf['Depleted'] = df_gdf['SOC_batt'] < depletion_threshold\n",
    "\n",
    "df_pre_depletion = df_gdf[~df_gdf['Depleted']].copy()\n",
    "df_post_depletion = df_gdf[df_gdf['Depleted']].copy()\n",
    "\n",
    "df_pre_depletion['Next_Grid_Cell'] = df_pre_depletion.groupby('deviceID')['Grid_Cell'].shift(-1)\n",
    "df_transitions = cudf.concat([df_pre_depletion, df_post_depletion]).dropna(subset=['Next_Grid_Cell'])\n",
    "\n",
    "# 🛠 Debugging Step: Check if df_transitions has data\n",
    "print(f\"Total transitions available for training: {len(df_transitions)}\")\n",
    "\n",
    "# ==========================\n",
    "# 🔹 4️⃣ Initialize Q-Table & Tracking Lists\n",
    "# ==========================\n",
    "q_table = {}\n",
    "q_table_convergence = []\n",
    "bellman_errors = []\n",
    "policy_consistency = []\n",
    "reward_per_episode = []\n",
    "training_accuracies = []\n",
    "validation_accuracies = []\n",
    "hyperparam_values = []\n",
    "accuracy_values = []\n",
    "\n",
    "# ==========================\n",
    "# 🔹 5️⃣ Helper Functions\n",
    "# ==========================\n",
    "def parse_grid_cell(grid_cell_str):\n",
    "    try:\n",
    "        x, y = map(int, grid_cell_str.split('_'))\n",
    "        return np.array([x, y])\n",
    "    except ValueError:\n",
    "        return np.array([0, 0])\n",
    "\n",
    "def track_q_table_convergence(prev_q_table, new_q_table):\n",
    "    \"\"\"Compute change in Q-table values between iterations.\"\"\"\n",
    "    delta_q = sum(abs(new_q_table.get(s, {}).get(a, 0) - prev_q_table.get(s, {}).get(a, 0))\n",
    "                  for s in new_q_table for a in new_q_table[s])\n",
    "    q_table_convergence.append(delta_q)\n",
    "\n",
    "def compute_policy_consistency(q_table, df_transitions):\n",
    "    \"\"\"Check how often the same best action is chosen for a state.\"\"\"\n",
    "    correct_choices = 0\n",
    "    total_choices = 0\n",
    "\n",
    "    for _, row in df_transitions.to_pandas().iterrows():\n",
    "        state, next_state = row['Grid_Cell'], row['Next_Grid_Cell']\n",
    "        if state in q_table and next_state in q_table[state]:\n",
    "            best_action = max(q_table[state], key=q_table[state].get)\n",
    "            if best_action == next_state:\n",
    "                correct_choices += 1\n",
    "            total_choices += 1\n",
    "\n",
    "    policy_consistency.append(correct_choices / total_choices if total_choices > 0 else 0)\n",
    "\n",
    "def compute_expected_reward(rewards_per_episode, gamma):\n",
    "    \"\"\"Compute expected cumulative reward using discounted sum formula.\"\"\"\n",
    "    total_reward = sum((gamma**t) * r for t, r in enumerate(rewards_per_episode))\n",
    "    reward_per_episode.append(total_reward)\n",
    "\n",
    "# ==========================\n",
    "# 🔹 Initialize Tracking Variables\n",
    "# ==========================\n",
    "hyperparam_values = []\n",
    "accuracy_values = []\n",
    "gamma_values = []\n",
    "\n",
    "# Store best Q-table across trials\n",
    "best_q_table = {}\n",
    "best_trial = {'accuracy': -np.inf, 'policy': [], 'rewards': [], 'q_table': {}}  # Store best trial\n",
    "\n",
    "# ==========================\n",
    "# 🔹 GPU-Accelerated Q-Learning (Final Stability-Optimized Version)\n",
    "# ==========================\n",
    "def train_rl(params):\n",
    "    global hyperparam_values, accuracy_values, gamma_values, best_q_table, best_trial   # Store hyperparameters and best Q-table\n",
    "\n",
    "    # Extract parameters\n",
    "    alpha_init = params['alpha']\n",
    "    gamma = params['gamma']\n",
    "    epsilon_init = 0.1  # Start with reduced randomness\n",
    "    epsilon_decay = params['epsilon_decay']\n",
    "    training_iterations = 40  # Increased training iterations\n",
    "\n",
    "    # Track hyperparameters\n",
    "    gamma_values.append(gamma)\n",
    "    hyperparam_values.append(params)\n",
    "\n",
    "    # Initialize Q-table (reuse best Q-table if available)\n",
    "    q_table = best_q_table.copy() if best_q_table else {}\n",
    "\n",
    "    # Initialize alpha and epsilon decay parameters\n",
    "    alpha = alpha_init\n",
    "    epsilon = epsilon_init\n",
    "    alpha_decay = 0.002  # Slower decay to allow stable learning\n",
    "    epsilon_min = 0.05\n",
    "    epsilon_lambda = 0.005  # Even smoother transition from exploration to exploitation\n",
    "\n",
    "    # Rolling average for policy consistency\n",
    "    policy_consistency_window = []\n",
    "    q_table_sizes = []  # Track Q-table growth\n",
    "    # Initialize prev_q_table before training\n",
    "    prev_q_table = {}  \n",
    "\n",
    "    # RL Training with Vectorized Operations\n",
    "    batch_size = 2000  # Large batch size stabilizes learning\n",
    "\n",
    "    for t in range(training_iterations):\n",
    "        for batch_start in range(0, len(df_transitions), batch_size):\n",
    "            batch = df_transitions.iloc[batch_start:batch_start + batch_size].to_pandas()  # Convert batch to Pandas\n",
    "\n",
    "            states = batch['Grid_Cell'].values\n",
    "            actions = batch['Next_Grid_Cell'].values\n",
    "\n",
    "            # Compute transition counts efficiently\n",
    "            unique_pairs, counts = np.unique(list(zip(states, actions)), axis=0, return_counts=True)\n",
    "\n",
    "            for (state, action), count in zip(unique_pairs, counts):\n",
    "                if state not in q_table:\n",
    "                    q_table[state] = {}\n",
    "\n",
    "                # Ensure each state has at least one valid action with a default Q-value\n",
    "                if action not in q_table[state]:\n",
    "                    q_table[state][action] = np.random.uniform(0.01, 0.1)  # Small random Q-value\n",
    "\n",
    "                # Convert Grid Cells to numerical coordinates\n",
    "                state_coords = parse_grid_cell(state)\n",
    "                action_coords = parse_grid_cell(action)\n",
    "\n",
    "                # Compute Euclidean distance penalty (More balanced)\n",
    "                distance_penalty = np.exp(-np.linalg.norm(state_coords - action_coords) / grid_size) * 0.2\n",
    "\n",
    "                # 1️⃣ **Final Adjusted Reward Function (More Stability)**\n",
    "                reward = np.log(count + 20) / 5 + 0.03 + distance_penalty - 0.008  # Smoother scaling\n",
    "\n",
    "                # ✅ **Regularized Q-learning update with stronger stability**\n",
    "                max_future_q = max(q_table[state].values(), default=0)\n",
    "                q_update = reward + gamma * max_future_q - q_table[state][action]\n",
    "                q_table[state][action] += alpha * q_update - 0.0015 * abs(q_table[state][action])  # Stronger Q-value regularization\n",
    "\n",
    "        # ✅ Preserve learned policies between training iterations\n",
    "        if t > 0:\n",
    "            prev_q_table = q_table.copy()\n",
    "\n",
    "        # ✅ Track Q-Table Convergence **AFTER each training iteration**\n",
    "        track_q_table_convergence(prev_q_table, q_table)\n",
    "        q_table_sizes.append(len(q_table))  # Log Q-table growth\n",
    "\n",
    "        # ✅ Compute Expected Reward Growth\n",
    "        compute_expected_reward([q_table[state][action] for state in q_table for action in q_table[state]], gamma)\n",
    "\n",
    "        # ✅ Adaptive Epsilon Decay (Slower and more stable)\n",
    "        epsilon = epsilon_min + (epsilon_init - epsilon_min) * np.exp(-epsilon_lambda * t)\n",
    "\n",
    "        # ✅ Adaptive Alpha Decay for Controlled Learning Rate\n",
    "        alpha = max(0.005, alpha_init / (1 + alpha_decay * t))  # Lower bound prevents sharp drops\n",
    "\n",
    "        # # ✅ **Final Stability Enhancements**\n",
    "        # # 🔹 Force Exploitation in Last 5% of Training Iterations\n",
    "        # if t > training_iterations * 0.95:\n",
    "        #     epsilon = 0.01  # Minimal exploration, forcing exploitation\n",
    "        \n",
    "        # # 🔹 Soft Lock on Policy Updates in the Last 3% of Iterations\n",
    "        # if t > training_iterations * 0.97:\n",
    "        #     alpha = 0.003  # Lock learning with very small updates\n",
    "\n",
    "        # # 🔹 **Final Q-Value Freezing Near Convergence**\n",
    "        # if t > training_iterations * 0.98:\n",
    "        #     for state in q_table:\n",
    "        #         for action in q_table[state]:\n",
    "        #             q_table[state][action] *= 0.999  # More gradual freezing\n",
    "\n",
    "    # ✅ Store best Q-table after training\n",
    "    best_q_table = q_table.copy()\n",
    "\n",
    "    # 🛠 Debugging Step: Check if Q-table was updated\n",
    "    print(f\"Total states in Q-table: {len(q_table)}\")\n",
    "\n",
    "    # ==========================\n",
    "    # 🔹 RL Prediction (Using Decayed Epsilon)\n",
    "    # ==========================\n",
    "    def predict_next_grid(state):\n",
    "        if state in q_table and q_table[state]:  # Ensure Q-values exist\n",
    "            if random.uniform(0, 1) < epsilon:\n",
    "                return random.choice(list(q_table[state].keys()))  # Explore\n",
    "            return max(q_table[state], key=q_table[state].get)  # Exploit\n",
    "        return random.choice(list(q_table.keys())) if q_table else None\n",
    "\n",
    "    df_post_depletion['Predicted_Grid_Cell_RL'] = df_post_depletion['Grid_Cell'].to_pandas().map(predict_next_grid)\n",
    "    df_post_depletion['Correct_Prediction_RL'] = df_post_depletion['Predicted_Grid_Cell_RL'] == df_post_depletion['Grid_Cell']\n",
    "    accuracy = df_post_depletion['Correct_Prediction_RL'].mean()\n",
    "    accuracy_values.append(accuracy)  # Store for later validation\n",
    "\n",
    "    if accuracy > best_trial['accuracy']:\n",
    "        best_trial['accuracy'] = accuracy\n",
    "        best_trial['policy'] = policy_consistency.copy()\n",
    "        best_trial['rewards'] = reward_per_episode.copy()\n",
    "        best_trial['q_table'] = q_table.copy()\n",
    "        best_q_table = q_table.copy()  # Store best Q-table permanently\n",
    "\n",
    "    # 🛠 Debugging Step: Check if predictions are being made\n",
    "    print(f\"Total predictions made: {df_post_depletion['Predicted_Grid_Cell_RL'].notna().sum()}\")\n",
    "    print(f\"Trial Accuracy: {accuracy:.2%} | Params: {params}\")\n",
    "\n",
    "    return {'loss': -accuracy, 'status': STATUS_OK}\n",
    "\n",
    "\n",
    "# ==========================\n",
    "# 🔹 Hyperparameter Optimization (Final Fine-Tuning)\n",
    "# ==========================\n",
    "param_space = {\n",
    "    'alpha': hp.uniform('alpha', 0.18, 0.23),  # Narrowed for smoother learning\n",
    "    'gamma': hp.uniform('gamma', 0.68, 0.75),  # Focused on long-term reward stability\n",
    "    'epsilon_decay': hp.uniform('epsilon_decay', 0.987, 0.993),  # Optimized for better convergence\n",
    "}\n",
    "\n",
    "trials = Trials()\n",
    "best_params = fmin(fn=train_rl, space=param_space, algo=tpe.suggest, max_evals=4, trials=trials)  # Reduced max_evals\n",
    "\n",
    "\n",
    "print(f\"Best RL Parameters: {best_params}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Set visualization style\n",
    "sns.set(style=\"whitegrid\")\n",
    "\n",
    "\n",
    "\n",
    "# ✅ 2️⃣ Policy Consistency Over Time\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.plot(policy_consistency, label=\"Policy Consistency\", color='green', linewidth=2)\n",
    "plt.xlabel(\"Training Iteration\")\n",
    "plt.ylabel(\"Consistency (%)\")\n",
    "plt.title(\"Policy Consistency Over Training\")\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "# ✅ 4️⃣ Expected Reward Growth (Checks if Learning is Improving)\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.plot(reward_per_episode, label=\"Expected Reward\", color='purple', marker=\"o\", linewidth=2)\n",
    "plt.xlabel(\"Training Iteration\")\n",
    "plt.ylabel(\"Total Discounted Reward\")\n",
    "plt.title(\"Cumulative Expected Reward Over Training\")\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "# ✅ 5️⃣ Accuracy Evolution Across Trials\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.plot(accuracy_values, label=\"Accuracy\", marker=\"o\", color='orange', linewidth=2)\n",
    "plt.xlabel(\"Trial Number\")\n",
    "plt.ylabel(\"Prediction Accuracy (%)\")\n",
    "plt.title(\"RL Model Accuracy Over Trials\")\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "# ✅ 6️⃣ Hyperparameter Sensitivity Analysis\n",
    "fig, axes = plt.subplots(1, 3, figsize=(18, 5))\n",
    "\n",
    "# 🎯 Alpha vs Accuracy\n",
    "axes[0].scatter([p[\"alpha\"] for p in hyperparam_values], accuracy_values, color='blue', alpha=0.6)\n",
    "axes[0].set_xlabel(\"Alpha (Learning Rate)\")\n",
    "axes[0].set_ylabel(\"Prediction Accuracy (%)\")\n",
    "axes[0].set_title(\"Effect of Alpha on Accuracy\")\n",
    "\n",
    "# 🎯 Gamma vs Accuracy\n",
    "axes[1].scatter(gamma_values, accuracy_values, color='red', alpha=0.6)\n",
    "axes[1].set_xlabel(\"Gamma (Discount Factor)\")\n",
    "axes[1].set_ylabel(\"Prediction Accuracy (%)\")\n",
    "axes[1].set_title(\"Effect of Gamma on Accuracy\")\n",
    "\n",
    "# 🎯 Epsilon vs Accuracy\n",
    "# axes[2].scatter([p[\"epsilon\"] for p in hyperparam_values], accuracy_values, color='green', alpha=0.6)\n",
    "# axes[2].set_xlabel(\"Epsilon (Exploration Rate)\")\n",
    "# axes[2].set_ylabel(\"Prediction Accuracy (%)\")\n",
    "# axes[2].set_title(\"Effect of Epsilon on Accuracy\")\n",
    "\n",
    "# plt.tight_layout()\n",
    "# plt.show()\n",
    "\n",
    "# Apply rolling average smoothing for reward per episode\n",
    "# reward_per_episode_smoothed = pd.Series(reward_per_episode).rolling(window=10, min_periods=1).mean()\n",
    "\n",
    "# # ✅ 4️⃣ Expected Reward Growth (Smoothed Version)\n",
    "# plt.figure(figsize=(10, 5))\n",
    "# plt.plot(reward_per_episode_smoothed, label=\"Smoothed Expected Reward\", color='purple', marker=\"o\", linewidth=2)\n",
    "# plt.xlabel(\"Training Iteration\")\n",
    "# plt.ylabel(\"Total Discounted Reward\")\n",
    "# plt.title(\"Smoothed Cumulative Expected Reward Over Training\")\n",
    "# plt.legend()\n",
    "# plt.show()\n",
    "\n",
    "\n",
    "# ==========================\n",
    "# 🔹 Plot Best Trial Results\n",
    "# ==========================\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.plot(best_trial['policy'], label=\"Best Policy Consistency\", color='green', linewidth=2)\n",
    "plt.xlabel(\"Training Iteration\")\n",
    "plt.ylabel(\"Consistency (%)\")\n",
    "plt.title(\"Policy Consistency Over Training (Best Trial)\")\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "\n",
    "plt.figure(figsize=(10, 5))\n",
    "colors =plt.cm.viridis(np.linspace(0, 1, len(rewards_per_eval)))  # Unique colors\n",
    "\n",
    "for i, (eval_id, rewards) in enumerate(rewards_per_eval.items()):\n",
    "    plt.plot(rewards, label=f\"Eval {eval_id + 1}\", color=colors[i], linewidth=2)\n",
    "\n",
    "plt.xlabel(\"Training Iteration\")\n",
    "plt.ylabel(\"Total Discounted Reward\")\n",
    "plt.title(\"Cumulative Expected Reward Over Training (All Evaluations)\")\n",
    "plt.legend()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Mathematical Formulation of Post-Depletion Trajectory Analysis**\n",
    "\n",
    "This analysis enables a detailed comparison of **actual vs predicted movement** following battery depletion, revealing:\n",
    "- How vehicles move after depletion.\n",
    "- The effectiveness of the **Markov-based prediction model**.\n",
    "- Patterns in vehicle trajectory shifts post-depletion.\n",
    "\n",
    "## **1. Extracting Unique Days of Battery Depletion**\n",
    "We define a unique day $ d $ as a calendar date where at least one vehicle experienced battery depletion:\n",
    "\n",
    "$$\n",
    "D = \\{ d \\mid \\exists i, SOC_{\\text{batt}, i} < 50\\%, \\text{on day } d \\}\n",
    "$$\n",
    "\n",
    "where $ D $ is the set of all days in which a depletion event occurred.\n",
    "\n",
    "## **2. Filtering Data for Each Depletion Day**\n",
    "For each $ d \\in D $, we extract:\n",
    "\n",
    "- **Post-depletion records**: \n",
    "  $$\n",
    "  X_d = \\{ x_i \\mid x_i \\in X, \\text{Timestamp}(x_i) = d, SOC_{\\text{batt}, i} < 50\\% \\}\n",
    "  $$\n",
    "\n",
    "- **Pre-depletion records**:\n",
    "  $$\n",
    "  Y_d = \\{ y_i \\mid y_i \\in Y, \\text{Timestamp}(y_i) = d, SOC_{\\text{batt}, i} \\geq 50\\% \\}\n",
    "  $$\n",
    "\n",
    "where:\n",
    "- $ X_d $ represents all vehicle records **after depletion**.\n",
    "- $ Y_d $ represents all vehicle records **before depletion**.\n",
    "\n",
    "## **3. Mapping Vehicles to Grid Cells**\n",
    "For each vehicle's recorded GPS point $ p_i $ on day $ d $:\n",
    "\n",
    "$$\n",
    "G_i = \\arg\\max_j \\mathbb{1}(p_i \\in P_j)\n",
    "$$\n",
    "\n",
    "where:\n",
    "- $ G_i $ is the assigned grid cell.\n",
    "- $ P_j $ represents the grid cells.\n",
    "- $ \\mathbb{1}(p_i \\in P_j) $ is an indicator function that is **1** if $ p_i $ is inside $ P_j $.\n",
    "\n",
    "Thus, we define:\n",
    "\n",
    "$$\n",
    "G_d^{\\text{actual}} = \\{ G_i \\mid x_i \\in X_d \\}\n",
    "$$\n",
    "\n",
    "$$\n",
    "G_d^{\\text{pre}} = \\{ G_i \\mid y_i \\in Y_d \\}\n",
    "$$\n",
    "\n",
    "## **4. Extracting Predicted Grid Cells**\n",
    "The predicted post-depletion grid cells for each vehicle are computed from the **Markov Transition Model**:\n",
    "\n",
    "$$\n",
    "G_{\\text{predicted}, i} = \\arg\\max_{G_j} P(G_j | G_i)\n",
    "$$\n",
    "\n",
    "for each vehicle location $ G_i $ before depletion.\n",
    "\n",
    "The predicted set is:\n",
    "\n",
    "$$\n",
    "G_d^{\\text{predicted}} = \\{ G_{\\text{predicted}, i} \\mid x_i \\in X_d \\}\n",
    "$$\n",
    "\n",
    "## **5. Visualizing the Spatial Trajectories**\n",
    "We generate a geospatial plot for each day $ d $:\n",
    "\n",
    "- **Pre-Depletion Trajectory** $ G_d^{\\text{pre}} $ (Black)\n",
    "- **Actual Post-Depletion Trajectory** $ G_d^{\\text{actual}} $ (Red)\n",
    "- **Predicted Trajectory** $ G_d^{\\text{predicted}} $ (Blue)\n",
    "\n",
    "Each grid cell is represented as a polygon, where:\n",
    "\n",
    "$$\n",
    "P_j = \\{ (x_k, y_k) \\mid k = 1,2,3,4 \\}\n",
    "$$\n",
    "\n",
    "and plotted based on its category:\n",
    "\n",
    "$$\n",
    "\\text{Color}(P_j) =\n",
    "\\begin{cases} \n",
    "\\text{black}, & P_j \\in G_d^{\\text{pre}} \\\\\n",
    "\\text{red}, & P_j \\in G_d^{\\text{actual}} \\\\\n",
    "\\text{blue}, & P_j \\in G_d^{\\text{predicted}}\n",
    "\\end{cases}\n",
    "$$\n",
    "\n",
    "## **6. Overlaying the OpenStreetMap Basemap**\n",
    "The plotted grid cells are projected onto a real-world **OpenStreetMap** (OSM) basemap with coordinate reference system:\n",
    "\n",
    "$$\n",
    "\\text{CRS} = \\text{EPSG:4326}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract unique days where depletion occurred\n",
    "depleted_days = df_post_depletion['Timestamp'].dt.date.unique()\n",
    "\n",
    "# Loop through each depleted day and generate a plot\n",
    "for day in depleted_days:\n",
    "    # Filter data for the current day\n",
    "    df_day = df_post_depletion[df_post_depletion['Timestamp'].dt.date == day]\n",
    "    df_pre_depletion_day = df_pre_depletion[df_pre_depletion['Timestamp'].dt.date == day]\n",
    "\n",
    "    # Convert actual, predicted, and pre-depletion data into GeoDataFrames\n",
    "    gdf_actual = grid_gdf.loc[grid_gdf.index.isin(df_day['Grid_Cell'])].copy()\n",
    "    gdf_actual['Color'] = 'red'\n",
    "\n",
    "    predicted_grid_cells = df_day['Predicted_Grid_Cell_Hybrid'].dropna().unique()\n",
    "    gdf_predicted = grid_gdf.loc[grid_gdf.index.isin(predicted_grid_cells)].copy()\n",
    "    gdf_predicted['Color'] = 'blue'\n",
    "\n",
    "    pre_depletion_grid_cells = df_pre_depletion_day['Grid_Cell'].unique()\n",
    "    gdf_pre_depletion = grid_gdf.loc[grid_gdf.index.isin(pre_depletion_grid_cells)].copy()\n",
    "    gdf_pre_depletion['Color'] = 'black'\n",
    "\n",
    "    # Skip plotting if all GeoDataFrames are empty for the day\n",
    "    if gdf_actual.empty and gdf_predicted.empty and gdf_pre_depletion.empty:\n",
    "        print(f\"Skipping {day}: No valid data for plotting.\")\n",
    "        continue\n",
    "\n",
    "    # Create the plot for the current day\n",
    "    fig, ax = plt.subplots(figsize=(12, 12))\n",
    "\n",
    "    # Plot Grid Cells\n",
    "    grid_gdf.plot(ax=ax, color='lightgrey', alpha=0.2)\n",
    "\n",
    "    # Plot Pre-Depletion Trajectory (Black) if not empty\n",
    "    if not gdf_pre_depletion.empty:\n",
    "        gdf_pre_depletion.plot(ax=ax, color='black', alpha=0.5, label=\"Pre-Depletion Trajectory\")\n",
    "\n",
    "    # Plot Actual Trajectory (Red) if not empty\n",
    "    if not gdf_actual.empty:\n",
    "        gdf_actual.plot(ax=ax, color='red', alpha=0.5, label=\"Actual Trajectory\")\n",
    "\n",
    "    # Plot Predicted Trajectory (Blue) if not empty\n",
    "    if not gdf_predicted.empty:\n",
    "        gdf_predicted.plot(ax=ax, color='blue', alpha=0.5, label=\"Predicted Trajectory\")\n",
    "\n",
    "    # Add Basemap\n",
    "    try:\n",
    "        ctx.add_basemap(ax, crs=grid_gdf.crs.to_string(), source=ctx.providers.CartoDB.Positron)\n",
    "    except Exception as e:\n",
    "        print(f\"Basemap Error on {day}: {e}\")\n",
    "\n",
    "    # Formatting\n",
    "    ax.set_title(f\"Actual vs Predicted Trajectory (Post-Depletion) - {day}\", fontsize=14)\n",
    "    ax.legend()\n",
    "    ax.set_axis_off()\n",
    "\n",
    "    # Show plot\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_temp = df.copy().reset_index(drop=True)  # Ensure indices are sequential\n",
    "df_temp=df_temp.sort_values(by=['Timestamp']).reset_index(drop=True) \n",
    "\n",
    "# Define different time thresholds to compare\n",
    "time_thresholds = {\n",
    "    # \"3 sec\": 3,\n",
    "    \"12 sec\": 12\n",
    "}\n",
    "\n",
    "# Create a dictionary to store SOC and sensor states for each threshold\n",
    "soc_depletion_results = {}\n",
    "\n",
    "# Iterate over different time thresholds\n",
    "for label, TIME_THRESHOLD in time_thresholds.items():\n",
    "\n",
    "    # Track last sensed timestamp, and stored energy during OFF periods\n",
    "    last_sensed_time = {}\n",
    "    stored_energy = {}\n",
    "\n",
    "    # Previous date\n",
    "    prev_date=None\n",
    "    \n",
    "    for i in range(len(df_temp)):\n",
    "        row = df_temp.iloc[i]\n",
    "        grid_key = (row['Lat_Grid'], row['Log_Grid'])  # Unique grid identifier\n",
    "        current_time = row['Timestamp']\n",
    "        current_date = row['Date']\n",
    "        device= row['deviceID']\n",
    "\n",
    "        # Initialise inter-row differences when OFF\n",
    "        d_diff_prev=0 \n",
    "        \n",
    "        # Reset stored energy at the start of a new day\n",
    "        if prev_date is not None and current_date != prev_date:\n",
    "            stored_energy={}  # Reset stored energy for all grid cells\n",
    "            df_temp.loc[i:, f'Energy_Saved_{label}'] = 0  # Reset energy savings for the new day\n",
    "            print(f\"[RESET] Reset stored energy for new day: {current_date}\")\n",
    "\n",
    "        prev_date = current_date  # Update previous date tracker\n",
    "\n",
    "        if df_temp.loc[i, 'SOC_batt']>99:\n",
    "            stored_energy[grid_key]=0\n",
    "            df_temp.loc[i:, f'Energy_Saved_{label}']=0\n",
    "\n",
    "        # If the grid was sensed recently (within the threshold), turn OFF the sensor\n",
    "        if grid_key in last_sensed_time and (current_time - last_sensed_time[grid_key]).total_seconds() < TIME_THRESHOLD:\n",
    "            df_temp.at[i, f'Sensor_ON_{label}'] = False   \n",
    "\n",
    "            # Accumulate stored energy\n",
    "            if i > 0 and pd.notna(df_temp.iloc[i - 1]['SOC_batt']) and pd.notna(row['SOC_batt']):\n",
    "            \n",
    "                # Find the last preceding row for this device\n",
    "                if device == df_temp.iloc[i-1]['deviceID']:\n",
    "                    d_diff = max(0, df_temp.iloc[i - 1]['SOC_batt'] - row['SOC_batt']) #current inter-row difference\n",
    "                    diff = max(d_diff_prev, d_diff)\n",
    "                    stored_energy[grid_key] = df_temp.loc[i-1, f'Energy_Saved_{label}']\n",
    "                    stored_energy[grid_key] += diff\n",
    "                    \n",
    "                    if diff != 0: \n",
    "                        df_temp.loc[i:, f'Energy_Saved_{label}'] = stored_energy[grid_key]\n",
    "                    else:\n",
    "                        df_temp.loc[i:, f'Energy_Saved_{label}'] = df_temp.loc[i-1, f'Energy_Saved_{label}']\n",
    "\n",
    "                    d_diff_prev=diff\n",
    "                    print(f\"[OFF]: Accumulated {diff:.2f}% for device {device}. Total stored: {stored_energy[grid_key]:.2f}%\")\n",
    "\n",
    "                else:\n",
    "                    d_diff = max(0, df_temp.loc[df_temp.deviceID == device, :]['SOC_batt'].iloc[-1] - row['SOC_batt']) #current inter-row difference\n",
    "                    diff = max(d_diff_prev, d_diff)\n",
    "                    stored_energy[grid_key] = df_temp.loc[df_temp.deviceID == device, :][f'Energy_Saved_{label}'].iloc[-1]\n",
    "                    stored_energy[grid_key] += diff\n",
    "                    \n",
    "                    if diff != 0: \n",
    "                        df_temp.loc[i:, f'Energy_Saved_{label}'] = stored_energy[grid_key]\n",
    "                    else:\n",
    "                        df_temp.loc[i:, f'Energy_Saved_{label}'] = df_temp.loc[df_temp.deviceID == device, :][f'Energy_Saved_{label}'].iloc[-1]\n",
    "\n",
    "                    d_diff_prev=diff\n",
    "                    print(f\"CHANGE [OFF]: Accumulated {diff:.2f}% for device {device}. Total stored: {stored_energy[grid_key]:.2f}%\")\n",
    "\n",
    "        else:\n",
    "            # Update last sensed time when the sensor turns ON\n",
    "            last_sensed_time[grid_key] = current_time\n",
    "            df_temp.at[i, f'Sensor_ON_{label}'] = True\n",
    "            d_diff_prev=0\n",
    "\n",
    "            if device == df_temp.iloc[i-1]['deviceID']:\n",
    "\n",
    "                # Ensure stored_energy is initialized per grid cell without overwriting previous values\n",
    "                if grid_key not in stored_energy:\n",
    "                    if i > 0:\n",
    "                        #Carry forward the stored energy from the last known row\n",
    "                        stored_energy[grid_key] = df_temp.loc[i-1, f'Energy_Saved_{label}']\n",
    "                        df_temp.loc[i:, f'Energy_Saved_{label}'] = stored_energy[grid_key]\n",
    "                    elif i == 0:\n",
    "                        stored_energy[grid_key] = 0  # First iteration, no prior energy \n",
    "                        df_temp.loc[i:, f'Energy_Saved_{label}'] = stored_energy[grid_key]  \n",
    "                print(f\"[ON]: device {device}. Total stored: {stored_energy[grid_key]:.2f}%\")\n",
    "\n",
    "            else:\n",
    "                 # Ensure stored_energy is initialized per grid cell without overwriting previous values\n",
    "                if grid_key not in stored_energy:\n",
    "                    if i > 0:\n",
    "                        #Carry forward the stored energy from the last known row\n",
    "                        stored_energy[grid_key] = df_temp.loc[df_temp.deviceID == device, :][f'Energy_Saved_{label}'].iloc[-1]\n",
    "                        df_temp.loc[i:, f'Energy_Saved_{label}'] = stored_energy[grid_key]\n",
    "                    elif i == 0:\n",
    "                        stored_energy[grid_key] = 0  # First iteration, no prior energy \n",
    "                        df_temp.loc[i:, f'Energy_Saved_{label}'] = stored_energy[grid_key]                 \n",
    "                \n",
    "                print(f\"CHANGE [ON]: device {device}. Total stored: {stored_energy[grid_key]:.2f}%\")\n",
    "\n",
    "    \n",
    "    # Compute new SOC_batt with savings\n",
    "    df_temp[f'SOC_batt_{label}'] = df_temp['SOC_batt'] + df_temp[f'Energy_Saved_{label}']\n",
    "    df_temp[f'SOC_batt_{label}'] = df_temp[f'SOC_batt_{label}'].clip(upper=100)\n",
    "\n",
    "    # Compute SOC depletion for this threshold\n",
    "    daily_soc = df_temp.groupby(['Date', 'deviceID'])[f'SOC_batt_{label}'].mean()\n",
    "    soc_depletion_results[label] = daily_soc\n",
    "\n",
    "\n",
    "# Baseline: Compute SOC depletion without constraints\n",
    "soc_depletion_results[\"Baseline\"] = df_temp.groupby(['Date', 'deviceID'])['SOC_batt'].mean()\n",
    "\n",
    "# Convert results to a DataFrame for plotting\n",
    "soc_depletion_df = pd.DataFrame(soc_depletion_results)\n",
    "\n",
    "# Save the updated dataset with sensor states and energy savings for each threshold\n",
    "output_path = \"/workspace/data/updated_SOC_batt_with_energy_savings.xlsx\"\n",
    "df_temp.to_excel(output_path, index=False)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define line styles and transparency levels for each threshold\n",
    "line_styles = {\n",
    "    \"Baseline\": \"--\",\n",
    "    # \"3 sec\": \"--\",\n",
    "    \"12 sec\": \"-\"\n",
    "}\n",
    "\n",
    "# Transparency levels for each threshold\n",
    "alpha_values = {\n",
    "    \"Baseline\": 0.5,  # 70% transparent\n",
    "    # \"3 sec\": 0.7,  # 30% transparent\n",
    "    \"12 sec\": 1   #Fully visible\n",
    "}\n",
    "\n",
    "# Predefined colors for devices\n",
    "predefined_colors = ['#007FFF', '#DC143C', '#FF4500','#39FF14', '#800080']\n",
    "device_ids = set()\n",
    "\n",
    "for soc_series in soc_depletion_results.values():\n",
    "    device_ids.update(soc_series.index.get_level_values('deviceID').unique())\n",
    "\n",
    "# Create a color map using predefined colors\n",
    "color_map = {device_id: predefined_colors[i % len(predefined_colors)] for i, device_id in enumerate(sorted(device_ids))}\n",
    "\n",
    "# Plot SOC depletion for different devices and thresholds\n",
    "plt.figure(figsize=(12, 6))\n",
    "\n",
    "# Iterate over thresholds and plot per device\n",
    "for label, soc_series in soc_depletion_results.items():  # soc_series is a MultiIndexed Series\n",
    "    for device_id in soc_series.index.get_level_values('deviceID').unique():  # Get unique devices\n",
    "        device_data = soc_series[soc_series.index.get_level_values('deviceID') == device_id]\n",
    "        plt.plot(\n",
    "            device_data.index.get_level_values('Date'),  # X-axis: Dates\n",
    "            device_data.values,  # Y-axis: SOC values\n",
    "            linestyle=line_styles[label],\n",
    "            color=color_map[device_id],  # Use predefined color for the device\n",
    "            # marker='o',\n",
    "            # markersize=3,\n",
    "            alpha=alpha_values[label],  # Apply transparency per threshold\n",
    "            label=f\"Device {device_id} - {label}\"\n",
    "        )\n",
    "\n",
    "plt.xlabel('Date')\n",
    "plt.ylabel('Mean SOC (%)')\n",
    "plt.title('SOC Depletion Comparison Across Devices and Time Constraints')\n",
    "\n",
    "# Place the legend outside the plot\n",
    "plt.legend(\n",
    "    bbox_to_anchor=(1.05, 1),  # Place legend to the right of the plot\n",
    "    loc='upper left',          # Align legend to the top-left of the bounding box\n",
    "    borderaxespad=0.           # Reduce spacing between the legend and the plot\n",
    ")\n",
    "plt.grid(True)\n",
    "plt.xticks(rotation=45)\n",
    "\n",
    "# Adjust layout to make room for the legend\n",
    "plt.tight_layout()\n",
    "\n",
    "# Show plot\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute the count of times the sensor was turned OFF for each constraint scenario\n",
    "off_counts = {}\n",
    "\n",
    "# Iterate over different time thresholds\n",
    "for label, TIME_THRESHOLD in time_thresholds.items():\n",
    "    df_copy = df.copy()  # Work on a copy of the dataset\n",
    "    df_copy['Sensor_ON'] = True  # Default: Sensor is ON\n",
    "\n",
    "    # Track last sensed timestamp per grid cell\n",
    "    last_sensed_time = {}\n",
    "    off_count = 0\n",
    "\n",
    "    for i, row in df_copy.iterrows():\n",
    "        grid_key = (row['Lat_Grid'], row['Log_Grid'])  # Unique grid identifier\n",
    "        current_time = row['Timestamp']\n",
    "\n",
    "        # If the grid was sensed recently (within the threshold), turn OFF the sensor\n",
    "        if grid_key in last_sensed_time and (current_time - last_sensed_time[grid_key]).total_seconds() < TIME_THRESHOLD:\n",
    "            df_copy.at[i, 'Sensor_ON'] = False\n",
    "            off_count += 1\n",
    "        else:\n",
    "            # Update last sensed time when the sensor turns ON\n",
    "            last_sensed_time[grid_key] = current_time\n",
    "\n",
    "    off_counts[label] = off_count\n",
    "\n",
    "# Convert to DataFrame for better visualization\n",
    "off_counts_df = pd.DataFrame.from_dict(off_counts, orient='index', columns=['Sensor OFF Count'])\n",
    "\n",
    "# Display results\n",
    "off_counts_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract unique days where depletion occurred\n",
    "depleted_days = df_post_depletion['Timestamp'].dt.date.unique()\n",
    "\n",
    "# Loop through each depleted day for visualization\n",
    "for day in depleted_days:\n",
    "    # Filter data for the current depleted day\n",
    "    df_day_pre = df_pre_coverage[df_pre_coverage['Date'] == day]\n",
    "    df_day_new = df_new_coverage_only[df_new_coverage_only['Date'] == day]\n",
    "    df_day_actual = df_post_depletion[df_post_depletion['Timestamp'].dt.date == day]\n",
    "    df_day_predicted = df_post_depletion[df_post_depletion['Timestamp'].dt.date == day]\n",
    "\n",
    "    # Convert to GeoDataFrames\n",
    "    gdf_pre = grid_gdf.loc[grid_gdf.index.isin(df_day_pre['Grid_Cell'])].copy()\n",
    "    gdf_pre['Color'] = 'black'  # Pre-depletion trajectory\n",
    "\n",
    "    gdf_new = grid_gdf.loc[grid_gdf.index.isin(df_day_new['Grid_Cell'])].copy()\n",
    "    gdf_new['Color'] = 'green'  # Newly sensed due to 10min rule\n",
    "\n",
    "    gdf_actual = grid_gdf.loc[grid_gdf.index.isin(df_day_actual['Grid_Cell'])].copy()\n",
    "    gdf_actual['Color'] = 'red'  # Actual trajectory after depletion\n",
    "\n",
    "    predicted_grid_cells = df_day_predicted['Predicted_Grid_Cell'].dropna().unique()\n",
    "    gdf_predicted = grid_gdf.loc[grid_gdf.index.isin(predicted_grid_cells)].copy()\n",
    "    gdf_predicted['Color'] = 'blue'  # Predicted trajectory\n",
    "\n",
    "    # Skip if no relevant data for the day\n",
    "    if gdf_pre.empty and gdf_new.empty and gdf_actual.empty and gdf_predicted.empty:\n",
    "        print(f\"Skipping {day}: No valid data for plotting.\")\n",
    "        continue\n",
    "\n",
    "    # Create the plot\n",
    "    fig, ax = plt.subplots(figsize=(12, 12))\n",
    "\n",
    "    # Plot Grid Cells (Background)\n",
    "    grid_gdf.plot(ax=ax, color='lightgrey', edgecolor='grey', alpha=0.2)\n",
    "\n",
    "    # Plot Pre-Depletion Trajectory (Black)\n",
    "    if not gdf_pre.empty:\n",
    "        gdf_pre.plot(ax=ax, color='black', alpha=0.5, edgecolor='black', label=\"Pre-Depletion Trajectory\")\n",
    "\n",
    "    # Plot Actual Post-Depletion Trajectory (Red)\n",
    "    if not gdf_actual.empty:\n",
    "        gdf_actual.plot(ax=ax, color='red', alpha=0.5, edgecolor='red', label=\"Actual Trajectory (Post-Depletion)\")\n",
    "\n",
    "    # Plot Predicted Post-Depletion Trajectory (Blue)\n",
    "    if not gdf_predicted.empty:\n",
    "        gdf_predicted.plot(ax=ax, color='blue', alpha=0.5, edgecolor='blue', label=\"Predicted Trajectory\")\n",
    "\n",
    "    # Plot Newly Sensed Cells Due to 10-Minute Rule (Green)\n",
    "    if not gdf_new.empty:\n",
    "        gdf_new.plot(ax=ax, color='green', alpha=0.5, edgecolor='green', label=\"Newly Sensed Cells (10-min Interval)\")\n",
    "\n",
    "    # Add Basemap\n",
    "    try:\n",
    "        ctx.add_basemap(ax, crs=grid_gdf.crs.to_string(), source=ctx.providers.OpenStreetMap.Mapnik)\n",
    "    except Exception as e:\n",
    "        print(f\"Basemap Error on {day}: {e}\")\n",
    "\n",
    "    # Formatting\n",
    "    ax.set_title(f\"Trajectory Visualization with 10-Minute Sensing Constraint - {day}\", fontsize=14)\n",
    "    ax.legend()\n",
    "    ax.set_axis_off()\n",
    "\n",
    "    # Show plot\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dynamically get the bounds from the data\n",
    "min_lat, max_lat = df['Lat'].min(), df['Lat'].max()\n",
    "min_lon, max_lon = df['Log'].min(), df['Log'].max()\n",
    "\n",
    "# Define grid size (120x120 meters)\n",
    "grid_size = 120\n",
    "lat_resolution = grid_size / 111320  # Convert meters to latitude degrees\n",
    "lon_resolution_at_lat = lambda lat: grid_size / (111320 * np.cos(np.radians(lat)))\n",
    "\n",
    "# Generate grid covering the dataset area\n",
    "grid = []\n",
    "lat = min_lat\n",
    "while lat < max_lat:\n",
    "    lon = min_lon\n",
    "    while lon < max_lon:\n",
    "        lon_res = lon_resolution_at_lat(lat)\n",
    "        grid.append(Polygon([\n",
    "            (lon, lat),\n",
    "            (lon + lon_res, lat),\n",
    "            (lon + lon_res, lat + lat_resolution),\n",
    "            (lon, lat + lat_resolution)\n",
    "        ]))\n",
    "        lon += lon_res\n",
    "    lat += lat_resolution\n",
    "\n",
    "# Create an empty GeoDataFrame for the grid\n",
    "grid_gdf = gpd.GeoDataFrame({'geometry': grid, 'Count': 0}, crs=\"EPSG:4326\")\n",
    "\n",
    "# Create a GeoDataFrame for the data points\n",
    "df_gdf = gpd.GeoDataFrame(df, geometry=gpd.points_from_xy(df['Log'], df['Lat']), crs=\"EPSG:4326\")\n",
    "\n",
    "# Assign each measurement to a grid square\n",
    "for index, point in df_gdf.iterrows():\n",
    "    match = grid_gdf.contains(point.geometry)\n",
    "    if match.any():\n",
    "        grid_gdf.loc[match.idxmax(), 'Count'] += 1\n",
    "\n",
    "# Apply Fractional Power Scaling\n",
    "gamma = 0.3  # Adjust for visibility\n",
    "grid_gdf['Scaled_Count'] = (grid_gdf['Count'] + 1) ** gamma\n",
    "\n",
    "# Normalize values for color mapping\n",
    "norm = Normalize(vmin=grid_gdf['Scaled_Count'].min(), vmax=grid_gdf['Scaled_Count'].max())\n",
    "cmap = plt.get_cmap('jet')\n",
    "\n",
    "# Convert scaled values to hex colors\n",
    "grid_gdf['Color'] = grid_gdf['Scaled_Count'].apply(lambda x: to_hex(cmap(norm(x))))\n",
    "\n",
    "# Create Folium map centered on Stockholm\n",
    "m = folium.Map(location=[df['Lat'].mean(), df['Log'].mean()], zoom_start=12, tiles='Cartodb dark_matter')\n",
    "\n",
    "# Function to color the grid based on scaled counts\n",
    "def style_function(feature):\n",
    "    color = feature['properties']['Color']  # Get precomputed color\n",
    "    return {\n",
    "        'fillColor': color,\n",
    "        'color': 'black',\n",
    "        'weight': 0.1,\n",
    "        'fillOpacity': 0.4\n",
    "    }\n",
    "\n",
    "# Add grid layer to Folium\n",
    "folium.GeoJson(\n",
    "    grid_gdf,\n",
    "    name=\"Measurement Grid\",\n",
    "    style_function=style_function,\n",
    "    tooltip=folium.GeoJsonTooltip(fields=['Count'], aliases=[\"Measurements:\"])\n",
    ").add_to(m)\n",
    "\n",
    "# Add layer control\n",
    "folium.LayerControl().add_to(m)\n",
    "\n",
    "# Display the map\n",
    "m\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

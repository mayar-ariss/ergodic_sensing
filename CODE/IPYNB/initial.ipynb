{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Open poweshell in new terminal and run\n",
    "\n",
    "docker build -t sensing-whale \"C:\\Users\\mayar\\OneDrive - Massachusetts Institute of Technology\\Desktop\\energy-aware\"\n",
    "\n",
    "After building the image, use -v to mount the local DATA directory inside /workspace/data/ in the container:\n",
    "\n",
    "docker run -it --gpus all --shm-size=1g --ulimit memlock=-1 --ulimit stack=67108864 -v \"C:\\Users\\mayar\\OneDrive - Massachusetts Institute of Technology\\Desktop\\energy-aware\\DATA:/workspace/data\" -p 8888:8888 sensing-whale"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "to detect docker containers run: docker ps\n",
    "\n",
    "to stop docker container: docker stop 'insert container name'\n",
    "\n",
    "to delete docker container: docker rm 'insert container name'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imported Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standard Library Imports\n",
    "import logging\n",
    "import multiprocessing as mp\n",
    "import os\n",
    "import random\n",
    "from collections import defaultdict\n",
    "from datetime import datetime\n",
    "\n",
    "# Check current working directory and list files\n",
    "print(os.getcwd())\n",
    "print(os.listdir())\n",
    "\n",
    "# Numerical & Data Processing \n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import cudf\n",
    "import cupy as cp\n",
    "import cuspatial\n",
    "import cuml  # RAPIDS cuML for accelerated machine learning\n",
    "import numba\n",
    "\n",
    "# Check GPU Status\n",
    "!nvidia-smi\n",
    "\n",
    "# Geospatial Processing \n",
    "import geopandas as gpd\n",
    "from shapely.geometry import Polygon, Point\n",
    "import shapely\n",
    "\n",
    "# Visualization \n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from matplotlib.colors import Normalize, to_hex\n",
    "import folium\n",
    "import branca.colormap as cm\n",
    "import matplotlib.colors as mcolors\n",
    "import optuna.visualization\n",
    "import contextily as ctx\n",
    "\n",
    "# Statistical & Curve Fitting \n",
    "import scipy.stats as stats\n",
    "from scipy.optimize import curve_fit\n",
    "from scipy.stats import kstest\n",
    "\n",
    "# Machine Learning & Optimization \n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error\n",
    "from hyperopt import fmin, tpe, hp, STATUS_OK, Trials\n",
    "import optuna\n",
    "import xgboost as xgb\n",
    "\n",
    "# Utility \n",
    "from kneed import KneeLocator\n",
    "from tqdm import tqdm  # Import tqdm for the progress bar\n",
    "\n",
    "print(\"RAPIDS & required libraries loaded successfully!\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Cleaning and Preprocessing\n",
    "\n",
    "Loads and process multi-sheet Excel data\n",
    "\n",
    "1. **File Loading**: Reads all sheets from `2022_vitals.xlsx` without headers.\n",
    "2. **Column Naming**: Assigns predefined column names for consistency.\n",
    "3. **Data Alignment**: \n",
    "   - Fixes misaligned rows by detecting valid `deviceID`.\n",
    "   - Ensures all rows have the correct number of columns.\n",
    "4. **Filtering**:\n",
    "   - Removes invalid or duplicate header rows.\n",
    "   - Drops rows with zero values for latitude (`Lat`) and longitude (`Log`).\n",
    "5. **Indexing**: Resets the index and assigns a sequential 1-based index.\n",
    "6. **Output**: Saves the cleaned data to `2022_vitals_cleaned.xlsx` and previews it."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Mount the local data directory to Docker;\n",
    "docker run -it --gpus all --shm-size=1g --ulimit memlock=-1 --ulimit stack=67108864 -p 8888:8888 `\n",
    "    -v \"C:\\Users\\mayar\\OneDrive - Massachusetts Institute of Technology\\Desktop\\energy-aware\\DATA:/workspace/data\" `\n",
    "    rapids-custom-container\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adjusted file path for Docker (mounted volume)\n",
    "file_path = \"/workspace/data/2022_vitals.xlsx\"\n",
    "output_path = \"/workspace/data/2022_vitals_cleaned.xlsx\"\n",
    "\n",
    "# Specify the column names explicitly\n",
    "column_names = [\n",
    "    \"deviceID\", \"Timestamp\", \"Lat\", \"Log\", \"SOC_batt\", \"temp_batt\", \"volatge_batt\",\n",
    "    \"voltage_particle\", \"current_batt\", \"isCharging\", \"isCharginS\", \"isCharged\",\n",
    "    \"Temp_int\", \"Hum_int\", \"solar_current\", \"Cellular_signal_strength\", \"index\"\n",
    "]\n",
    "\n",
    "# Load all sheets into a dictionary\n",
    "sheets_dict = pd.read_excel(file_path, sheet_name=None, header=None)  # No header initially\n",
    "\n",
    "# Process each sheet\n",
    "processed_sheets = []\n",
    "for sheet_name, sheet_data in sheets_dict.items():\n",
    "    # Ensure the number of columns matches the expected number\n",
    "    sheet_data = sheet_data.iloc[:, :len(column_names)]\n",
    "\n",
    "    # Fix misaligned rows where the first column is invalid\n",
    "    def fix_alignment(row):\n",
    "        # Convert the row to a list\n",
    "        row_list = row.tolist()\n",
    "\n",
    "        # Find the first valid `deviceID` (assumes valid `deviceID` has > 5 characters)\n",
    "        for i, value in enumerate(row_list):\n",
    "            if isinstance(value, str) and len(value) > 5:  # Valid `deviceID` found\n",
    "                # Align the row starting from the valid `deviceID`\n",
    "                aligned_row = row_list[i:i + len(column_names)]\n",
    "                # Ensure the row is padded or trimmed to match `column_names`\n",
    "                return aligned_row + [None] * (len(column_names) - len(aligned_row))\n",
    "\n",
    "        # If no valid `deviceID` found, return row of NaN\n",
    "        return [None] * len(column_names)\n",
    "\n",
    "    # Apply alignment fix to all rows\n",
    "    sheet_data = sheet_data.apply(fix_alignment, axis=1, result_type=\"expand\")\n",
    "    \n",
    "    # Assign column names\n",
    "    sheet_data.columns = column_names\n",
    "\n",
    "    # Drop rows where 'deviceID' is still invalid or starts with \"deviceID\"\n",
    "    sheet_data = sheet_data[sheet_data['deviceID'].notna()]\n",
    "    sheet_data = sheet_data[sheet_data['deviceID'] != \"deviceID\"]  # Remove rows starting with \"deviceID\"\n",
    "\n",
    "    # Append processed sheet\n",
    "    processed_sheets.append(sheet_data)\n",
    "\n",
    "# Concatenate all sheets into one DataFrame\n",
    "df = pd.concat(processed_sheets, ignore_index=True)\n",
    "\n",
    "# Drop rows where Lat or Log is 0\n",
    "df = df[(df['Lat'] != 0) & (df['Log'] != 0)]\n",
    "\n",
    "# Replace SOC_batt values below 0 with 0\n",
    "df.loc[df['SOC_batt'] < 0, 'SOC_batt'] = 0\n",
    "\n",
    "# Correct indexing column to start at 1 and increment sequentially\n",
    "df.reset_index(drop=True, inplace=True)  # Reset pandas index\n",
    "df['index'] = df.index + 1  # Create a 1-based index\n",
    "\n",
    "# Save cleaned data back to Excel\n",
    "df.to_excel(output_path, index=False)\n",
    "\n",
    "# Print cleaned data preview\n",
    "print(\"Data cleaning completed. Saved to:\", output_path)\n",
    "print(df.head())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Spatiotemporal Binning and Stationary Period Detection**\n",
    "\n",
    "This enables **spatial binning**, **stationary period detection**, and **temporal filtering** for robust movement analysis.\n",
    "\n",
    "## **1. Timestamp Conversion**\n",
    "The Unix timestamp $ T_i $ is converted into a standard datetime format:\n",
    "\n",
    "$$\n",
    "T_i^{\\text{datetime}} = T_i^{\\text{unix}} \\times \\frac{1}{86400} + \\text{epoch}\n",
    "$$\n",
    "\n",
    "where:\n",
    "- $ T_i^{\\text{unix}} $ is the raw Unix timestamp in **seconds**,\n",
    "- $ 86400 $ seconds = **1 day**,\n",
    "- **Epoch** is the reference starting time (January 1, 1970).\n",
    "\n",
    "## **2. Ensuring Numeric Latitude and Longitude**\n",
    "We enforce that latitude ($ \\text{Lat} $) and longitude ($ \\text{Log} $) are real-valued:\n",
    "\n",
    "$$\n",
    "\\text{Lat}, \\text{Log} \\in \\mathbb{R}\n",
    "$$\n",
    "\n",
    "Non-numeric values are coerced to **NaN**.\n",
    "\n",
    "## **3. Discretization into a 40m Grid**\n",
    "### **3.1 Latitude Grid Resolution**\n",
    "Since the **Earth's meridional circumference** is approximately **40,030 km**, the degree-to-meter conversion near the equator is:\n",
    "\n",
    "$$\n",
    "1^\\circ \\approx 111,320 \\text{ meters}\n",
    "$$\n",
    "\n",
    "Thus, the spatial resolution of a **40m grid** in latitude is:\n",
    "\n",
    "$$\n",
    "\\Delta \\text{Lat} = \\frac{120}{111320}\n",
    "$$\n",
    "\n",
    "The **grid-aligned latitude** is computed as:\n",
    "\n",
    "$$\n",
    "\\text{Lat\\_Grid} = \\left\\lfloor \\frac{\\text{Lat}}{\\Delta \\text{Lat}} \\right\\rfloor \\times \\Delta \\text{Lat}\n",
    "$$\n",
    "\n",
    "### **3.2 Longitude Grid Resolution**\n",
    "Unlike latitude, **longitude spacing** varies with latitude due to Earthâ€™s curvature. The **longitude degree-to-meter conversion** is:\n",
    "\n",
    "$$\n",
    "1^\\circ \\approx 111320 \\times \\cos(\\text{Lat})\n",
    "$$\n",
    "\n",
    "Thus, the **longitude resolution** at a given latitude is:\n",
    "\n",
    "$$\n",
    "\\Delta \\text{Log} = \\frac{40}{111320 \\cos(\\text{Lat})}\n",
    "$$\n",
    "\n",
    "The **grid-aligned longitude** is computed as:\n",
    "\n",
    "$$\n",
    "\\text{Log\\_Grid} = \\left\\lfloor \\frac{\\text{Log}}{\\Delta \\text{Log}} \\right\\rfloor \\times \\Delta \\text{Log}\n",
    "$$\n",
    "\n",
    "## **4. Sorting by Time and Device**\n",
    "To track movement **chronologically** for each vehicle, we sort:\n",
    "\n",
    "$$\n",
    "\\text{Sort}(df, \\text{by}=[\\text{deviceID}, \\text{Timestamp}])\n",
    "$$\n",
    "\n",
    "## **5. Identifying Stationary Periods**\n",
    "For each vehicle, we determine if it remained in the same grid cell over consecutive timestamps:\n",
    "\n",
    "$$\n",
    "\\text{Same\\_Grid}_i =\n",
    "\\begin{cases} \n",
    "1, & (\\text{Lat\\_Grid}_i = \\text{Lat\\_Grid}_{i-1}) \\land (\\text{Log\\_Grid}_i = \\text{Log\\_Grid}_{i-1}) \\\\\n",
    "0, & \\text{otherwise}\n",
    "\\end{cases}\n",
    "$$\n",
    "\n",
    "where:\n",
    "- $ \\text{Same\\_Grid}_i = 1 $ means no movement occurred.\n",
    "- $ \\text{Same\\_Grid}_i = 0 $ means movement occurred.\n",
    "\n",
    "## **6. Computing Time Spent in a Grid Cell**\n",
    "The time difference between consecutive records within the same grid is:\n",
    "\n",
    "$$\n",
    "\\Delta t_i = T_i - T_{i-1}\n",
    "$$\n",
    "\n",
    "The **total duration** a vehicle spends within a specific grid cell before moving is:\n",
    "\n",
    "$$\n",
    "\\text{Cumulative\\_Time}_{i} = \\sum_{k=1}^{i} \\Delta t_k\n",
    "$$\n",
    "\n",
    "where:\n",
    "- The summation continues **until movement occurs**.\n",
    "\n",
    "## **7. Assigning a Group ID to Each Stationary Period**\n",
    "A **unique group identifier** is assigned to each stationary period using a cumulative sum:\n",
    "\n",
    "$$\n",
    "\\text{Group}_i =\n",
    "\\sum_{j=1}^{i} (1 - \\text{Same\\_Grid}_j)\n",
    "$$\n",
    "\n",
    "Each transition into a **new grid cell** increments the group ID.\n",
    "\n",
    "## **8. Removing Prolonged Stationary Vehicles**\n",
    "Vehicles remaining in the **same grid for over x hours** (xxx seconds) are excluded:\n",
    "\n",
    "$$\n",
    "\\text{Remove } i \\text{ if } \\text{Cumulative\\_Time}_i \\geq xxx \\text{ sec}\n",
    "$$\n",
    "\n",
    "## **9. Cleanup**\n",
    "All intermediate columns used for calculations are dropped to optimize storage.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert Unix timestamp to datetime\n",
    "df['Timestamp'] = pd.to_datetime(df['Timestamp'], unit='s')\n",
    "\n",
    "# Ensure 'Lat' and 'Log' are numeric\n",
    "df['Lat'] = pd.to_numeric(df['Lat'], errors='coerce')\n",
    "df['Log'] = pd.to_numeric(df['Log'], errors='coerce')\n",
    "\n",
    "# Spatial Resolution: 40m grid\n",
    "grid_size=40\n",
    "lat_resolution = grid_size / 111320 \n",
    "df['Lat_Grid'] = (df['Lat'] // lat_resolution) * lat_resolution\n",
    "\n",
    "# Longitude resolution depends on latitude\n",
    "df['Lon_Resolution'] = grid_size / (111320 * np.cos(np.radians(df['Lat'])))\n",
    "df['Log_Grid'] = (df['Log'] // df['Lon_Resolution']) * df['Lon_Resolution']\n",
    "\n",
    "# Drop auxiliary column\n",
    "df = df.drop(columns=['Lon_Resolution'])\n",
    "\n",
    "# Step 1: Sort by deviceID and Timestamp\n",
    "df = df.sort_values(by=['deviceID', 'Timestamp'])\n",
    "\n",
    "# Step 2: Detect continuous stationary periods\n",
    "df['Prev_Lat_Grid'] = df.groupby('deviceID')['Lat_Grid'].shift(1)\n",
    "df['Prev_Log_Grid'] = df.groupby('deviceID')['Log_Grid'].shift(1)\n",
    "df['Prev_Timestamp'] = df.groupby('deviceID')['Timestamp'].shift(1)\n",
    "\n",
    "# Step 3: Identify whether the taxi has stayed in the same grid\n",
    "df['Same_Grid'] = (df['Lat_Grid'] == df['Prev_Lat_Grid']) & (df['Log_Grid'] == df['Prev_Log_Grid'])\n",
    "\n",
    "# Step 4: Compute time spent in the grid continuously\n",
    "df['Time_Diff'] = (df['Timestamp'] - df['Prev_Timestamp']).dt.total_seconds()\n",
    "\n",
    "# Step 5: Assign a group ID that resets when the taxi leaves a grid\n",
    "df['Group'] = (~df['Same_Grid']).cumsum()\n",
    "\n",
    "# Step 6: Compute total time spent in each visit to the grid\n",
    "df['Cumulative_Time'] = df.groupby(['deviceID', 'Lat_Grid', 'Log_Grid', 'Group'])['Time_Diff'].cumsum()\n",
    "\n",
    "# Ensure 'Cumulative_Time' is not NaN or negative (if any filtering was done previously)\n",
    "df_filtered = df[df['Cumulative_Time'] > 0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Identifying Optimal Threshold for Cumulative Time in Grids**\n",
    "\n",
    "## **1. Calculate Empirical Cumulative Distribution Function (ECDF)**\n",
    "The ECDF is calculated to provide insight into the distribution of cumulative time spent in each grid. The sorted cumulative times and corresponding ECDF values are computed as:\n",
    "\n",
    "$\n",
    "\\text{sorted\\_times} = \\text{np.sort}(\\text{cumulative\\_time\\_hours})\n",
    "$\n",
    "\n",
    "$\n",
    "\\text{ecdf\\_values} = \\frac{i}{N}, \\quad i = 1, 2, \\ldots, N\n",
    "$\n",
    "\n",
    "Where $N$ is the total number of data points.\n",
    "\n",
    "## **2. Optimal Threshold Detection Using Kneedle Algorithm**\n",
    "The **Kneedle Algorithm** is applied to find the threshold point where the slope of the ECDF curve changes most significantly. This point is considered the optimal threshold that separates short-term and long-term grid occupations.\n",
    "\n",
    "$\n",
    "\\text{kneedle} = \\text{KneeLocator}(\\text{sorted\\_times}, \\text{ecdf\\_values}, S=1.0, \\text{curve}=\"concave\", \\text{direction}=\"increasing\")\n",
    "$\n",
    "\n",
    "The detected knee point (optimal threshold) is converted back to seconds:\n",
    "\n",
    "$\n",
    "\\text{optimal\\_threshold\\_seconds} = \\text{optimal\\_threshold\\_hours} \\times 3600\n",
    "$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert Cumulative_Time to hours for easier interpretation\n",
    "cumulative_time_hours = df_filtered['Cumulative_Time'] / 3600\n",
    "\n",
    "# Calculate ECDF values for analysis\n",
    "sorted_times = np.sort(cumulative_time_hours)\n",
    "ecdf_values = np.arange(1, len(sorted_times) + 1) / len(sorted_times)\n",
    "\n",
    "# Find the optimal threshold using the Kneedle algorithm\n",
    "kneedle = KneeLocator(sorted_times, ecdf_values, S=1.0, curve=\"concave\", direction=\"increasing\")\n",
    "optimal_threshold_hours = kneedle.knee\n",
    "optimal_threshold_seconds = optimal_threshold_hours * 3600\n",
    "\n",
    "# Plot 1: Histogram with Optimal Threshold\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.hist(cumulative_time_hours, bins=100, color='skyblue', label='Cumulative Time Distribution')\n",
    "plt.axvline(optimal_threshold_hours, color='red', linestyle='--', label=f'Optimal Threshold = {optimal_threshold_hours:.2f} hours')\n",
    "plt.title('Distribution of Cumulative Time Spent in Each Grid (With Optimal Threshold)')\n",
    "plt.xlabel('Cumulative Time Spent in Grid (hours)')\n",
    "plt.ylabel('Frequency')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()\n",
    "\n",
    "# Plot 2: ECDF with Optimal Threshold\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.ecdfplot(cumulative_time_hours, label='ECDF of Cumulative Time')\n",
    "plt.axvline(optimal_threshold_hours, color='red', linestyle='--', label=f'Optimal Threshold = {optimal_threshold_hours:.2f} hours')\n",
    "plt.title('ECDF of Cumulative Time Spent in Each Grid (With Optimal Threshold)')\n",
    "plt.xlabel('Cumulative Time Spent in Grid (hours)')\n",
    "plt.ylabel('Proportion of Data Points')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()\n",
    "\n",
    "# Display the optimal threshold\n",
    "print(f\"Optimal Threshold Found: {optimal_threshold_hours:.2f} hours ({optimal_threshold_seconds:.0f} seconds)\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "original_entry_count = len(df)\n",
    "\n",
    "# Step 7: Remove vehicles that stayed continuously in the same grid for more than optimal_threshold_seconds\n",
    "df = df[~(df['Cumulative_Time'] >= optimal_threshold_seconds)]\n",
    "\n",
    "removed_entries = original_entry_count - len(df)\n",
    "\n",
    "# Drop helper columns\n",
    "df = df.drop(columns=['Prev_Lat_Grid', 'Prev_Log_Grid', 'Prev_Timestamp', 'Same_Grid', 'Time_Diff', 'Group', 'Cumulative_Time'])\n",
    "\n",
    "# Print the number of removed entries\n",
    "print(f\"Number of entries removed due to vehicles staying continuously in the same grid for more than {optimal_threshold_hours} hours: {removed_entries}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Spatial Grid Aggregation\n",
    "\n",
    "This script generates a **40m x 40m geospatial grid** and counts the number of data points within each grid cell:\n",
    "\n",
    "1. **Dynamic Boundary Definition**: \n",
    "   - Extracts min/max latitude and longitude from the dataset.\n",
    "2. **Grid Construction**:\n",
    "   - Defines **40m resolution** for latitude and dynamically calculates longitude resolution.\n",
    "   - Iterates over the spatial extent to generate **polygonal grid cells**.\n",
    "3. **GeoDataFrame Creation**:\n",
    "   - Converts the grid into a `GeoDataFrame` (`grid_gdf`).\n",
    "   - Converts data points into a `GeoDataFrame` (`df_gdf`).\n",
    "4. **Spatial Aggregation**:\n",
    "   - Checks which points fall within each grid cell.\n",
    "   - Increments the count of data points within corresponding grid polygons."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dynamically define the bounds from the DataFrame\n",
    "min_lat = df['Lat'].min()\n",
    "max_lat = df['Lat'].max()\n",
    "min_lon = df['Log'].min()\n",
    "max_lon = df['Log'].max()\n",
    "\n",
    "lon_resolution_at_lat = lambda lat: grid_size / (111320 * np.cos(np.radians(lat)))\n",
    "\n",
    "# Generate grid of polygons\n",
    "grid = []\n",
    "lat = min_lat\n",
    "while lat < max_lat:\n",
    "    lon = min_lon\n",
    "    while lon < max_lon:\n",
    "        lon_res = lon_resolution_at_lat(lat)\n",
    "        grid.append(Polygon([\n",
    "            (lon, lat),\n",
    "            (lon + lon_res, lat),\n",
    "            (lon + lon_res, lat + lat_resolution),\n",
    "            (lon, lat + lat_resolution)\n",
    "        ]))\n",
    "        lon += lon_res\n",
    "    lat += lat_resolution\n",
    "\n",
    "# Create an empty GeoDataFrame for the grid\n",
    "grid_gdf = gpd.GeoDataFrame({'geometry': grid, 'Count': 0}, crs=\"EPSG:4326\")\n",
    "\n",
    "# Create a GeoDataFrame for the points in df\n",
    "df_gdf = gpd.GeoDataFrame(df, geometry=gpd.points_from_xy(df['Log'], df['Lat']), crs=\"EPSG:4326\")\n",
    "\n",
    "# Perform spatial join (vectorized operation)\n",
    "joined = gpd.sjoin(df_gdf, grid_gdf, how=\"left\", predicate=\"within\")\n",
    "\n",
    "# Count points per grid cell\n",
    "grid_gdf['Count'] = joined.groupby(joined.index_right).size()\n",
    "\n",
    "# Fill NaN with 0 for empty grid cells\n",
    "grid_gdf['Count'].fillna(0, inplace=True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Battery Depletion\n",
    "\n",
    "Identify the **number of unique days** where at least one deviceâ€™s battery **(SOC_batt)** dropped below **50%**:\n",
    "\n",
    "1. **Extract Date Information**:\n",
    "   - Converts the timestamp to **date-only format**.\n",
    "\n",
    "2. **Filter for Battery Depletion Events**:\n",
    "   - Selects records where `SOC_batt` is below **50%** (can be changed).\n",
    "\n",
    "3. **Count Unique Affected Days**:\n",
    "   - Computes the number of distinct days where a depletion event occurred."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assuming df is already loaded with the necessary data\n",
    "# Identify unique days where at least one device's SOC_batt dropped below 50%\n",
    "df['Date'] = df['Timestamp'].dt.date  # Extract the date\n",
    "days_with_depletion = df[df['SOC_batt'] < 50]['Date'].nunique()\n",
    "\n",
    "# Display the number of days with a battery drop below 50%\n",
    "days_with_depletion\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **XGBoost-Based SOC Forecasting with Optuna Hyperparameter Optimization**\n",
    "\n",
    "This implementation builds a **data-driven model** for **predicting battery State of Charge (SOC) depletion** in sensor-equipped vehicles. The model:\n",
    "1. **Extracts spatiotemporal and power-related features** from historical sensor data.\n",
    "2. **Trains an XGBoost regressor** to predict future SOC values.\n",
    "3. **Optimizes model hyperparameters** using Bayesian search via **Optuna**.\n",
    "4. **Performs multi-step forecasting**, predicting SOC for the next **seven time steps**.\n",
    "5. **Quantifies predictive uncertainty** and **adjusts dynamic SOC thresholds** for safety monitoring.\n",
    "\n",
    "## **1. Data Preprocessing and Feature Engineering**\n",
    "\n",
    "The dataset contains time-series measurements for multiple vehicles, each identified by a **deviceID**. The dataset is first sorted **chronologically** for each device:\n",
    "\n",
    "$$\n",
    "\\text{Sort}(df, \\text{by}=[\\text{deviceID}, \\text{Timestamp}])\n",
    "$$\n",
    "\n",
    "SOC and related features are converted to **numeric types** to ensure proper mathematical operations:\n",
    "\n",
    "$$\n",
    "X = \\{\\text{SOC\\_batt}, \\text{current\\_batt}, \\text{solar\\_current}, \\text{voltage\\_batt}, \\text{charging\\_status} \\}\n",
    "$$\n",
    "\n",
    "### **Feature Engineering**\n",
    "New predictive features are created, capturing both **short-term trends** and **time-based influences**:\n",
    "\n",
    "- **Hourly time representation:** $ \\text{Hour} = \\text{Timestamp.hour} $, capturing **daily charge-discharge patterns**.\n",
    "- **Lagged values:** Previous SOC and power readings are used as predictors:\n",
    "\n",
    "$$\n",
    "\\text{Prev\\_SOC}_t = \\text{SOC\\_batt}_{t-1}, \\quad \\text{Prev\\_Current}_t = \\text{current\\_batt}_{t-1}\n",
    "$$\n",
    "\n",
    "- **Rolling depletion rate:** Defined as the moving average of SOC depletion over a 5-step window:\n",
    "\n",
    "$$\n",
    "\\text{Depletion\\_Rate}_t = \\frac{1}{5} \\sum_{i=t-4}^{t} (\\text{SOC\\_batt}_i - \\text{SOC\\_batt}_{i-1})\n",
    "$$\n",
    "\n",
    "where $ \\text{Depletion\\_Rate}_t $ estimates **battery discharge trends**.\n",
    "\n",
    "The final **feature matrix** is:\n",
    "\n",
    "$$\n",
    "X = \\{ \\text{Hour}, \\text{Prev\\_SOC}, \\text{Prev\\_Current}, \\text{Prev\\_Solar\\_Current}, \\text{Prev\\_Voltage}, \\text{Prev\\_Charging}, \\text{Depletion\\_Rate} \\}\n",
    "$$\n",
    "\n",
    "and the target variable is:\n",
    "\n",
    "$$\n",
    "y = \\text{SOC\\_batt}\n",
    "$$\n",
    "\n",
    "## **2. Training the XGBoost Model with Bayesian Optimization**\n",
    "XGBoost, a **gradient-boosted tree regressor**, is trained to minimize **SOC prediction error**. Hyperparameter tuning is performed using **Optuna**, a Bayesian optimization framework.\n",
    "\n",
    "### **Optimization Objective**\n",
    "The model is optimized to **minimize the Mean Absolute Error (MAE)**:\n",
    "\n",
    "$$\n",
    "\\mathcal{L} = \\frac{1}{N} \\sum_{i=1}^{N} | y_i - \\hat{y}_i |\n",
    "$$\n",
    "\n",
    "where:\n",
    "- $ y_i $ is the **actual SOC value**,\n",
    "- $ \\hat{y}_i $ is the **predicted SOC**,\n",
    "- $ N $ is the number of test samples.\n",
    "\n",
    "### **Search Space for Hyperparameter Tuning**\n",
    "The following hyperparameters are optimized:\n",
    "- **Number of boosting rounds**: $ n_{\\text{estimators}} \\in [100, 500] $\n",
    "- **Learning rate**: $ \\eta \\in [0.06, 0.12] $ (log-uniform)\n",
    "- **Tree depth**: $ d \\in [4,6] $\n",
    "- **Subsampling ratio**: $ \\text{subsample} \\in [0.5,1.0] $\n",
    "- **Column sampling ratio**: $ \\text{colsample\\_bytree} \\in [0.5,1.0] $\n",
    "- **Regularization parameters**: $ \\lambda, \\alpha \\in [0.001, 10] $\n",
    "\n",
    "The Bayesian search selects hyperparameters that minimize **validation MAE**.\n",
    "\n",
    "## **3. Multi-Step Forecasting**\n",
    "To predict future SOC depletion, the trained model is used iteratively for **seven future time steps**.\n",
    "\n",
    "For each step $ t $, the next SOC is predicted as:\n",
    "\n",
    "$$\n",
    "\\hat{y}_t = f(X_t, \\theta^*)\n",
    "$$\n",
    "\n",
    "where:\n",
    "- $ f $ is the trained **XGBoost model**,\n",
    "- $ X_t $ contains the latest **SOC, charging state, and depletion rate**,\n",
    "- $ \\theta^* $ represents the **optimal hyperparameters**.\n",
    "\n",
    "Each prediction is fed back into the model, allowing **rolling forecasts**:\n",
    "\n",
    "$$\n",
    "X_{t+1} \\gets X_t, \\quad X_{t+1}[\\text{Prev\\_SOC}] = \\hat{y}_t\n",
    "$$\n",
    "\n",
    "ensuring **dynamic simulation of SOC depletion**.\n",
    "\n",
    "## **4. Uncertainty Estimation and Dynamic Thresholding**\n",
    "To ensure **safe operation**, a **dynamic SOC threshold** is computed for each future step, adjusting based on:\n",
    "- **Historical depletion rates**\n",
    "- **Charging behavior**\n",
    "- **Battery degradation uncertainty**\n",
    "\n",
    "A **Bayesian prior** is set on SOC:\n",
    "\n",
    "$$\n",
    "\\mu_{\\text{prior}} = \\mathbb{E}[y_{\\text{test}}], \\quad \\sigma_{\\text{prior}} = \\text{std}(y_{\\text{test}})\n",
    "$$\n",
    "\n",
    "Thresholds are adjusted based on:\n",
    "1. **Failure probability**:\n",
    "\n",
    "$$\n",
    "P_{\\text{failure}} = \\Phi\\left(\\frac{10 - \\mu_{\\text{prior}}}{\\sigma_{\\text{prior}}} \\right)\n",
    "$$\n",
    "\n",
    "where $ \\Phi $ is the **cumulative normal distribution**, modeling the probability of SOC dropping below 10%.\n",
    "\n",
    "2. **Dynamic SOC Threshold Computation**:\n",
    "\n",
    "$$\n",
    "\\text{Safe\\_SOC}_t = 30 + (10 \\cdot P_{\\text{failure}}) + (5 \\cdot \\sigma_t)\n",
    "$$\n",
    "\n",
    "where:\n",
    "- The **base threshold is 30% SOC**.\n",
    "- **Additional margins** are added based on **failure probability** and **rolling standard deviation**.\n",
    "\n",
    "Charging and solar effects further refine the threshold:\n",
    "\n",
    "$$\n",
    "\\text{Safe\\_SOC}_t = \\text{Safe\\_SOC}_t + 5 \\cdot \\mathbb{1}(\\text{Charging}) - 3 \\cdot \\mathbb{1}(\\text{Solar\\_High})\n",
    "$$\n",
    "\n",
    "ensuring **dynamic safety monitoring**.\n",
    "\n",
    "## **5. Evaluation Metrics**\n",
    "Final model performance is evaluated using:\n",
    "- **Mean Absolute Error (MAE)**:\n",
    "\n",
    "$$\n",
    "\\text{MAE} = \\frac{1}{N} \\sum_{i=1}^{N} | y_i - \\hat{y}_i |\n",
    "$$\n",
    "\n",
    "- **Root Mean Squared Error (RMSE)**:\n",
    "\n",
    "$$\n",
    "\\text{RMSE} = \\sqrt{\\frac{1}{N} \\sum_{i=1}^{N} (y_i - \\hat{y}_i)^2}\n",
    "$$\n",
    "\n",
    "- **Symmetric Mean Absolute Percentage Error (SMAPE)**:\n",
    "\n",
    "$$\n",
    "\\text{SMAPE} = 100 \\cdot \\frac{1}{N} \\sum_{i=1}^{N} \\frac{| y_i - \\hat{y}_i |}{( | y_i | + | \\hat{y}_i | ) / 2}\n",
    "$$\n",
    "\n",
    "Residual analysis confirms the **distribution of errors**, and rolling error plots show **stability over time**.\n",
    "\n",
    "## **6. Feature Importance Analysis Using XGBoost**\n",
    "\n",
    "Understanding **feature importance** is crucial in machine learning models to **interpret predictive behavior** and **assess model reliability**. In this study, we employ **XGBoostâ€™s feature importance analysis** to quantify the impact of each input variable on **SOC (State of Charge) prediction**.\n",
    "\n",
    "XGBoost provides an **intrinsic feature ranking mechanism**, which assigns **importance scores** to each predictor based on how frequently and effectively it contributes to **minimizing prediction error**.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load and preprocess data\n",
    "df_threshold = df.copy()\n",
    "\n",
    "# Sort and prepare dataset\n",
    "df_threshold = df_threshold.sort_values(by=['deviceID', 'Timestamp'])\n",
    "df_threshold['Timestamp'] = pd.to_datetime(df_threshold['Timestamp'])\n",
    "df_threshold.set_index('Timestamp', inplace=True)\n",
    "\n",
    "# Select a single device\n",
    "device_id = df_threshold['deviceID'].unique()[0]\n",
    "df_device = df_threshold[df_threshold['deviceID'] == device_id].copy()\n",
    "\n",
    "# Convert SOC and related features to numeric\n",
    "for col in ['SOC_batt', 'current_batt', 'solar_current', 'isCharginS', 'volatge_batt', 'isCharging']:\n",
    "    df_device[col] = pd.to_numeric(df_device[col], errors='coerce')\n",
    "\n",
    "df_device.dropna(inplace=True)\n",
    "\n",
    "# Feature Engineering\n",
    "df_device['Hour'] = df_device.index.hour\n",
    "df_device['Prev_SOC'] = df_device['SOC_batt'].shift(1)\n",
    "df_device['Prev_Current'] = df_device['current_batt'].shift(1)\n",
    "df_device['Prev_Solar_Current'] = df_device['solar_current'].shift(1)\n",
    "df_device['Prev_Solar'] = df_device['isCharginS'].shift(1)\n",
    "df_device['Prev_Voltage'] = df_device['volatge_batt'].shift(1)\n",
    "df_device['Prev_Charging'] = df_device['isCharging'].shift(1)\n",
    "\n",
    "# Compute rolling depletion rate\n",
    "df_device['Depletion_Rate'] = df_device['SOC_batt'].diff().rolling(window=5).mean()\n",
    "df_device['Depletion_Rate'].fillna(0, inplace=True)\n",
    "\n",
    "df_device.dropna(inplace=True)\n",
    "\n",
    "# Define input features and target\n",
    "features = ['Hour', 'Prev_SOC', 'Prev_Current', 'Prev_Solar_Current', 'Prev_Solar', 'Prev_Voltage', 'Prev_Charging', 'Depletion_Rate']\n",
    "X = df_device[features]\n",
    "y = df_device['SOC_batt']\n",
    "\n",
    "# Train-test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, shuffle=False)\n",
    "\n",
    "# Hyperparameter Tuning with Optuna\n",
    "def objective(trial):\n",
    "    params = {\n",
    "        'objective': 'reg:squarederror',\n",
    "        'n_estimators': trial.suggest_int('n_estimators', 100, 500),\n",
    "        'learning_rate': trial.suggest_float('learning_rate', 0.07, 0.12, log=True),  # Narrow range\n",
    "        'max_depth': trial.suggest_int('max_depth', 4, 6),  # Prevent deep overfitting\n",
    "        'subsample': trial.suggest_float('subsample', 0.5, 1.0),\n",
    "        'colsample_bytree': trial.suggest_float('colsample_bytree', 0.5, 1.0),\n",
    "        'gamma': trial.suggest_float('gamma', 0.01, 1.0, log=True),\n",
    "        'reg_alpha': trial.suggest_float('reg_alpha', 0.001, 10, log=True),\n",
    "        'reg_lambda': trial.suggest_float('reg_lambda', 0.001, 10, log=True)\n",
    "    }\n",
    "    model = xgb.XGBRegressor(**params)\n",
    "    model.fit(X_train, y_train)\n",
    "    y_pred = model.predict(X_test)\n",
    "    return mean_absolute_error(y_test, y_pred)\n",
    "\n",
    "# Run Optuna optimization\n",
    "# Define total trials\n",
    "N_TRIALS = 400\n",
    "STARTUP_TRIALS = 10\n",
    "\n",
    "# Suppress excessive Optuna logging\n",
    "optuna.logging.set_verbosity(optuna.logging.WARNING)  # Show only important messages\n",
    "\n",
    "# Initialize tqdm progress bar\n",
    "pbar = tqdm(total=N_TRIALS, desc=\"Optimization Progress\", position=0, leave=False, dynamic_ncols=True)\n",
    "\n",
    "# Callback function to update progress smoothly\n",
    "def progress_callback(study, trial):\n",
    "    pbar.update(1)  # Increment progress bar by 1\n",
    "    if study.best_value is not None:\n",
    "        pbar.set_postfix({\"Best MAE\": f\"{study.best_value:.4f}\"})  # Update in progress bar instead of printing\n",
    "\n",
    "# Run Optuna optimization\n",
    "study = optuna.create_study(\n",
    "    sampler=optuna.samplers.TPESampler(n_startup_trials=STARTUP_TRIALS),\n",
    "    direction='minimize'\n",
    ")\n",
    "study.optimize(objective, n_trials=N_TRIALS, callbacks=[progress_callback])\n",
    "\n",
    "# Close tqdm progress bar after completion\n",
    "pbar.close()\n",
    "\n",
    "# Train final model with best parameters\n",
    "best_params = study.best_params\n",
    "xgb_model = xgb.XGBRegressor(**best_params)\n",
    "xgb_model.fit(X_train, y_train)\n",
    "\n",
    "# Predict SOC\n",
    "y_pred = xgb_model.predict(X_test)\n",
    "\n",
    "# Multi-Step Forecasting (Predict Next 7 Steps)\n",
    "future_steps = 7\n",
    "X_future = X_test.copy()  # Copy of the last known test input\n",
    "predictions = np.zeros((len(X_test), future_steps))  # Shape: (test_size, future_steps)\n",
    "dynamic_thresholds = np.zeros((len(X_test), future_steps))  # Shape: (test_size, future_steps)\n",
    "\n",
    "# Initialize Bayesian prior for SOC distribution\n",
    "prior_mean = np.mean(y_test)  # Initial mean SOC\n",
    "prior_std = np.std(y_test)  # Initial standard deviation\n",
    "\n",
    "# Precompute failure probability for efficiency\n",
    "failure_prob_cache = stats.norm.cdf(10, loc=prior_mean, scale=prior_std)\n",
    "\n",
    "for step in range(future_steps):\n",
    "    y_future = xgb_model.predict(X_future)  # Predict future SOC\n",
    "    predictions[:, step] = y_future  # Store predictions\n",
    "\n",
    "    # Vectorized Depletion Rate Calculation\n",
    "    if step >= 5:\n",
    "        depletion_factor = np.mean(np.diff(predictions[:, max(0, step-5):step]), axis=1)\n",
    "    else:\n",
    "        depletion_factor = np.mean(np.diff(predictions[:, :step+1]), axis=1) if step > 0 else np.zeros(len(y_future))\n",
    "\n",
    "    # Rolling Standard Deviation Instead of Monte Carlo Simulations\n",
    "    prediction_uncertainty = np.std(predictions[:, max(0, step-5):step], axis=1)\n",
    "\n",
    "    # Dynamic Threshold Adjustment (Vectorized)\n",
    "    safe_soc = 30 + (10 * failure_prob_cache) + (5 * prediction_uncertainty)\n",
    "\n",
    "    # Charging & Solar Adjustments\n",
    "    safe_soc += (X_future['Prev_Charging'].values * 5)  # Add 5% if charging\n",
    "    safe_soc -= (X_future['Prev_Solar_Current'].values > 0) * 3  # Reduce by 3% if solar is strong\n",
    "\n",
    "    # Apply Bounds\n",
    "    safe_soc = np.clip(safe_soc, 10, 100)  # Keep in range\n",
    "    dynamic_thresholds[:, step] = safe_soc  # Store computed thresholds\n",
    "\n",
    "    # Update Prior for Next Step\n",
    "    prior_mean = np.mean(y_future)\n",
    "    prior_std = np.std(y_future)\n",
    "\n",
    "    # Update X_future for next step\n",
    "    X_future = X_future.copy()\n",
    "    X_future['Prev_SOC'] = y_future  # Use predicted SOC as input for next step\n",
    "\n",
    "\n",
    "# Compute Evaluation Metrics\n",
    "mae = mean_absolute_error(y_test, y_pred)\n",
    "rmse = np.sqrt(mean_squared_error(y_test, y_pred))\n",
    "mape = np.mean(np.abs((y_test - y_pred) / y_test)) * 100\n",
    "\n",
    "# Function to Compute SMAPE\n",
    "def smape(y_true, y_pred):\n",
    "    return 100 * np.mean(np.abs(y_pred - y_true) / ((np.abs(y_true) + np.abs(y_pred)) / 2))\n",
    "\n",
    "smape_score = smape(y_test, y_pred)\n",
    "\n",
    "print(f\"Validation Metrics:\")\n",
    "print(f\"Mean Absolute Error (MAE): {mae:.2f}%\")\n",
    "print(f\"Root Mean Square Error (RMSE): {rmse:.2f}%\")\n",
    "print(f\"Mean Absolute Percentage Error (MAPE): {mape:.2f}%\")\n",
    "print(f\"Symmetric Mean Absolute Percentage Error (SMAPE): {smape_score:.2f}%\")\n",
    "\n",
    "# Residual Analysis\n",
    "residuals = y_test - y_pred\n",
    "plt.figure(figsize=(12, 6))\n",
    "sns.histplot(residuals, bins=50, kde=True, color=\"purple\")\n",
    "plt.axvline(residuals.mean(), color='red', linestyle='dashed', label=f\"Mean Residual: {residuals.mean():.2f}\")\n",
    "plt.title(\"Residual Distribution (y_test - y_pred)\")\n",
    "plt.xlabel(\"Residual\")\n",
    "plt.ylabel(\"Frequency\")\n",
    "plt.legend()\n",
    "plt.grid()\n",
    "plt.show()\n",
    "\n",
    "# Rolling Error Analysis (50-step window)\n",
    "rolling_window = 50\n",
    "rolling_mae = np.convolve(np.abs(residuals), np.ones(rolling_window)/rolling_window, mode='valid')\n",
    "rolling_rmse = np.convolve(np.square(residuals), np.ones(rolling_window)/rolling_window, mode='valid')\n",
    "rolling_rmse = np.sqrt(rolling_rmse)\n",
    "\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.plot(y_test.index[rolling_window-1:], rolling_mae, label=\"Rolling MAE\", color='blue')\n",
    "plt.plot(y_test.index[rolling_window-1:], rolling_rmse, label=\"Rolling RMSE\", color='orange')\n",
    "plt.title(\"Rolling Error Analysis (MAE & RMSE Over Time)\")\n",
    "plt.xlabel(\"Time\")\n",
    "plt.ylabel(\"Error\")\n",
    "plt.legend()\n",
    "plt.grid()\n",
    "plt.show()\n",
    "\n",
    "# Extract trials and corresponding objective values\n",
    "trials = [t.number for t in study.trials if t.state == optuna.trial.TrialState.COMPLETE]\n",
    "values = [t.value for t in study.trials if t.state == optuna.trial.TrialState.COMPLETE]\n",
    "\n",
    "# Best value tracking\n",
    "best_values = [min(values[:i+1]) for i in range(len(values))]\n",
    "\n",
    "# Compute percentage improvement\n",
    "initial_value = values[0]  # First trial\n",
    "final_value = min(values)  # Best trial\n",
    "\n",
    "# Convergence Plot\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.plot(trials, values, marker='o', linestyle='-', color='b', label=\"MAE per trial\")\n",
    "plt.plot(trials, best_values, marker='o', linestyle='-', color='g', label=\"Best MAE found\")\n",
    "\n",
    "# Add a percentage bar\n",
    "plt.xlabel(\"Trial Number\")\n",
    "plt.ylabel(\"Objective Value (MAE)\")\n",
    "plt.title(\"Optuna Optimization History (Convergence Plot)\")\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()\n",
    "\n",
    "fig1 = optuna.visualization.plot_optimization_history(study)\n",
    "fig1.show()\n",
    "\n",
    "fig2 = optuna.visualization.plot_param_importances(study)\n",
    "fig2.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ## Plot Actual SOC (Train & Test) & Predictions (Only for Test)\n",
    "\n",
    "# ax[0].plot(y_train.index, y_train, label=\"Actual SOC (Train)\", color='blue', linestyle=\"dashed\", alpha=0.7)  # Show only actual values for train\n",
    "# ax[0].plot(y_test.index, y_test, label=\"Actual SOC (Test)\", color='black', linestyle=\"dashed\")\n",
    "# ax[0].plot(y_test.index, y_pred, label=\"Predicted SOC (Test)\", color='red')\n",
    "\n",
    "# ax[0].set_title(f\"Optimized XGBoost SOC Prediction vs Actual SOC (Train & Test)\")\n",
    "# ax[0].set_ylabel(\"SOC Battery Level (%)\")\n",
    "# ax[0].legend(loc=\"upper left\", bbox_to_anchor=(1, 1))  # Legend outside\n",
    "# ax[0].grid(True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Identify Gaps Greater Than 1 Day in Training & Test Data\n",
    "gap_threshold = pd.Timedelta(days=1)\n",
    "\n",
    "# Find gaps in training data\n",
    "time_gaps_train = y_train.index.to_series().diff() > gap_threshold\n",
    "train_segment_indices = np.where(time_gaps_train)[0]\n",
    "\n",
    "# Find gaps in test data\n",
    "time_gaps_test = y_test.index.to_series().diff() > gap_threshold\n",
    "test_segment_indices = np.where(time_gaps_test)[0]\n",
    "\n",
    "# Copy actual values to introduce NaNs where gaps exist (preserving gaps only for actual SOC)\n",
    "y_train_gapfree = y_train.copy()\n",
    "y_test_gapfree = y_test.copy()\n",
    "\n",
    "y_train_gapfree.iloc[train_segment_indices] = np.nan\n",
    "y_test_gapfree.iloc[test_segment_indices] = np.nan\n",
    "\n",
    "# Clip predictions to a maximum of 100%\n",
    "y_pred_clipped = np.clip(y_pred, 0, 100)  # Ensures predictions stay between 0% and 100%\n",
    "predictions_clipped = np.clip(predictions, 0, 100)  # Multi-step forecast predictions clipped\n",
    "\n",
    "# Create subplots\n",
    "fig, ax = plt.subplots(2, 1, figsize=(12, 10), sharex=False)\n",
    "\n",
    "# Plot Actual SOC (Train & Test) with Gaps & Predictions (No Gaps, Clipped at 100%)\n",
    "ax[0].plot(y_train.index, y_train_gapfree, label=\"Actual SOC (Train)\", color='blue', alpha=0.7, linewidth=1)  # Gaps in train\n",
    "ax[0].plot(y_test.index, y_test_gapfree, label=\"Actual SOC (Test)\", color='blue', linestyle=\"dashed\", alpha=0.7, linewidth=1)  # Gaps in test\n",
    "ax[0].plot(y_test.index, y_pred_clipped, label=\"Predicted SOC (Test)\", color='red', linestyle=\"dashed\", alpha=0.7, linewidth=1)  # Predictions remain continuous but clipped\n",
    "\n",
    "ax[0].set_title(\"Optimized XGBoost SOC Prediction vs Actual SOC (Train & Test)\")\n",
    "ax[0].set_ylabel(\"SOC Battery Level (%)\")\n",
    "ax[0].legend(loc=\"upper left\", bbox_to_anchor=(1, 1))  # Legend outside\n",
    "ax[0].grid(True)\n",
    "\n",
    "# Multi-Step Forecasting & Safe SOC Threshold (Clipped at 100%)\n",
    "for i in range(future_steps):\n",
    "    ax[1].plot(y_test.index[:len(predictions_clipped[:, i])], predictions_clipped[:, i], linestyle=\"dotted\", linewidth=1, alpha=0.7, label=f\"Step {i+1} Forecast\")\n",
    "\n",
    "ax[1].plot(y_test.index[:len(dynamic_thresholds[:, i])], dynamic_thresholds[:, -1], linestyle=\"solid\", color='green', alpha=0.8, label=\"Safe SOC Threshold (Final Step)\")\n",
    "\n",
    "ax[1].set_title(\"Multi-Step SOC Forecast with Dynamic Safe Thresholds\")\n",
    "ax[1].set_xlabel(\"Time\")\n",
    "ax[1].set_ylabel(\"SOC Battery Level (%)\")\n",
    "ax[1].legend(loc=\"upper left\", bbox_to_anchor=(1, 1))  # Legend outside\n",
    "ax[1].grid(True)\n",
    "\n",
    "# Adjust layout\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# **Step 1: Retrain Model on Full Dataset**\n",
    "X_full = df_device[features]\n",
    "y_full = df_device['SOC_batt']\n",
    "\n",
    "xgb_model_full = xgb.XGBRegressor(**best_params)\n",
    "xgb_model_full.fit(X_full, y_full)  # Train on full dataset\n",
    "\n",
    "# **Step 2: Predict SOC for All Data**\n",
    "y_pred_full = xgb_model_full.predict(X_full)\n",
    "\n",
    "# **Step 3: Compute Dynamic Safe SOC Threshold for All Data**\n",
    "prior_mean = np.mean(y_full)\n",
    "prior_std = np.std(y_full)\n",
    "\n",
    "failure_prob_cache = stats.norm.cdf(10, loc=prior_mean, scale=prior_std)\n",
    "\n",
    "safe_soc_thresholds = np.zeros(len(y_full))\n",
    "\n",
    "for i in range(len(y_pred_full)):\n",
    "    depletion_factor = np.mean(np.diff(y_pred_full[max(0, i-5):i])) if i >= 5 else 0\n",
    "    prediction_uncertainty = np.std(y_pred_full[max(0, i-5):i]) if i >= 5 else 0\n",
    "\n",
    "    safe_soc = 30 + (10 * failure_prob_cache) + (5 * prediction_uncertainty)\n",
    "    safe_soc += (X_full['Prev_Charging'].iloc[i] * 5)\n",
    "    safe_soc -= (X_full['Prev_Solar_Current'].iloc[i] > 0) * 3\n",
    "    safe_soc = np.clip(safe_soc, 15, 45)\n",
    "    \n",
    "    safe_soc_thresholds[i] = safe_soc\n",
    "\n",
    "# **Step 4: Identify Gaps Greater Than 1 Day**\n",
    "gap_threshold = pd.Timedelta(days=1)\n",
    "time_gaps = df_device.index.to_series().diff() > gap_threshold  # Find where gaps are > 3 days\n",
    "segment_indices = np.where(time_gaps)[0]  # Indices where gaps exist\n",
    "\n",
    "# **Step 5: Plot Results with Unlinked Segments**\n",
    "plt.figure(figsize=(12, 6))\n",
    "\n",
    "# Track first segment to add legend only once\n",
    "first_segment = True\n",
    "\n",
    "# Loop through segments and plot separately\n",
    "start_idx = 0\n",
    "for idx in segment_indices:\n",
    "    if first_segment:\n",
    "        plt.plot(df_device.index[start_idx:idx], y_full[start_idx:idx], label=\"Actual SOC\", color='black', linestyle=\"dashed\")\n",
    "        plt.plot(df_device.index[start_idx:idx], y_pred_full[start_idx:idx], label=\"Predicted SOC\", color='red')\n",
    "        plt.plot(df_device.index[start_idx:idx], safe_soc_thresholds[start_idx:idx], linestyle=\"dashdot\", color='blue', label=\"Safe SOC Threshold\")\n",
    "        first_segment = False  # After first plot, disable legend labels\n",
    "    else:\n",
    "        plt.plot(df_device.index[start_idx:idx], y_full[start_idx:idx], color='black', linestyle=\"dashed\")\n",
    "        plt.plot(df_device.index[start_idx:idx], y_pred_full[start_idx:idx], color='red')\n",
    "        plt.plot(df_device.index[start_idx:idx], safe_soc_thresholds[start_idx:idx], linestyle=\"dashdot\", color='blue')\n",
    "\n",
    "    start_idx = idx  # Move start index to the next segment\n",
    "\n",
    "# Plot the last segment\n",
    "plt.plot(df_device.index[start_idx:], y_full[start_idx:], color='black', linestyle=\"dashed\")\n",
    "plt.plot(df_device.index[start_idx:], y_pred_full[start_idx:], color='red')\n",
    "plt.plot(df_device.index[start_idx:], safe_soc_thresholds[start_idx:], linestyle=\"dashdot\", color='blue')\n",
    "\n",
    "plt.xlabel(\"Time\")\n",
    "plt.ylabel(\"SOC Battery Level (%)\")\n",
    "plt.title(\"Deployed Model: SOC Prediction & Safe Threshold Over Time (Unlinked Gaps > 1 Day)\")\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Load and preprocess data for all devices\n",
    "df_threshold = df.copy()\n",
    "\n",
    "# Sort and prepare dataset\n",
    "df_threshold = df_threshold.sort_values(by=['deviceID', 'Timestamp'])\n",
    "df_threshold.set_index('Timestamp', inplace=True)\n",
    "\n",
    "# Step 2: Initialize List for Safe SOC Thresholds**\n",
    "all_safe_soc_thresholds = []\n",
    "\n",
    "# Step 3: Loop over each device\n",
    "for device_id in df_threshold['deviceID'].unique():\n",
    "    print(f\"\\nðŸ”¹ Processing Device ID: {device_id}\")\n",
    "\n",
    "    # **Extract data for the current device**\n",
    "    df_device = df_threshold[df_threshold['deviceID'] == device_id].copy()\n",
    "\n",
    "    # Convert SOC and related features to numeric\n",
    "    for col in ['SOC_batt', 'current_batt', 'solar_current', 'isCharginS', 'volatge_batt', 'isCharging']:\n",
    "        df_device[col] = pd.to_numeric(df_device[col], errors='coerce')\n",
    "\n",
    "    df_device.dropna(inplace=True)\n",
    "\n",
    "    # **Feature Engineering**\n",
    "    df_device['Hour'] = df_device.index.hour\n",
    "    df_device['Prev_SOC'] = df_device['SOC_batt'].shift(1)\n",
    "    df_device['Prev_Current'] = df_device['current_batt'].shift(1)\n",
    "    df_device['Prev_Solar_Current'] = df_device['solar_current'].shift(1)\n",
    "    df_device['Prev_Solar'] = df_device['isCharginS'].shift(1)\n",
    "    df_device['Prev_Voltage'] = df_device['volatge_batt'].shift(1)\n",
    "    df_device['Prev_Charging'] = df_device['isCharging'].shift(1)\n",
    "\n",
    "    # Compute rolling depletion rate\n",
    "    df_device['Depletion_Rate'] = df_device['SOC_batt'].diff().rolling(window=5).mean()\n",
    "    df_device['Depletion_Rate'].fillna(0, inplace=True)\n",
    "\n",
    "    df_device.dropna(inplace=True)\n",
    "\n",
    "    # Define input features and target\n",
    "    features = ['Hour', 'Prev_SOC', 'Prev_Current', 'Prev_Solar_Current', 'Prev_Solar', 'Prev_Voltage', 'Prev_Charging', 'Depletion_Rate']\n",
    "    X_full = df_device[features]\n",
    "    y_full = df_device['SOC_batt']\n",
    "\n",
    "    # Step 4: Train XGBoost model for this device\n",
    "    xgb_model_full = xgb.XGBRegressor(**best_params)\n",
    "    xgb_model_full.fit(X_full, y_full)\n",
    "\n",
    "    # Step 5: Predict SOC for all timestamps in this device\n",
    "    y_pred_full = xgb_model_full.predict(X_full)\n",
    "\n",
    "    # Step 6: Compute Dynamic Safe SOC Threshold\n",
    "    prior_mean = np.mean(y_full)\n",
    "    prior_std = np.std(y_full)\n",
    "    failure_prob_cache = stats.norm.cdf(10, loc=prior_mean, scale=prior_std)\n",
    "\n",
    "    safe_soc_thresholds = np.zeros(len(y_full))\n",
    "\n",
    "    for i in range(len(y_pred_full)):\n",
    "        depletion_factor = np.mean(np.diff(y_pred_full[max(0, i-5):i])) if i >= 5 else 0\n",
    "        prediction_uncertainty = np.std(y_pred_full[max(0, i-5):i]) if i >= 5 else 0\n",
    "\n",
    "        safe_soc = 50 + (10 * failure_prob_cache) + (5 * prediction_uncertainty)\n",
    "        safe_soc += (X_full['Prev_Charging'].iloc[i] * 5)\n",
    "        safe_soc -= (X_full['Prev_Solar_Current'].iloc[i] > 0) * 3\n",
    "        safe_soc = np.clip(safe_soc, 10, 70)\n",
    "\n",
    "        safe_soc_thresholds[i] = safe_soc\n",
    "\n",
    "    # Step 7: Store Safe SOC Thresholds for Merging\n",
    "    device_results = pd.DataFrame({\n",
    "        'deviceID': df_device['deviceID'].values,\n",
    "        'Timestamp': df_device.index,\n",
    "        'Safe_SOC_Threshold': safe_soc_thresholds\n",
    "    })\n",
    "    all_safe_soc_thresholds.append(device_results)\n",
    "\n",
    "    # Step 8: Identify Gaps Greater Than 1 Day\n",
    "    gap_threshold = pd.Timedelta(days=1)\n",
    "    time_gaps = df_device.index.to_series().diff() > gap_threshold\n",
    "    segment_indices = np.where(time_gaps)[0]\n",
    "\n",
    "    # Step 9: Plot Results for Each Device with Unlinked Segments\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    first_segment = True\n",
    "\n",
    "    start_idx = 0\n",
    "    for idx in segment_indices:\n",
    "        if first_segment:\n",
    "            plt.plot(df_device.index[start_idx:idx], y_full[start_idx:idx], label=\"Actual SOC\", color='black', linestyle=\"dashed\")\n",
    "            plt.plot(df_device.index[start_idx:idx], y_pred_full[start_idx:idx], label=\"Predicted SOC\", color='red')\n",
    "            plt.plot(df_device.index[start_idx:idx], safe_soc_thresholds[start_idx:idx], linestyle=\"dashdot\", color='blue', label=\"Safe SOC Threshold\")\n",
    "            first_segment = False\n",
    "        else:\n",
    "            plt.plot(df_device.index[start_idx:idx], y_full[start_idx:idx], color='black', linestyle=\"dashed\")\n",
    "            plt.plot(df_device.index[start_idx:idx], y_pred_full[start_idx:idx], color='red')\n",
    "            plt.plot(df_device.index[start_idx:idx], safe_soc_thresholds[start_idx:idx], linestyle=\"dashdot\", color='blue')\n",
    "\n",
    "        start_idx = idx  \n",
    "\n",
    "    plt.plot(df_device.index[start_idx:], y_full[start_idx:], color='black', linestyle=\"dashed\")\n",
    "    plt.plot(df_device.index[start_idx:], y_pred_full[start_idx:], color='red')\n",
    "    plt.plot(df_device.index[start_idx:], safe_soc_thresholds[start_idx:], linestyle=\"dashdot\", color='blue')\n",
    "\n",
    "    plt.xlabel(\"Time\")\n",
    "    plt.ylabel(\"SOC Battery Level (%)\")\n",
    "    plt.title(f\"Device {device_id}: SOC Prediction & Safe Threshold Over Time (Unlinked Gaps > 1 Day)\")\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    plt.show()\n",
    "\n",
    "# Step 10: Merge Computed Safe SOC Thresholds into `df_gdf`\n",
    "safe_soc_thresholds_df = pd.concat(all_safe_soc_thresholds)\n",
    "df_gdf = df_gdf.merge(safe_soc_thresholds_df, on=['deviceID', 'Timestamp'], how='left')\n",
    "\n",
    "# Step 11: Replace Static Threshold with Dynamic Safe SOC\n",
    "df_gdf['Depleted'] = df_gdf['SOC_batt'] < df_gdf['Safe_SOC_Threshold']  # Dynamic threshold check\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **SOC Depletion Modeling with Sensor OFF Strategies**\n",
    "\n",
    "## **Context and Objective**\n",
    "This implementation models the **State of Charge (SOC) depletion** of sensor-equipped vehicles while applying **dynamic sensor OFF strategies** based on pre-defined **time thresholds**. The goal is to assess the impact of temporarily turning **OFF** sensors to conserve energy, tracking **SOC savings**, and evaluating the difference in battery depletion rates under different sensor control policies.\n",
    "\n",
    "The core of this method involves:\n",
    "1. **Tracking the energy savings** from sensor OFF periods.\n",
    "2. **Modeling sensor activation behavior** based on pre-defined **time thresholds**.\n",
    "3. **Computing the modified SOC depletion** with stored energy savings.\n",
    "4. **Comparing the baseline SOC depletion with sensor control policies.**\n",
    "\n",
    "## **1. Data Preparation and Sorting**\n",
    "The dataset is first copied and indexed sequentially to ensure **chronological order**:\n",
    "\n",
    "$$\n",
    "\\text{Sort}(df, \\text{by}=[\\text{Timestamp}])\n",
    "$$\n",
    "\n",
    "This sorting is essential for maintaining **temporal consistency**, ensuring that energy tracking occurs in the correct **sequential order**.\n",
    "\n",
    "## **2. Definition of Sensor OFF Strategy**\n",
    "A sensor OFF threshold $ T_{\\text{threshold}} $ is defined, which determines how long a **grid cell** can remain **inactive** before reactivation is allowed. Given:\n",
    "\n",
    "$$\n",
    "T_{\\text{threshold}} = \\{ 12 \\text{ sec} \\}\n",
    "$$\n",
    "\n",
    "a sensor remains **OFF** if a vehicle has recently occupied the same **grid cell** within the threshold:\n",
    "\n",
    "$$\n",
    "\\text{Sensor\\_ON}_i =\n",
    "\\begin{cases} \n",
    "0, & \\text{if } (t_i - t_{\\text{last sensed}}) < T_{\\text{threshold}} \\\\\n",
    "1, & \\text{otherwise}\n",
    "\\end{cases}\n",
    "$$\n",
    "\n",
    "where:\n",
    "- $ t_i $ is the **current timestamp**.\n",
    "- $ t_{\\text{last sensed}} $ is the timestamp of the **last recorded sensor activation**.\n",
    "- $ T_{\\text{threshold}} $ is the **predefined time limit** for keeping the sensor OFF.\n",
    "\n",
    "## **3. Energy Storage Mechanism**\n",
    "During **sensor OFF periods**, the energy that would have been consumed is tracked using a **stored energy accumulation model**. The change in **SOC depletion rate** due to the sensor OFF mechanism is computed as:\n",
    "\n",
    "$$\n",
    "\\Delta \\text{SOC}_i = \\max(0, \\text{SOC}_{i-1} - \\text{SOC}_i)\n",
    "$$\n",
    "\n",
    "where:\n",
    "- $ \\text{SOC}_i $ is the **current SOC measurement**.\n",
    "- $ \\text{SOC}_{i-1} $ is the **previous SOC measurement**.\n",
    "\n",
    "Stored energy savings accumulate over time:\n",
    "\n",
    "$$\n",
    "\\text{Energy\\_Saved}_i = \\sum_{j=1}^{i} \\Delta \\text{SOC}_j, \\quad \\text{if Sensor\\_ON} = 0\n",
    "$$\n",
    "\n",
    "where:\n",
    "- $ \\text{Energy\\_Saved}_i $ represents the **total accumulated SOC savings**.\n",
    "- The accumulation occurs **only when the sensor is OFF**.\n",
    "\n",
    "## **4. Dynamic SOC Update with Energy Savings**\n",
    "The SOC for a given time step is **recomputed** using the stored energy:\n",
    "\n",
    "$$\n",
    "\\text{SOC\\_batt}_i' = \\text{SOC\\_batt}_i + \\text{Energy\\_Saved}_i\n",
    "$$\n",
    "\n",
    "where:\n",
    "- $ \\text{SOC\\_batt}_i' $ is the **corrected SOC value** incorporating energy savings.\n",
    "- $ \\text{SOC\\_batt}_i $ is the **original SOC value**.\n",
    "- $ \\text{Energy\\_Saved}_i $ represents the **cumulative stored SOC improvements**.\n",
    "\n",
    "To ensure **SOC does not exceed 100%**, the final corrected SOC values are clipped:\n",
    "\n",
    "$$\n",
    "\\text{SOC\\_batt}_i' = \\min(100, \\text{SOC\\_batt}_i + \\text{Energy\\_Saved}_i)\n",
    "$$\n",
    "\n",
    "## **5. Baseline and Threshold Comparisons**\n",
    "To assess the impact of sensor OFF strategies, SOC depletion is **compared across different thresholds**. The average daily SOC depletion per device is computed as:\n",
    "\n",
    "$$\n",
    "\\text{SOC\\_depletion} = \\frac{1}{N} \\sum_{i=1}^{N} \\text{SOC\\_batt}_i'\n",
    "$$\n",
    "\n",
    "where:\n",
    "- $ N $ is the number of **time steps in a day**.\n",
    "- $ \\text{SOC\\_batt}_i' $ represents the **SOC levels incorporating sensor OFF savings**.\n",
    "\n",
    "The **baseline depletion rate** without any sensor OFF strategy is also computed:\n",
    "\n",
    "$$\n",
    "\\text{Baseline\\_SOC} = \\frac{1}{N} \\sum_{i=1}^{N} \\text{SOC\\_batt}_i\n",
    "$$\n",
    "\n",
    "where no energy savings are incorporated.\n",
    "\n",
    "## **6. Expected Outcomes and Justification**\n",
    "This method enables:\n",
    "- **Quantification of energy savings** from sensor OFF strategies.\n",
    "- **Comparison of SOC depletion trends** across different sensor OFF thresholds.\n",
    "- **Validation of sensor optimization policies** to maximize battery lifespan.\n",
    "\n",
    "By implementing **grid-based sensing optimization**, the system **reduces unnecessary sensor activations**, ensuring **longer sensor endurance** while maintaining **effective data collection**.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_temp = df.copy().reset_index(drop=True)  # Ensure indices are sequential\n",
    "df_temp=df_temp.sort_values(by=['Timestamp']).reset_index(drop=True) \n",
    "\n",
    "# Define different time thresholds to compare\n",
    "time_thresholds = {\n",
    "    # \"3 sec\": 3,\n",
    "    \"360 sec\": 360\n",
    "}\n",
    "\n",
    "# Create a dictionary to store SOC and sensor states for each threshold\n",
    "soc_depletion_results = {}\n",
    "\n",
    "# Iterate over different time thresholds\n",
    "for label, TIME_THRESHOLD in time_thresholds.items():\n",
    "\n",
    "    # Track last sensed timestamp, and stored energy during OFF periods\n",
    "    last_sensed_time = {}\n",
    "    stored_energy = {}\n",
    "\n",
    "    # Previous date\n",
    "    prev_date=None\n",
    "    \n",
    "    for i in range(len(df_temp)):\n",
    "        row = df_temp.iloc[i]\n",
    "        grid_key = (row['Lat_Grid'], row['Log_Grid'])  # Unique grid identifier\n",
    "        current_time = row['Timestamp']\n",
    "        current_date = row['Date']\n",
    "        device= row['deviceID']\n",
    "\n",
    "        # Initialise inter-row differences when OFF\n",
    "        d_diff_prev=0 \n",
    "        \n",
    "        # Reset stored energy at the start of a new day\n",
    "        if prev_date is not None and current_date != prev_date:\n",
    "            stored_energy={}  # Reset stored energy for all grid cells\n",
    "            df_temp.loc[i:, f'Energy_Saved_{label}'] = 0  # Reset energy savings for the new day\n",
    "            print(f\"[RESET] Reset stored energy for new day: {current_date}\")\n",
    "\n",
    "        prev_date = current_date  # Update previous date tracker\n",
    "\n",
    "        if df_temp.loc[i, 'SOC_batt']>99:\n",
    "            stored_energy[grid_key]=0\n",
    "            df_temp.loc[i:, f'Energy_Saved_{label}']=0\n",
    "\n",
    "        # If the grid was sensed recently (within the threshold), turn OFF the sensor\n",
    "        if grid_key in last_sensed_time and (current_time - last_sensed_time[grid_key]).total_seconds() < TIME_THRESHOLD:\n",
    "            df_temp.at[i, f'Sensor_ON_{label}'] = False   \n",
    "\n",
    "            # Accumulate stored energy\n",
    "            if i > 0 and pd.notna(df_temp.iloc[i - 1]['SOC_batt']) and pd.notna(row['SOC_batt']):\n",
    "            \n",
    "                # Find the last preceding row for this device\n",
    "                if device == df_temp.iloc[i-1]['deviceID']:\n",
    "                    d_diff = max(0, df_temp.iloc[i - 1]['SOC_batt'] - row['SOC_batt']) #current inter-row difference\n",
    "                    diff = max(d_diff_prev, d_diff)\n",
    "                    stored_energy[grid_key] = df_temp.loc[i-1, f'Energy_Saved_{label}']\n",
    "                    stored_energy[grid_key] += diff\n",
    "                    \n",
    "                    if diff != 0: \n",
    "                        df_temp.loc[i:, f'Energy_Saved_{label}'] = stored_energy[grid_key]\n",
    "                    else:\n",
    "                        df_temp.loc[i:, f'Energy_Saved_{label}'] = df_temp.loc[i-1, f'Energy_Saved_{label}']\n",
    "\n",
    "                    d_diff_prev=diff\n",
    "                    print(f\"[OFF]: Accumulated {diff:.2f}% for device {device}. Total stored: {stored_energy[grid_key]:.2f}%\")\n",
    "\n",
    "                else:\n",
    "                    d_diff = max(0, df_temp.loc[df_temp.deviceID == device, :]['SOC_batt'].iloc[-1] - row['SOC_batt']) #current inter-row difference\n",
    "                    diff = max(d_diff_prev, d_diff)\n",
    "                    stored_energy[grid_key] = df_temp.loc[df_temp.deviceID == device, :][f'Energy_Saved_{label}'].iloc[-1]\n",
    "                    stored_energy[grid_key] += diff\n",
    "                    \n",
    "                    if diff != 0: \n",
    "                        df_temp.loc[i:, f'Energy_Saved_{label}'] = stored_energy[grid_key]\n",
    "                    else:\n",
    "                        df_temp.loc[i:, f'Energy_Saved_{label}'] = df_temp.loc[df_temp.deviceID == device, :][f'Energy_Saved_{label}'].iloc[-1]\n",
    "\n",
    "                    d_diff_prev=diff\n",
    "                    print(f\"CHANGE [OFF]: Accumulated {diff:.2f}% for device {device}. Total stored: {stored_energy[grid_key]:.2f}%\")\n",
    "\n",
    "        else:\n",
    "            # Update last sensed time when the sensor turns ON\n",
    "            last_sensed_time[grid_key] = current_time\n",
    "            df_temp.at[i, f'Sensor_ON_{label}'] = True\n",
    "            d_diff_prev=0\n",
    "\n",
    "            if device == df_temp.iloc[i-1]['deviceID']:\n",
    "\n",
    "                # Ensure stored_energy is initialized per grid cell without overwriting previous values\n",
    "                if grid_key not in stored_energy:\n",
    "                    if i > 0:\n",
    "                        #Carry forward the stored energy from the last known row\n",
    "                        stored_energy[grid_key] = df_temp.loc[i-1, f'Energy_Saved_{label}']\n",
    "                        df_temp.loc[i:, f'Energy_Saved_{label}'] = stored_energy[grid_key]\n",
    "                    elif i == 0:\n",
    "                        stored_energy[grid_key] = 0  # First iteration, no prior energy \n",
    "                        df_temp.loc[i:, f'Energy_Saved_{label}'] = stored_energy[grid_key]  \n",
    "                print(f\"[ON]: device {device}. Total stored: {stored_energy[grid_key]:.2f}%\")\n",
    "\n",
    "            else:\n",
    "                 # Ensure stored_energy is initialized per grid cell without overwriting previous values\n",
    "                if grid_key not in stored_energy:\n",
    "                    if i > 0:\n",
    "                        #Carry forward the stored energy from the last known row\n",
    "                        stored_energy[grid_key] = df_temp.loc[df_temp.deviceID == device, :][f'Energy_Saved_{label}'].iloc[-1]\n",
    "                        df_temp.loc[i:, f'Energy_Saved_{label}'] = stored_energy[grid_key]\n",
    "                    elif i == 0:\n",
    "                        stored_energy[grid_key] = 0  # First iteration, no prior energy \n",
    "                        df_temp.loc[i:, f'Energy_Saved_{label}'] = stored_energy[grid_key]                 \n",
    "                \n",
    "                print(f\"CHANGE [ON]: device {device}. Total stored: {stored_energy[grid_key]:.2f}%\")\n",
    "\n",
    "    \n",
    "    # Compute new SOC_batt with savings\n",
    "    df_temp[f'SOC_batt_{label}'] = df_temp['SOC_batt'] + df_temp[f'Energy_Saved_{label}']\n",
    "    df_temp[f'SOC_batt_{label}'] = df_temp[f'SOC_batt_{label}'].clip(upper=100)\n",
    "\n",
    "    # Compute SOC depletion for this threshold\n",
    "    daily_soc = df_temp.groupby(['Date', 'deviceID'])[f'SOC_batt_{label}'].mean()\n",
    "    soc_depletion_results[label] = daily_soc\n",
    "\n",
    "\n",
    "# Baseline: Compute SOC depletion without constraints\n",
    "soc_depletion_results[\"Baseline\"] = df_temp.groupby(['Date', 'deviceID'])['SOC_batt'].mean()\n",
    "\n",
    "# Convert results to a DataFrame for plotting\n",
    "soc_depletion_df = pd.DataFrame(soc_depletion_results)\n",
    "\n",
    "# Save the updated dataset with sensor states and energy savings for each threshold\n",
    "output_path = \"/workspace/data/updated_SOC_batt_with_energy_savings.xlsx\"\n",
    "df_temp.to_excel(output_path, index=False)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define line styles and transparency levels for each threshold\n",
    "line_styles = {\n",
    "    \"Baseline\": \"--\",\n",
    "    # \"3 sec\": \"--\",\n",
    "    \"360 sec\": \"-\"\n",
    "}\n",
    "\n",
    "# Transparency levels for each threshold\n",
    "alpha_values = {\n",
    "    \"Baseline\": 0.5,  # 50% transparent\n",
    "    # \"3 sec\": 0.7,  # 30% transparent\n",
    "    \"360 sec\": 1   #Fully visible\n",
    "}\n",
    "\n",
    "# Predefined colors for devices\n",
    "predefined_colors = ['#007FFF', '#DC143C', '#FF4500','#39FF14', '#800080']\n",
    "device_ids = set()\n",
    "\n",
    "for soc_series in soc_depletion_results.values():\n",
    "    device_ids.update(soc_series.index.get_level_values('deviceID').unique())\n",
    "\n",
    "# Create a color map using predefined colors\n",
    "color_map = {device_id: predefined_colors[i % len(predefined_colors)] for i, device_id in enumerate(sorted(device_ids))}\n",
    "\n",
    "# Plot SOC depletion for different devices and thresholds\n",
    "plt.figure(figsize=(12, 6))\n",
    "\n",
    "# Iterate over thresholds and plot per device\n",
    "for label, soc_series in soc_depletion_results.items():  # soc_series is a MultiIndexed Series\n",
    "    for device_id in soc_series.index.get_level_values('deviceID').unique():  # Get unique devices\n",
    "        device_data = soc_series[soc_series.index.get_level_values('deviceID') == device_id]\n",
    "        plt.plot(\n",
    "            device_data.index.get_level_values('Date'),  # X-axis: Dates\n",
    "            device_data.values,  # Y-axis: SOC values\n",
    "            linestyle=line_styles[label],\n",
    "            color=color_map[device_id],  # Use predefined color for the device\n",
    "            # marker='o',\n",
    "            # markersize=3,\n",
    "            alpha=alpha_values[label],  # Apply transparency per threshold\n",
    "            label=f\"Device {device_id} - {label}\"\n",
    "        )\n",
    "\n",
    "plt.xlabel('Date')\n",
    "plt.ylabel('Mean SOC (%)')\n",
    "plt.title('SOC Depletion Comparison Across Devices and Time Constraints')\n",
    "\n",
    "# Place the legend outside the plot\n",
    "plt.legend(\n",
    "    bbox_to_anchor=(1.05, 1),  # Place legend to the right of the plot\n",
    "    loc='upper left',          # Align legend to the top-left of the bounding box\n",
    "    borderaxespad=0.           # Reduce spacing between the legend and the plot\n",
    ")\n",
    "plt.grid(True)\n",
    "plt.xticks(rotation=45)\n",
    "\n",
    "# Adjust layout to make room for the legend\n",
    "plt.tight_layout()\n",
    "\n",
    "# Show plot\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Baseline vs. Adaptive Gamma Scaling for Grid Analysis\n",
    "\n",
    "### Baseline Calculation (Without Log Transformation)\n",
    "1. **Compute Gamma Scaling Parameters:**\n",
    "   The baseline gamma scaling factor is computed using the mean ($ \\mu $) and standard deviation ($ \\sigma $) of the counts in each grid cell. The **Coefficient of Variation (CV)** is defined as:\n",
    "\n",
    "   $\n",
    "   CV = \\frac{\\sigma}{\\mu}\n",
    "   $\n",
    "\n",
    "   The gamma scaling factor is then calculated as:\n",
    "\n",
    "   $\n",
    "   \\text{baseline\\_gamma} = \\frac{1}{1 + CV}\n",
    "   $\n",
    "\n",
    "2. **Apply Gamma Scaling:**\n",
    "   To scale the counts, the following transformation is applied:\n",
    "\n",
    "   $\n",
    "   \\text{Scaled\\_Count\\_Baseline} = (\\text{Count} + 1)^{\\text{baseline\\_gamma}}\n",
    "   $\n",
    "\n",
    "3. **Normalize Counts:**\n",
    "   The scaled counts are normalized to facilitate visual comparison between different grid cells.\n",
    "\n",
    "---\n",
    "\n",
    "### Adaptive Calculation (Sensor-On Data)\n",
    "1. **Filtering Sensor-On Data:**  \n",
    "   Only the rows where the sensor is active are considered for the analysis.\n",
    "\n",
    "2. **Perform Spatial Join:**  \n",
    "   The filtered data points are spatially joined with the adaptive grid cells to identify which grid cell each point belongs to.\n",
    "\n",
    "3. **Count Calculation:**  \n",
    "   The number of points per grid cell is aggregated using:\n",
    "\n",
    "   $\n",
    "   \\text{Count} = \\sum_{i=1}^{N} \\text{Point}_i\n",
    "   $\n",
    "\n",
    "4. **Apply Gamma Scaling:**  \n",
    "   Similar to the baseline calculation, the adaptive scaling factor is computed as:\n",
    "\n",
    "   $\n",
    "   \\text{adaptive\\_gamma} = \\frac{1}{1 + CV_{\\text{adaptive}}}\n",
    "   $\n",
    "\n",
    "   The scaled counts are then calculated as:\n",
    "\n",
    "   $\n",
    "   \\text{Scaled\\_Count\\_Adaptive} = (\\text{Count} + 1)^{\\text{adaptive\\_gamma}}\n",
    "   $\n",
    "\n",
    "5. **Normalize Counts:**  \n",
    "   The adaptive scaled counts are normalized in a similar fashion to the baseline counts.\n",
    "\n",
    "---\n",
    "\n",
    "### Common Scaling for Comparison\n",
    "1. **Determine Global Minimum and Maximum:**  \n",
    "   The global minimum and maximum values are computed for both baseline and adaptive scaled counts to ensure consistent scaling.\n",
    "\n",
    "2. **Sigmoid Scaling for Visualization:**  \n",
    "   To enhance contrast for visualization, a sigmoid function is applied to the normalized counts:\n",
    "\n",
    "   $\n",
    "   \\text{Visual} = \\frac{1}{1 + \\exp\\left(-k \\left(\\text{Normalized\\_Scaled} - 0.5\\right)\\right)}\n",
    "   $\n",
    "\n",
    "   Where:\n",
    "   - $ k $ is a tunable parameter that controls the sharpness of the sigmoid curve.\n",
    "   - $\\text{Normalized\\_Scaled}$ represents the normalized count values scaled to a range of [0, 1].\n",
    "\n",
    "---\n",
    "\n",
    "### Common $ v_{min} $ Calculation\n",
    "The common minimum value for visualization is determined by taking the minimum of the baseline and adaptive visual representations.\n",
    "\n",
    "---\n",
    "\n",
    "### Common $ v_{max} $ Calculation (Using Kneedle Algorithm)\n",
    "1. **Quantile Range Creation:**  \n",
    "   A range of quantile values ($ q $) from 0.90 to 1.0 is generated to capture the upper tail of the distribution.\n",
    "\n",
    "2. **CV Difference Calculation:**  \n",
    "   The coefficient of variation ($CV$) is calculated at each quantile cutoff for both baseline and adaptive visualizations. The difference between the two is then recorded as:\n",
    "\n",
    "   $\n",
    "   \\text{CV Difference} = \\left| \\text{CV}_{\\text{Baseline}} - \\text{CV}_{\\text{Adaptive}} \\right|\n",
    "   $\n",
    "\n",
    "3. **Filtering and Kneedle Detection:**  \n",
    "   Initial flat portions of the curve are removed using a threshold filter to eliminate insignificant changes.  \n",
    "   The **Kneedle Algorithm** is applied to detect the optimal quantile where the difference in CVs is most pronounced.\n",
    "\n",
    "4. **Determine Common $ v_{max} $:**  \n",
    "   The common maximum value is determined by taking the minimum of the baseline and adaptive quantiles at the optimal quantile level:\n",
    "\n",
    "   $\n",
    "   v_{\\text{max}} = \\min\\left( \\text{Baseline Quantile}, \\text{Adaptive Quantile} \\right)\n",
    "   $\n",
    "\n",
    "---\n",
    "\n",
    "### Visualization and Results Display\n",
    "The calculated values of $ v_{\\text{min}} $ and $ v_{\\text{max}} $ are applied for consistent and enhanced visualization of both baseline and adaptive counts. The optimal quantile provides a well-calibrated view of the difference between baseline and adaptive approaches.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make copies to avoid overwriting\n",
    "grid_gdf_baseline = grid_gdf.copy()\n",
    "grid_gdf_adaptive = grid_gdf.copy()\n",
    "\n",
    "### BASELINE\n",
    "# Compute Gamma Scaling on Raw Counts (Without Log Transformation)\n",
    "baseline_counts = df_temp.groupby(['Lat_Grid', 'Log_Grid']).size()\n",
    "mean_count_baseline = grid_gdf_baseline['Count'].mean()\n",
    "std_count_baseline = grid_gdf_baseline['Count'].std()\n",
    "CV_baseline = std_count_baseline / mean_count_baseline if mean_count_baseline > 0 else 1\n",
    "baseline_gamma = 1 / (1 + CV_baseline)\n",
    "\n",
    "# Apply gamma scaling directly on raw counts\n",
    "grid_gdf_baseline['Scaled_Count_Baseline'] = (grid_gdf_baseline['Count'] + 1) ** baseline_gamma\n",
    "\n",
    "# Normalize the Scaled Counts for Better Visual Comparison\n",
    "scaled_min_baseline = grid_gdf_baseline['Scaled_Count_Baseline'].min()\n",
    "scaled_max_baseline = grid_gdf_baseline['Scaled_Count_Baseline'].max()\n",
    "\n",
    "### ADAPTIVE\n",
    "# Define the label used in code\n",
    "label = \"360 sec\"\n",
    "\n",
    "# Filter the dataset to include only rows where the sensor is ON\n",
    "adaptive_df = df_temp[df_temp[f'Sensor_ON_{label}'] == True]\n",
    "\n",
    "# Create a GeoDataFrame for the points in df\n",
    "adaptive_df_gdf = gpd.GeoDataFrame(adaptive_df, geometry=gpd.points_from_xy(adaptive_df['Log'], adaptive_df['Lat']), crs=\"EPSG:4326\")\n",
    "\n",
    "# Perform spatial join for Adaptive case\n",
    "joined = gpd.sjoin(adaptive_df_gdf, grid_gdf_adaptive, how=\"left\", predicate=\"within\")\n",
    "grid_gdf_adaptive['Count'] = joined.groupby(joined.index_right).size()\n",
    "grid_gdf_adaptive['Count'].fillna(0, inplace=True)\n",
    "\n",
    "# Apply Gamma Scaling on Raw Counts (Without Log Transformation)\n",
    "adaptive_counts = adaptive_df.groupby(['Lat_Grid', 'Log_Grid']).size()\n",
    "mean_count_adaptive = grid_gdf_adaptive['Count'].mean()\n",
    "std_count_adaptive = grid_gdf_adaptive['Count'].std()\n",
    "CV_adaptive = std_count_adaptive / mean_count_adaptive if mean_count_adaptive > 0 else 1\n",
    "adaptive_gamma = 1 / (1 + CV_adaptive)\n",
    "\n",
    "# Apply gamma scaling directly on raw counts\n",
    "grid_gdf_adaptive['Scaled_Count_Adaptive'] = (grid_gdf_adaptive['Count'] + 1) ** adaptive_gamma\n",
    "\n",
    "# Normalize the Scaled Counts for Better Visual Comparison\n",
    "scaled_min_adaptive = grid_gdf_adaptive['Scaled_Count_Adaptive'].min()\n",
    "scaled_max_adaptive = grid_gdf_adaptive['Scaled_Count_Adaptive'].max()\n",
    "\n",
    "# Global min and max for both datasets\n",
    "global_min = min(scaled_min_baseline, scaled_min_adaptive)\n",
    "global_max = max(scaled_max_baseline, scaled_max_adaptive)\n",
    "print(f\"Global Min: {global_min}, Global Max: {global_max}\")\n",
    "\n",
    "# Apply Sigmoid Scaling ONLY for Visualization (Baseline)\n",
    "grid_gdf_baseline['Normalized_Scaled_Baseline'] = (grid_gdf_baseline['Scaled_Count_Baseline'] - global_min) / (global_max - global_min)\n",
    "k = 10  # Adjust this value for better contrast enhancement\n",
    "grid_gdf_baseline['Visual_Baseline'] = 1 / (1 + np.exp(-k * (grid_gdf_baseline['Normalized_Scaled_Baseline'] - 0.5)))\n",
    "\n",
    "# Apply Sigmoid Scaling ONLY for Visualization (Adaptive)\n",
    "grid_gdf_adaptive['Normalized_Scaled_Adaptive'] = (grid_gdf_adaptive['Scaled_Count_Adaptive'] - global_min) / (global_max - global_min)\n",
    "grid_gdf_adaptive['Visual_Adaptive'] = 1 / (1 + np.exp(-k * (grid_gdf_adaptive['Normalized_Scaled_Adaptive'] - 0.5)))\n",
    "\n",
    "\n",
    "### COMMON VMIN\n",
    "# Compute the common vmin for both Baseline and Adaptive visualizations\n",
    "common_vmin = min(\n",
    "    grid_gdf_baseline['Visual_Baseline'].min(),\n",
    "    grid_gdf_adaptive['Visual_Adaptive'].min()\n",
    ")\n",
    "\n",
    "print(f\"Computed common_vmin: {common_vmin:.4f}\")\n",
    "\n",
    "### COMMON VMAX \n",
    "\n",
    "# Generate quantile range\n",
    "quantile_range = np.linspace(0.90, 1, 1000)\n",
    "cv_differences = []\n",
    "\n",
    "for q in quantile_range:\n",
    "    baseline_q = grid_gdf_baseline['Visual_Baseline'].quantile(q)\n",
    "    adaptive_q = grid_gdf_adaptive['Visual_Adaptive'].quantile(q)\n",
    "    \n",
    "    cv_baseline = np.std(grid_gdf_baseline['Visual_Baseline'][grid_gdf_baseline['Visual_Baseline'] <= baseline_q]) / np.mean(grid_gdf_baseline['Visual_Baseline'][grid_gdf_baseline['Visual_Baseline'] <= baseline_q])\n",
    "    cv_adaptive = np.std(grid_gdf_adaptive['Visual_Adaptive'][grid_gdf_adaptive['Visual_Adaptive'] <= adaptive_q]) / np.mean(grid_gdf_adaptive['Visual_Adaptive'][grid_gdf_adaptive['Visual_Adaptive'] <= adaptive_q])\n",
    "    \n",
    "    cv_differences.append(abs(cv_baseline - cv_adaptive))\n",
    "\n",
    "# Filter out the initial flat part by thresholding\n",
    "threshold_value = 0.01  # Adjust this value if needed\n",
    "filtered_indices = np.where(np.array(cv_differences) > threshold_value)[0]\n",
    "\n",
    "# Apply Kneedle only if there's a significant increase\n",
    "if len(filtered_indices) > 0:\n",
    "    filtered_quantiles = quantile_range[filtered_indices]\n",
    "    filtered_cv_differences = np.array(cv_differences)[filtered_indices]\n",
    "\n",
    "    # Use kneed to find the optimal quantile\n",
    "    kneedle = KneeLocator(filtered_quantiles, filtered_cv_differences, curve='convex', direction='increasing')\n",
    "    optimal_quantile = round(kneedle.elbow, 3)  # Display to three decimals\n",
    "else:\n",
    "    # Fallback if no significant increase is detected\n",
    "    optimal_quantile = 0.990  # A sensible high quantile to use\n",
    "\n",
    "# Plotting the improvement trend\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.plot(quantile_range, cv_differences, color='blue', label='CV Difference')\n",
    "plt.axvline(optimal_quantile, color='red', linestyle='--', label=f'Optimal Quantile = {optimal_quantile:.3f}')\n",
    "plt.title('CV Difference vs Quantile Cutoff (Using Kneedle on Filtered Data)')\n",
    "plt.xlabel('Quantile Cutoff')\n",
    "plt.ylabel('CV Difference')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()\n",
    "\n",
    "print(f\"Optimal Quantile Found: {optimal_quantile:.3f}\")\n",
    "\n",
    "\n",
    "common_vmax = min(\n",
    "    grid_gdf_baseline['Visual_Baseline'].quantile(optimal_quantile),\n",
    "    grid_gdf_adaptive['Visual_Adaptive'].quantile(optimal_quantile)\n",
    ")\n",
    "\n",
    "print(f\"Computed common_vmin: {common_vmax:.4f}\")\n",
    "\n",
    "\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Baseline Optimal Gamma: {baseline_gamma:.4f}\")\n",
    "\n",
    "colormap = cm.LinearColormap(\n",
    "    colors=['darkblue', 'cyan', 'yellow', 'orangered'],\n",
    "    vmin=common_vmin,\n",
    "    vmax=common_vmax\n",
    ")\n",
    "\n",
    "# Folium map\n",
    "m = folium.Map(location=[df['Lat'].mean(), df['Log'].mean()], zoom_start=12, tiles='Cartodb dark_matter')\n",
    "\n",
    "# Add GeoJSON overlay\n",
    "geojson = folium.GeoJson(\n",
    "    grid_gdf_baseline,\n",
    "    style_function=lambda feature: {\n",
    "        'fillColor': colormap(feature['properties']['Visual_Baseline']),\n",
    "        'weight': 0.0,\n",
    "        'fillOpacity': 0.4\n",
    "    }\n",
    ").add_to(m)\n",
    "\n",
    "colormap.caption = \"Scaled Count Intensity (Baseline)\"\n",
    "colormap.add_to(m)  # Attach to the map\n",
    "\n",
    "m"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Baseline vs. Adaptive Sensing Statistics and Improvement Calculation\n",
    "\n",
    "### Baseline Statistics Calculation\n",
    "The baseline statistics are calculated based on all the measurements collected within grid cells. \n",
    "\n",
    "1. **Count Calculation:**\n",
    "   The total number of measurements per grid cell is calculated using:\n",
    "\n",
    "   $\n",
    "   \\text{Baseline Counts} = \\sum_{i=1}^{N} \\text{Point}_i\n",
    "   $\n",
    "\n",
    "   Where $ N $ is the total number of measurements per grid cell.\n",
    "\n",
    "2. **Statistical Metrics:**\n",
    "   - Mean count ($ \\mu_{\\text{baseline}} $):\n",
    "\n",
    "    $\n",
    "    \\mu_{\\text{baseline}} = \\frac{1}{M} \\sum_{j=1}^{M} C_j\n",
    "    $\n",
    "\n",
    "    Where $ M $ is the number of grid cells, and $ C_j $ is the count of measurements in cell $ j $.\n",
    "\n",
    "   - Standard deviation ($ \\sigma_{\\text{baseline}} $):\n",
    "\n",
    "    $\n",
    "    \\sigma_{\\text{baseline}} = \\sqrt{\\frac{1}{M} \\sum_{j=1}^{M} (C_j - \\mu_{\\text{baseline}})^2}\n",
    "    $\n",
    "\n",
    "   - Coefficient of Variation ($ CV_{\\text{baseline}} $):\n",
    "\n",
    "    $\n",
    "    CV_{\\text{baseline}} = \\frac{\\sigma_{\\text{baseline}}}{\\mu_{\\text{baseline}}}\n",
    "    $\n",
    "\n",
    "---\n",
    "\n",
    "### Adaptive Sensing Statistics Calculation\n",
    "The adaptive statistics are calculated based on the filtered measurements where the sensor is **ON**.\n",
    "\n",
    "1. **Count Calculation:**\n",
    "   Similar to the baseline calculation, but only considering the filtered dataset:\n",
    "\n",
    "   $\n",
    "   \\text{Adaptive Counts} = \\sum_{i=1}^{N'} \\text{Point}_i\n",
    "   $\n",
    "\n",
    "   Where $ N' \\leq N $ represents the subset of measurements where the sensor is active.\n",
    "\n",
    "2. **Statistical Metrics:**\n",
    "\n",
    "   - Mean count ($ \\mu_{\\text{adaptive}} $):\n",
    "\n",
    "    $\n",
    "    \\mu_{\\text{adaptive}} = \\frac{1}{M'} \\sum_{j=1}^{M'} C_j\n",
    "    $\n",
    "\n",
    "   - Standard deviation ($ \\sigma_{\\text{adaptive}} $):\n",
    "\n",
    "    $\n",
    "    \\sigma_{\\text{adaptive}} = \\sqrt{\\frac{1}{M'} \\sum_{j=1}^{M'} (C_j - \\mu_{\\text{adaptive}})^2}\n",
    "    $\n",
    "\n",
    "   - Coefficient of Variation ($ CV_{\\text{adaptive}} $):\n",
    "\n",
    "    $\n",
    "    CV_{\\text{adaptive}} = \\frac{\\sigma_{\\text{adaptive}}}{\\mu_{\\text{adaptive}}}\n",
    "    $\n",
    "\n",
    "---\n",
    "\n",
    "### Improvement in Spatial Uniformity\n",
    "The improvement in spatial uniformity between the baseline and adaptive sensing techniques is quantified using the change in the Coefficient of Variation:\n",
    "\n",
    "1. **Change in Coefficient of Variation ($ \\Delta CV $):**\n",
    "\n",
    "   $\n",
    "   \\Delta CV = CV_{\\text{baseline}} - CV_{\\text{adaptive}}\n",
    "   $\n",
    "\n",
    "2. **Percentage Improvement in Uniformity:**\n",
    "\n",
    "   $\n",
    "   \\text{Percentage Improvement} = \\left( \\frac{\\Delta CV}{CV_{\\text{baseline}}} \\right) \\times 100\n",
    "   $\n",
    "\n",
    "---\n",
    "\n",
    "### Visualization of Measurement Distribution\n",
    "The distributions of counts per grid cell for both baseline and adaptive sensing methods are plotted using histograms.\n",
    "\n",
    "- The histograms are compared side-by-side to illustrate differences in measurement uniformity.\n",
    "- The title, labels, and legend provide context for understanding the comparison.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute baseline statistics\n",
    "baseline_counts = df_temp.groupby(['Lat_Grid', 'Log_Grid']).size()\n",
    "mean_count_baseline = baseline_counts.mean()\n",
    "std_count_baseline = baseline_counts.std()\n",
    "cv_baseline = std_count_baseline / mean_count_baseline\n",
    "\n",
    "print(f\"Baseline Statistics:\")\n",
    "print(f\"Mean Count: {mean_count_baseline:.4f}\")\n",
    "print(f\"Standard Deviation: {std_count_baseline:.4f}\")\n",
    "print(f\"CV (Baseline): {cv_baseline:.4f}\")\n",
    "\n",
    "# Define the label used in code \n",
    "label = \"360 sec\"\n",
    "\n",
    "# Filter the dataset to include only rows where the sensor is ON\n",
    "adaptive_df = df_temp[df_temp[f'Sensor_ON_{label}'] == True]\n",
    "\n",
    "# Compute adaptive statistics\n",
    "adaptive_counts = adaptive_df.groupby(['Lat_Grid', 'Log_Grid']).size()\n",
    "mean_count_adaptive = adaptive_counts.mean()\n",
    "std_count_adaptive = adaptive_counts.std()\n",
    "cv_adaptive = std_count_adaptive / mean_count_adaptive\n",
    "\n",
    "print(f\"\\n Adaptive Sensing Statistics ({label}):\")\n",
    "print(f\"Mean Count: {mean_count_adaptive:.4f}\")\n",
    "print(f\"Standard Deviation: {std_count_adaptive:.4f}\")\n",
    "print(f\"CV (Adaptive): {cv_adaptive:.4f}\")\n",
    "\n",
    "# Calculate improvement in uniformity\n",
    "delta_cv = cv_baseline - cv_adaptive\n",
    "percentage_improvement = (delta_cv / cv_baseline) * 100\n",
    "\n",
    "print(f\"\\n Improvement in Spatial Uniformity (Î”CV): {delta_cv:.4f}\")\n",
    "print(f\" Percentage Improvement in Uniformity: {percentage_improvement:.2f}%\")\n",
    "\n",
    "# Plot histogram of counts for baseline and adaptive scenarios\n",
    "plt.figure(figsize=(12, 6))\n",
    "\n",
    "plt.hist(baseline_counts, bins=50, alpha=0.5, label='Baseline (All Measurements)')\n",
    "plt.hist(adaptive_counts, bins=50, alpha=0.5, label=f'Adaptive Sensing ({label})')\n",
    "\n",
    "plt.title('Distribution of Measurements per Grid Cell')\n",
    "plt.xlabel('Number of Measurements per Grid Cell')\n",
    "plt.ylabel('Frequency')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Adaptive Optimal Gamma: {adaptive_gamma:.4f}\")\n",
    "\n",
    "colormap = cm.LinearColormap(\n",
    "    colors=['darkblue', 'cyan', 'yellow', 'orangered'],\n",
    "    vmin=common_vmin,\n",
    "    vmax=common_vmax\n",
    ")\n",
    "\n",
    "# Folium map\n",
    "m = folium.Map(location=[df['Lat'].mean(), df['Log'].mean()], zoom_start=12, tiles='Cartodb dark_matter')\n",
    "\n",
    "# Add GeoJSON overlay for the adaptive map\n",
    "geojson = folium.GeoJson(\n",
    "    grid_gdf_adaptive,\n",
    "    style_function=lambda feature: {\n",
    "        'fillColor': colormap(feature['properties']['Visual_Adaptive']),\n",
    "        'weight': 0.0,\n",
    "        'fillOpacity': 0.4\n",
    "    }\n",
    ").add_to(m)\n",
    "\n",
    "colormap.caption = \"Count Intensity (Adaptive)\"\n",
    "colormap.add_to(m)\n",
    "\n",
    "m"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Enhanced Difference Calculation for Visual Maps\n",
    "\n",
    "### Normalization of Visual Maps\n",
    "To allow for a consistent comparison between **Baseline** and **Adaptive** maps, both visual maps are normalized between $0$ and $1$ using min-max normalization:\n",
    "\n",
    "$\n",
    "\\text{Normalized\\_Visual\\_Baseline} = \\frac{\\text{Visual\\_Baseline} - \\text{visual\\_baseline\\_min}}{\\text{visual\\_baseline\\_max} - \\text{visual\\_baseline\\_min}}\n",
    "$\n",
    "\n",
    "$\n",
    "\\text{Normalized\\_Visual\\_Adaptive} = \\frac{\\text{Visual\\_Adaptive} - \\text{visual\\_adaptive\\_min}}{\\text{visual\\_adaptive\\_max} - \\text{visual\\_adaptive\\_min}}\n",
    "$\n",
    "\n",
    "Where:\n",
    "- $ \\text{visual\\_baseline\\_min}, \\text{visual\\_baseline\\_max} $ are the minimum and maximum values of the baseline visualization.\n",
    "- $ \\text{visual\\_adaptive\\_min}, \\text{visual\\_adaptive\\_max} $ are the minimum and maximum values of the adaptive visualization.\n",
    "\n",
    "---\n",
    "\n",
    "### Enhanced Difference Calculation\n",
    "The relative improvement is calculated by comparing the **Adaptive Visualization** against the **Baseline Visualization**. \n",
    "\n",
    "1. **Difference Calculation:**\n",
    "   $\n",
    "   \\text{Difference} = \\text{Normalized\\_Visual\\_Adaptive} - \\text{Normalized\\_Visual\\_Baseline}\n",
    "   $\n",
    "\n",
    "2. **Difference Sign Identification:**\n",
    "   The sign of the difference is identified to distinguish between positive and negative improvements.\n",
    "\n",
    "   $\n",
    "   \\text{Difference\\_Sign} = \\text{sign}(\\text{Difference})\n",
    "   $\n",
    "\n",
    "---\n",
    "\n",
    "### Sigmoid Scaling for Enhanced Visualization\n",
    "To enhance the contrast of the differences, a **sigmoid function** is applied. This mapping provides a more sensitive representation of differences, particularly useful for visual comparison.\n",
    "\n",
    "$\n",
    "\\text{Enhanced\\_Difference} = \\text{Difference\\_Sign} \\times \\left( \\frac{1}{1 + \\exp(-k \\cdot \\text{Difference})} - 0.5 \\right)\n",
    "$\n",
    "\n",
    "Where:\n",
    "- $ k $ is a tuning parameter controlling the steepness of the sigmoid function.  \n",
    "- Higher $ k $ values result in stronger contrast, highlighting even small differences.\n",
    "\n",
    "---\n",
    "\n",
    "### Normalization of Enhanced Differences\n",
    "The enhanced differences are normalized to a range of $ [0, 1] $ for visual mapping:\n",
    "\n",
    "$\n",
    "\\text{Enhanced\\_Difference\\_Vis} = \\frac{\\text{Enhanced\\_Difference} - \\text{enhanced\\_diff\\_min}}{\\text{enhanced\\_diff\\_max} - \\text{enhanced\\_diff\\_min}}\n",
    "$\n",
    "\n",
    "Where:\n",
    "- $ \\text{enhanced\\_diff\\_min} $ and $ \\text{enhanced\\_diff\\_max} $ are the minimum and maximum values of $ \\text{Enhanced\\_Difference} $.\n",
    "\n",
    "---\n",
    "\n",
    "### Colormap Definition\n",
    "To highlight the improvements, a **Blue-White Colormap** is defined, where:\n",
    "\n",
    "- **White (0):** Represents no improvement.  \n",
    "- **Blue (1):** Represents maximum improvement.\n",
    "\n",
    "---\n",
    "\n",
    "### Folium Map Generation\n",
    "A **Folium Map** is generated to visualize the improvements:\n",
    "\n",
    "1. **GeoJSON Overlay:**  \n",
    "   The difference map is added as a GeoJSON overlay with custom styling based on the normalized enhanced differences.\n",
    "\n",
    "2. **Colormap:**  \n",
    "   A colormap legend is added to the map to indicate the intensity of improvement."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import branca.colormap as cm\n",
    "\n",
    "\n",
    "# Ensure Both Visual Maps Are Normalized Between 0 and 1\n",
    "visual_baseline_min = grid_gdf_baseline['Visual_Baseline'].min()\n",
    "visual_baseline_max = grid_gdf_baseline['Visual_Baseline'].max()\n",
    "grid_gdf_baseline['Normalized_Visual_Baseline'] = (grid_gdf_baseline['Visual_Baseline'] - visual_baseline_min) / (visual_baseline_max - visual_baseline_min)\n",
    "\n",
    "visual_adaptive_min = grid_gdf_adaptive['Visual_Adaptive'].min()\n",
    "visual_adaptive_max = grid_gdf_adaptive['Visual_Adaptive'].max()\n",
    "grid_gdf_adaptive['Normalized_Visual_Adaptive'] = (grid_gdf_adaptive['Visual_Adaptive'] - visual_adaptive_min) / (visual_adaptive_max - visual_adaptive_min)\n",
    "\n",
    "# Calculate Relative Improvement (Enhanced Difference Calculation)\n",
    "grid_gdf_adaptive['Difference'] = grid_gdf_adaptive['Normalized_Visual_Adaptive'] - grid_gdf_baseline['Normalized_Visual_Baseline']\n",
    "grid_gdf_adaptive['Difference_Sign'] = np.sign(grid_gdf_adaptive['Difference'])\n",
    "\n",
    "# Sigmoid Scaling (Adjust k for more/less aggressive enhancement)\n",
    "# k = 30  # Increase for stronger contrast, decrease for smoother visualization\n",
    "grid_gdf_adaptive['Enhanced_Difference'] = grid_gdf_adaptive['Difference_Sign'] * (1 / (1 + np.exp(-k * grid_gdf_adaptive['Difference'])) - 0.5)\n",
    "\n",
    "# Normalization of Enhanced Differences for Optimal Visualization\n",
    "enhanced_diff_min = grid_gdf_adaptive['Enhanced_Difference'].min()\n",
    "enhanced_diff_max = grid_gdf_adaptive['Enhanced_Difference'].max()\n",
    "\n",
    "# Normalizing to range [0, 1] for Blue intensities\n",
    "grid_gdf_adaptive['Enhanced_Difference_Vis'] = (grid_gdf_adaptive['Enhanced_Difference'] - enhanced_diff_min) / (enhanced_diff_max - enhanced_diff_min)\n",
    "\n",
    "# Define a More Sensitive Colormap with Strong Contrast (Optimized Blue Scale)\n",
    "colormap = cm.LinearColormap(\n",
    "    colors=['white','blue'],  # Only Blue to highlight improvements\n",
    "    vmin=0,  # Minimum of the normalized data\n",
    "    vmax=1   # Maximum of the normalized data\n",
    ")\n",
    "\n",
    "# Folium map\n",
    "m = folium.Map(location=[df['Lat'].mean(), df['Log'].mean()], zoom_start=12, tiles='Cartodb dark_matter')\n",
    "\n",
    "# Add GeoJSON overlay for the difference map\n",
    "geojson = folium.GeoJson(\n",
    "    grid_gdf_adaptive,\n",
    "    style_function=lambda feature: {\n",
    "        'fillColor': colormap(feature['properties']['Enhanced_Difference_Vis']),\n",
    "        'weight': 0.0,\n",
    "        'fillOpacity': 0.4\n",
    "    }\n",
    ").add_to(m)\n",
    "\n",
    "colormap.caption = \"Improvement Map (Uniformity Improvement in Blue)\"\n",
    "colormap.add_to(m)\n",
    "\n",
    "m\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from scipy.stats import ttest_ind, mannwhitneyu, ks_2samp, levene\n",
    "\n",
    "# Compute Statistical Tests\n",
    "t_stat, t_pvalue = ttest_ind(baseline_counts, adaptive_counts, equal_var=False)\n",
    "u_stat, mw_pvalue = mannwhitneyu(baseline_counts, adaptive_counts, alternative='two-sided')\n",
    "ks_stat, ks_pvalue = ks_2samp(baseline_counts, adaptive_counts)\n",
    "levene_stat, levene_pvalue = levene(baseline_counts, adaptive_counts)\n",
    "\n",
    "# Improvement metrics\n",
    "delta_cv = cv_baseline - cv_adaptive\n",
    "percentage_improvement = (delta_cv / cv_baseline) * 100\n",
    "\n",
    "# Store results in a comparative DataFrame\n",
    "results_df = pd.DataFrame({\n",
    "    'Metric': ['Mean Count (Î¼)', 'Standard Deviation (Ïƒ)', 'Coefficient of Variation (CV)', \n",
    "               'Improvement in Uniformity (Î”CV)', 'Percentage Improvement in Uniformity', \n",
    "               'T-Test (p-value)', 'Mann-Whitney U (p-value)', 'Kolmogorov-Smirnov (p-value)', 'Levene Test (p-value)'],\n",
    "    'Baseline': [mean_count_baseline, std_count_baseline, cv_baseline, delta_cv, percentage_improvement,\n",
    "                 t_pvalue, mw_pvalue, ks_pvalue, levene_pvalue],\n",
    "    'Adaptive': [mean_count_adaptive, std_count_adaptive, cv_adaptive, np.nan, np.nan, np.nan, np.nan, np.nan, np.nan]\n",
    "})\n",
    "\n",
    "# Display the results\n",
    "print(results_df)\n",
    "\n",
    "# Log Transformation to make extreme values more visible\n",
    "baseline_counts_log = np.log1p(baseline_counts)  # Log transform baseline counts\n",
    "adaptive_counts_log = np.log1p(adaptive_counts)  # Log transform adaptive counts\n",
    "\n",
    "# Plot PDFs for Baseline vs. Adaptive (Log scale)\n",
    "plt.figure(figsize=(12, 6))\n",
    "sns.kdeplot(baseline_counts_log, label='Baseline', color='red', fill=True)\n",
    "sns.kdeplot(adaptive_counts_log, label='Adaptive', color='green', fill=True)\n",
    "plt.title('Log Transformed Probability Density Functions (Baseline vs. Adaptive)')\n",
    "plt.xlabel('Log(Number of Measurements per Grid Cell)')\n",
    "plt.ylabel('Density')\n",
    "plt.legend()\n",
    "plt.xlim(0.5, 2)\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Energy Efficiency Analysis\n",
    "if 'SOC_batt' in df_temp.columns and 'SOC_batt_360 sec' in df_temp.columns:\n",
    "    # Calculate total SOC for baseline and adaptive approaches (sum of SOC values)\n",
    "    total_soc_baseline = df_temp['SOC_batt'].sum()\n",
    "    total_soc_adaptive = df_temp['SOC_batt_360 sec'].sum()\n",
    "\n",
    "    # Compute energy savings as the difference in total SOC\n",
    "    energy_savings = total_soc_adaptive - total_soc_baseline\n",
    "    percentage_energy_savings = (energy_savings / total_soc_baseline) * 100\n",
    "\n",
    "    # Prepare a DataFrame for energy efficiency analysis\n",
    "    energy_efficiency_df = pd.DataFrame({\n",
    "        'Metric': ['Total SOC (Baseline)', 'Total SOC (Adaptive)', \n",
    "                   'Energy Savings (Absolute)', 'Percentage Energy Savings'],\n",
    "        'Value': [total_soc_baseline, total_soc_adaptive, energy_savings, percentage_energy_savings]\n",
    "    })\n",
    "    \n",
    "    # Plot total SOC comparison with restricted x-axis range\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    sns.kdeplot(df_temp['SOC_batt'], label='Baseline SOC', color='red', fill=True)\n",
    "    sns.kdeplot(df_temp['SOC_batt_360 sec'], label='Adaptive SOC', color='green', fill=True)\n",
    "    plt.title('Total SOC Comparison (Baseline vs. Adaptive)')\n",
    "    plt.xlabel('SOC')\n",
    "    plt.ylabel('Density')\n",
    "    plt.xlim(0, 100)  # Crop the x-axis to the valid range of SOC values\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    plt.show()\n",
    "\n",
    "    print(energy_efficiency_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Temporal Analysis of CV Over Time\n",
    "df_temp['Hour'] = pd.to_datetime(df_temp['Timestamp']).dt.hour\n",
    "\n",
    "# Group by hour and calculate CV for baseline and adaptive approaches\n",
    "hourly_stats = df_temp.groupby('Hour').agg({\n",
    "    'SOC_batt': ['mean', 'std'],\n",
    "    'SOC_batt_360 sec': ['mean', 'std']\n",
    "})\n",
    "\n",
    "# Compute CV for each hour\n",
    "hourly_stats['CV_Baseline'] = hourly_stats[('SOC_batt', 'std')] / hourly_stats[('SOC_batt', 'mean')]\n",
    "hourly_stats['CV_Adaptive'] = hourly_stats[('SOC_batt_360 sec', 'std')] / hourly_stats[('SOC_batt_360 sec', 'mean')]\n",
    "\n",
    "# Plot Temporal Changes in CV\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.plot(hourly_stats.index, hourly_stats['CV_Baseline'], label='Baseline CV', marker='o', color='red')\n",
    "plt.plot(hourly_stats.index, hourly_stats['CV_Adaptive'], label='Adaptive CV', marker='o', color='green')\n",
    "plt.title('Temporal Changes in Coefficient of Variation (CV)')\n",
    "plt.xlabel('Hour of the Day')\n",
    "plt.ylabel('Coefficient of Variation (CV)')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grid_gdf_adaptive.loc[grid_gdf_adaptive['Difference'] > 0, 'Difference'].sort_values(ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Filter positive differences and sort them in descending order\n",
    "positive_differences = grid_gdf_adaptive.loc[\n",
    "    (grid_gdf_adaptive['Difference'] > 0) & (grid_gdf_adaptive['Difference'] < .002), 'Difference'\n",
    "].sort_values(ascending=False)\n",
    "\n",
    "# Plot the distribution using Seaborn's histogram (with KDE overlay)\n",
    "plt.figure(figsize=(12, 6))\n",
    "sns.histplot(positive_differences, bins=50, kde=True, color='blue')\n",
    "plt.title('Distribution of Positive \"Difference\" Values')\n",
    "plt.xlabel('\"Difference\" Values')\n",
    "plt.ylabel('Count')\n",
    "plt.grid(True)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.metrics import silhouette_score\n",
    "from joblib import Parallel, delayed\n",
    "import hdbscan\n",
    "\n",
    "# Generate a range of potential thresholds\n",
    "thresholds = np.linspace(1e-3, 3e-1, 120) # based on previous distribution plot\n",
    "threshold_results = []\n",
    "\n",
    "def evaluate_threshold(threshold):\n",
    "    # Filter the DataFrame based on the current threshold\n",
    "    filtered_data = grid_gdf_adaptive[grid_gdf_adaptive['Difference'] > threshold]\n",
    "    spatial_data = filtered_data[['Lat_Grid', 'Log_Grid', 'Difference']].dropna()\n",
    "    \n",
    "    if spatial_data.empty:\n",
    "        return {'threshold': threshold, 'best_score': -np.inf, 'min_cluster_size': None, 'min_samples': None, 'alpha': None}\n",
    "    \n",
    "    # Normalize the data\n",
    "    scaler = StandardScaler()\n",
    "    scaled_data = scaler.fit_transform(spatial_data[['Lat_Grid', 'Log_Grid', 'Difference']])\n",
    "    \n",
    "    best_score = -np.inf\n",
    "    best_min_cluster_size = None\n",
    "    best_min_samples = None\n",
    "    \n",
    "    # Explore a reasonable set of min_cluster_size and min_samples values\n",
    "    min_cluster_sizes = np.arange(3, 300)\n",
    "    min_samples_list = np.arange(3, 300)\n",
    "    \n",
    "    results = []\n",
    "    for min_cluster_size in min_cluster_sizes:\n",
    "        for min_samples in min_samples_list:\n",
    "            hdbscan_clusterer = hdbscan.HDBSCAN(min_cluster_size=min_cluster_size, min_samples=min_samples,  gen_min_span_tree=True)\n",
    "            labels = hdbscan_clusterer.fit_predict(scaled_data)\n",
    "            \n",
    "            # Filter out noise points for silhouette score calculation\n",
    "            non_noise_data = scaled_data[labels != -1]\n",
    "            non_noise_labels = labels[labels != -1]\n",
    "            \n",
    "            if len(np.unique(non_noise_labels)) > 1:\n",
    "                silhouette_avg = silhouette_score(non_noise_data, non_noise_labels)\n",
    "                stability_score = hdbscan_clusterer.relative_validity_\n",
    "                \n",
    "                # Store the results\n",
    "                results.append({\n",
    "                    'min_cluster_size': min_cluster_size,\n",
    "                    'min_samples': min_samples,\n",
    "                    'silhouette_score': silhouette_avg,\n",
    "                    'stability_score': stability_score\n",
    "                })\n",
    "    \n",
    "    # Create a DataFrame of results for this threshold\n",
    "    results_df = pd.DataFrame(results)\n",
    "    \n",
    "    if results_df.empty:\n",
    "        return {'threshold': threshold, 'best_score': -np.inf, 'min_cluster_size': None, 'min_samples': None, 'alpha': None}\n",
    "\n",
    "    # Normalize the scores to [0, 1] range for PCA\n",
    "    scaler = MinMaxScaler()\n",
    "    normalized_scores = scaler.fit_transform(results_df[['stability_score', 'silhouette_score']].dropna())\n",
    "    \n",
    "    # Apply PCA\n",
    "    pca = PCA(n_components=2)\n",
    "    pca.fit(normalized_scores)\n",
    "    principal_components = pca.components_\n",
    "    \n",
    "    # Calculate alpha dynamically for this threshold\n",
    "    component_0 = np.abs(principal_components[0, 0])\n",
    "    component_1 = np.abs(principal_components[0, 1])\n",
    "    alpha = component_0 / (component_0 + component_1)\n",
    "\n",
    "    # Compute Combined Score\n",
    "    results_df['combined_score'] = (\n",
    "        alpha * results_df['stability_score'] + (1 - alpha) * results_df['silhouette_score']\n",
    "    )\n",
    "    \n",
    "    # Find the best configuration for this threshold\n",
    "    best_index = results_df['combined_score'].idxmax()\n",
    "    best_config = results_df.loc[best_index]\n",
    "    \n",
    "    return {\n",
    "        'threshold': threshold,\n",
    "        'best_score': best_config['combined_score'],\n",
    "        'min_cluster_size': best_config['min_cluster_size'],\n",
    "        'min_samples': best_config['min_samples'],\n",
    "        'alpha': alpha\n",
    "    }\n",
    "\n",
    "# Run evaluation in parallel for all thresholds\n",
    "results = Parallel(n_jobs=-1, verbose=10)(delayed(evaluate_threshold)(threshold) for threshold in thresholds)\n",
    "results_df = pd.DataFrame(results)\n",
    "\n",
    "from scipy.signal import find_peaks\n",
    "\n",
    "# Apply Moving Average to smooth the curve\n",
    "results_df['smoothed_best_score'] = results_df['best_score'].rolling(window=3, center=True).mean()\n",
    "\n",
    "# Find local maxima in the smoothed curve\n",
    "peaks, _ = find_peaks(results_df['smoothed_best_score'])\n",
    "\n",
    "# Get the best peak\n",
    "if len(peaks) > 0:\n",
    "    optimal_index = peaks[np.argmax(results_df.loc[peaks, 'smoothed_best_score'])]\n",
    "    optimal_threshold = results_df.loc[optimal_index, 'threshold']\n",
    "else:\n",
    "    # If no peaks found, fall back to the maximum score\n",
    "    optimal_threshold = results_df.loc[results_df['smoothed_best_score'].idxmax(), 'threshold']\n",
    "\n",
    "# Display Results\n",
    "optimal_result = results_df.loc[results_df['threshold'] == optimal_threshold].iloc[0]\n",
    "print(f\"Optimal Threshold Found: {optimal_threshold:.4e}\")\n",
    "print(\"Optimal Configuration:\")\n",
    "print(optimal_result)\n",
    "\n",
    "# Print the alpha for the best configuration\n",
    "print(f\"\\nAlpha for Best Configuration: {optimal_result['alpha']:.4f}\")\n",
    "\n",
    "# Plotting the best score vs threshold\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(results_df['threshold'], results_df['best_score'], label='Best Combined Score')\n",
    "plt.plot(results_df['threshold'], results_df['smoothed_best_score'], label='Smoothed Score', linestyle='--')\n",
    "plt.axvline(optimal_threshold, color='red', linestyle='--', label=f'Optimal Threshold = {optimal_threshold:.4e}')\n",
    "plt.title('Threshold vs Best Combined Score')\n",
    "plt.xlabel('Threshold Value')\n",
    "plt.ylabel('Best Combined Score')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from folium import plugins\n",
    "import hdbscan\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import cm\n",
    "import numpy as np\n",
    "from branca.colormap import LinearColormap\n",
    "\n",
    "min_cluster_size= int(best_config['min_cluster_size'])\n",
    "min_samples= int(best_config['min_samples'])\n",
    "\n",
    "# Ensure the required columns are present\n",
    "if 'Lat_Grid' not in grid_gdf_adaptive.columns or 'Log_Grid' not in grid_gdf_adaptive.columns:\n",
    "    grid_gdf_adaptive['Lat_Grid'] = adaptive_df['Lat_Grid']\n",
    "    grid_gdf_adaptive['Log_Grid'] = adaptive_df['Log_Grid']\n",
    "\n",
    "# Ensure 'Difference' column is calculated\n",
    "if 'Difference' not in grid_gdf_adaptive.columns:\n",
    "    grid_gdf_adaptive['Difference'] = grid_gdf_adaptive['Normalized_Visual_Adaptive'] - grid_gdf_baseline['Normalized_Visual_Baseline']\n",
    "\n",
    "# Step 1: Filter the DataFrame to remove areas with negligible improvements\n",
    "filtered_data = grid_gdf_adaptive[grid_gdf_adaptive['Difference'] > optimal_threshold]\n",
    "\n",
    "# Select relevant columns for clustering\n",
    "spatial_data = filtered_data[['Lat_Grid', 'Log_Grid', 'Difference']].dropna()\n",
    "\n",
    "# Apply HDBSCAN\n",
    "hdbscan_clusterer = hdbscan.HDBSCAN(min_cluster_size=min_cluster_size, min_samples=min_samples, gen_min_span_tree=True)\n",
    "spatial_data = spatial_data.copy()  # Create a copy to allow modification\n",
    "spatial_data['HDBSCAN_Cluster_Label'] = hdbscan_clusterer.fit_predict(spatial_data[['Lat_Grid', 'Log_Grid']])\n",
    "\n",
    "# Normalize cluster labels for visualization (to fit in a colormap range)\n",
    "labels = spatial_data['HDBSCAN_Cluster_Label'].values\n",
    "unique_labels = np.unique(labels)\n",
    "num_unique_labels = len(unique_labels)\n",
    "\n",
    "# Generate colors using the 'jet' colormap\n",
    "colormap = plt.get_cmap('jet', num_unique_labels)\n",
    "colors = {label: colormap(i / num_unique_labels) for i, label in enumerate(unique_labels)}\n",
    "\n",
    "# Convert colors to hex format for Folium\n",
    "colors_hex = {label: f'#{int(r*255):02x}{int(g*255):02x}{int(b*255):02x}' \n",
    "              for label, (r, g, b, _) in colors.items()}\n",
    "\n",
    "# Create a Folium map\n",
    "map_clusters = folium.Map(location=[spatial_data['Lat_Grid'].mean(), spatial_data['Log_Grid'].mean()], \n",
    "                          zoom_start=13, tiles='Cartodb dark_matter')\n",
    "\n",
    "# Plot each point in the spatial data with its cluster label\n",
    "for _, row in spatial_data.iterrows():\n",
    "    label = row['HDBSCAN_Cluster_Label']\n",
    "    color = colors_hex[label] if label in colors_hex else '#000000'  # Use black for noise points (-1)\n",
    "    \n",
    "    folium.CircleMarker(\n",
    "        location=[row['Lat_Grid'], row['Log_Grid']],\n",
    "        radius=7,\n",
    "        color=color,           \n",
    "        weight=0,\n",
    "        fill=True,\n",
    "        fill_color=color,       \n",
    "        fill_opacity=0.3        \n",
    "    ).add_to(map_clusters)\n",
    "\n",
    "# Generate a LinearColormap\n",
    "cluster_colormap = LinearColormap(\n",
    "    colors=[colors_hex[label] for label in unique_labels if label != -1],\n",
    "    vmin=min(unique_labels),\n",
    "    vmax=max(unique_labels),\n",
    "    caption=\"Cluster Labels\"\n",
    ")\n",
    "\n",
    "# Add the colormap to the map\n",
    "cluster_colormap.add_to(map_clusters)\n",
    "\n",
    "# Display the map\n",
    "map_clusters\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import silhouette_score\n",
    "import hdbscan\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from joblib import Parallel, delayed\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "\n",
    "# Step 1: Prepare Data for Clustering\n",
    "spatial_data = filtered_data[['Lat_Grid', 'Log_Grid', 'Difference']].dropna()\n",
    "\n",
    "# Step 2: Normalize Data for Better Performance of HDBSCAN\n",
    "scaler = StandardScaler()\n",
    "scaled_data = scaler.fit_transform(spatial_data[['Lat_Grid', 'Log_Grid', 'Difference']])\n",
    "\n",
    "# Step 3: Define the function to evaluate each HDBSCAN model\n",
    "def evaluate_hdbscan(min_cluster_size, min_sample):\n",
    "    # Fit HDBSCAN Model\n",
    "    hdbscan_clusterer = hdbscan.HDBSCAN(\n",
    "        min_cluster_size=min_cluster_size, \n",
    "        min_samples=min_sample, \n",
    "        gen_min_span_tree=True\n",
    "    )\n",
    "    labels = hdbscan_clusterer.fit_predict(scaled_data)\n",
    "    \n",
    "    # Filter out noise points (-1) for silhouette score calculation\n",
    "    non_noise_data = scaled_data[labels != -1]\n",
    "    non_noise_labels = labels[labels != -1]\n",
    "    \n",
    "    if len(np.unique(non_noise_labels)) > 1:  # Check if we have more than one cluster\n",
    "        # Compute Silhouette Score\n",
    "        silhouette_avg = silhouette_score(non_noise_data, non_noise_labels)\n",
    "    else:\n",
    "        silhouette_avg = np.nan  # Not enough clusters to calculate silhouette score\n",
    "    \n",
    "    # Calculate Cluster Stability Score (HDBSCAN metric)\n",
    "    stability = hdbscan_clusterer.relative_validity_\n",
    "    \n",
    "    # Return the results as a dictionary\n",
    "    return {\n",
    "        'min_cluster_size': min_cluster_size,\n",
    "        'min_samples': min_sample,\n",
    "        'silhouette_score': silhouette_avg,\n",
    "        'stability_score': stability,\n",
    "        'num_clusters': len(np.unique(non_noise_labels))\n",
    "    }\n",
    "\n",
    "# Step 4: Generate Parameter Grid\n",
    "# min_cluster_sizes = np.arange(3, 15)  \n",
    "# min_samples = np.arange(1, 41)       \n",
    "min_cluster_sizes = np.arange(2, 71)\n",
    "min_samples = np.arange(1, 61)\n",
    "\n",
    "# Step 5: Parallelize the computation across all processors\n",
    "results = Parallel(n_jobs=-1, verbose=10)(delayed(evaluate_hdbscan)(size, sample) \n",
    "                                          for size in min_cluster_sizes for sample in min_samples)\n",
    "\n",
    "\n",
    "# Step 6: Display Results in a DataFrame\n",
    "stability_df = pd.DataFrame(results)\n",
    "\n",
    "# Step 1: Normalize the scores to [0, 1] range\n",
    "scaler = MinMaxScaler()\n",
    "normalized_scores = scaler.fit_transform(stability_df[['stability_score', 'silhouette_score']].dropna())\n",
    "\n",
    "# Step 2: Apply PCA\n",
    "pca = PCA(n_components=2)\n",
    "pca.fit(normalized_scores)\n",
    "principal_components = pca.components_\n",
    "\n",
    "# Step 3: Extract the direction of the first principal component (using absolute values)\n",
    "component_0 = np.abs(principal_components[0, 0])\n",
    "component_1 = np.abs(principal_components[0, 1])\n",
    "\n",
    "alpha = component_0 / (component_0 + component_1)\n",
    "print(f\"Optimal Alpha (Using PCA Method): {alpha:.4f}\")\n",
    "\n",
    "# Step 4: Compute Combined Score\n",
    "stability_df['combined_score'] = (\n",
    "    alpha * stability_df['stability_score'] + (1 - alpha) * stability_df['silhouette_score']\n",
    ")\n",
    "\n",
    "# Step 5: Find the best configuration\n",
    "best_index = stability_df['combined_score'].idxmax()\n",
    "best_config = stability_df.loc[best_index]\n",
    "\n",
    "print(\"\\nBest Configuration Based on Combined Score:\")\n",
    "print(best_config)\n",
    "\n",
    "# Step 6: Visualize Pareto Front\n",
    "plt.figure(figsize=(14, 6))\n",
    "plt.scatter(\n",
    "    stability_df['silhouette_score'],\n",
    "    stability_df['stability_score'],\n",
    "    c=stability_df['combined_score'],\n",
    "    cmap='viridis',\n",
    "    s=stability_df['min_cluster_size'] * 10,\n",
    ")\n",
    "\n",
    "# Highlight the best configuration\n",
    "plt.scatter(\n",
    "    best_config['silhouette_score'],\n",
    "    best_config['stability_score'],\n",
    "    color='red',\n",
    "    label='Optimal Configuration',\n",
    "    s=100,\n",
    "    edgecolor='black'\n",
    ")\n",
    "\n",
    "plt.colorbar(label='Combined Score')\n",
    "plt.title('Pareto Front: Stability Score vs Silhouette Score')\n",
    "plt.xlabel('Silhouette Score')\n",
    "plt.ylabel('Stability Score')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add a combined score column\n",
    "stability_df['combined_score'] = (\n",
    "    alpha * stability_df['stability_score'] + (1 - alpha) * stability_df['silhouette_score']\n",
    ")\n",
    "\n",
    "# Filter out entries with NaN silhouette scores and stability scores\n",
    "valid_entries = stability_df.dropna(subset=['combined_score'])\n",
    "\n",
    "# Sort by combined score\n",
    "sorted_df = valid_entries.sort_values(by='combined_score', ascending=False)\n",
    "\n",
    "# Select the best configuration (the top row of the sorted DataFrame)\n",
    "best_configuration = sorted_df.iloc[0]\n",
    "\n",
    "# Print the best configuration\n",
    "print(\"Best HDBSCAN Configuration Based on Combined Score:\")\n",
    "print(best_configuration)\n",
    "\n",
    "# Plotting the scatter plot with min_samples effect\n",
    "plt.figure(figsize=(14, 6))\n",
    "\n",
    "# Create scatter plot\n",
    "scatter = plt.scatter(\n",
    "    stability_df['silhouette_score'],\n",
    "    stability_df['stability_score'],\n",
    "    c=stability_df['min_cluster_size'],          # Color based on min_cluster_size\n",
    "    cmap='viridis',\n",
    "    s=(stability_df['min_samples'] + 1) * 5, # Size based on min_samples (larger size -> larger min_samples)\n",
    "    alpha=0.7                                # Make the plot semi-transparent for better visibility\n",
    ")\n",
    "\n",
    "# Adding the colorbar\n",
    "plt.colorbar(scatter, label='Min Number of Clusters')\n",
    "\n",
    "# Adding titles and labels\n",
    "plt.title('Silhouette Score vs Stability Score (Point Size Reflects min_samples)')\n",
    "plt.xlabel('Silhouette Score')\n",
    "plt.ylabel('Stability Score')\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import kruskal, spearmanr, mannwhitneyu\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import PCA\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import cm\n",
    "import hdbscan\n",
    "\n",
    "# Step 1: Apply HDBSCAN to the filtered data\n",
    "hdbscan_clusterer = hdbscan.HDBSCAN(min_cluster_size=5, min_samples=1)\n",
    "filtered_data['HDBSCAN_Cluster_Label'] = hdbscan_clusterer.fit_predict(filtered_data[['Lat_Grid', 'Log_Grid']])\n",
    "\n",
    "# Filter out noise points (label -1)\n",
    "filtered_data_without_noise = filtered_data[filtered_data['HDBSCAN_Cluster_Label'] != -1]\n",
    "\n",
    "# Merge filtered data with df_temp to get the necessary features\n",
    "merged_data = pd.merge(filtered_data_without_noise, df_temp, on=['Lat_Grid', 'Log_Grid'], how='inner')\n",
    "\n",
    "# Extract relevant features for clustering analysis\n",
    "features_of_interest = ['SOC_batt', 'solar_current', 'current_batt', 'Difference', 'HDBSCAN_Cluster_Label']\n",
    "filtered_data_with_features = merged_data[features_of_interest].dropna()\n",
    "\n",
    "# Step 2: Kruskal-Wallis Test\n",
    "cluster_labels = filtered_data_with_features['HDBSCAN_Cluster_Label'].unique()\n",
    "clustered_groups = [filtered_data_with_features[filtered_data_with_features['HDBSCAN_Cluster_Label'] == label]['SOC_batt'] for label in cluster_labels]\n",
    "\n",
    "kruskal_stat, kruskal_p = kruskal(*clustered_groups)\n",
    "print(f\"Kruskal-Wallis Test: H = {kruskal_stat:.4f}, p-value = {kruskal_p:.4e}\")\n",
    "\n",
    "# Step 3: Visualize Data Distribution\n",
    "plt.figure(figsize=(12, 6))\n",
    "sns.boxplot(x='HDBSCAN_Cluster_Label', y='SOC_batt', data=filtered_data_with_features)\n",
    "plt.title('SOC_batt Distribution Across Clusters')\n",
    "plt.xlabel('HDBSCAN Cluster Label')\n",
    "plt.ylabel('SOC_batt')\n",
    "plt.show()\n",
    "\n",
    "# Step 4: Correlation Analysis (Spearman's Rank Correlation)\n",
    "correlations = {}\n",
    "for feature in features_of_interest[:-1]:  # Skip the last column (HDBSCAN_Cluster_Label)\n",
    "    corr, p_value = spearmanr(filtered_data_with_features[feature], filtered_data_with_features['HDBSCAN_Cluster_Label'])\n",
    "    correlations[feature] = (corr, p_value)\n",
    "\n",
    "correlations_df = pd.DataFrame(correlations, index=['Correlation Coefficient', 'p-value']).T\n",
    "print(\"Correlation Analysis with Cluster Labels:\\n\", correlations_df)\n",
    "\n",
    "# Step 5: Pairwise Comparison (Mann-Whitney U Test)\n",
    "pairwise_results = []\n",
    "for i in range(len(cluster_labels)):\n",
    "    for j in range(i + 1, len(cluster_labels)):\n",
    "        cluster_i = filtered_data_with_features[filtered_data_with_features['HDBSCAN_Cluster_Label'] == cluster_labels[i]]['SOC_batt'].astype(float).dropna().values\n",
    "        cluster_j = filtered_data_with_features[filtered_data_with_features['HDBSCAN_Cluster_Label'] == cluster_labels[j]]['SOC_batt'].astype(float).dropna().values\n",
    "        \n",
    "        # Only perform the test if both clusters have at least two observations\n",
    "        if len(cluster_i) > 1 and len(cluster_j) > 1:\n",
    "            u_stat, mw_pvalue = mannwhitneyu(cluster_i, cluster_j)\n",
    "            pairwise_results.append({\n",
    "                'Cluster 1': cluster_labels[i],\n",
    "                'Cluster 2': cluster_labels[j],\n",
    "                'U-statistic': u_stat,\n",
    "                'p-value': mw_pvalue\n",
    "            })\n",
    "\n",
    "pairwise_results_df = pd.DataFrame(pairwise_results)\n",
    "print(\"Pairwise Comparison Results (Mann-Whitney U Test):\\n\", pairwise_results_df)\n",
    "\n",
    "# Step 6: Multivariate Analysis (PCA) - Excluding Noise Points\n",
    "feature_matrix = filtered_data_with_features[['SOC_batt', 'solar_current', 'current_batt', 'Difference']].values\n",
    "scaler = StandardScaler()\n",
    "scaled_features = scaler.fit_transform(feature_matrix)\n",
    "pca = PCA(n_components=3)\n",
    "pca_results = pca.fit_transform(scaled_features)\n",
    "\n",
    "# Plot PCA results - 3D\n",
    "fig = plt.figure(figsize=(10, 8))\n",
    "ax = fig.add_subplot(111, projection='3d')\n",
    "scatter = ax.scatter(pca_results[:, 0], pca_results[:, 1], pca_results[:, 2], \n",
    "                     c=filtered_data_with_features['HDBSCAN_Cluster_Label'], cmap='jet', s=10)\n",
    "ax.set_title('3D PCA of Environmental and Energy Features (Noise Points Excluded)')\n",
    "ax.set_xlabel(f'PC1 ({pca.explained_variance_ratio_[0]*100:.2f}% Variance)')\n",
    "ax.set_ylabel(f'PC2 ({pca.explained_variance_ratio_[1]*100:.2f}% Variance)')\n",
    "ax.set_zlabel(f'PC3 ({pca.explained_variance_ratio_[2]*100:.2f}% Variance)')\n",
    "fig.colorbar(scatter, ax=ax, label='HDBSCAN Cluster Label')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import PCA\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import cm\n",
    "\n",
    "# Step 1: Prepare the Data\n",
    "pca_features = filtered_data_with_features[['SOC_batt', 'solar_current', 'current_batt', 'Difference']].astype(float).dropna()\n",
    "pca_labels = filtered_data_with_features.loc[pca_features.index, 'HDBSCAN_Cluster_Label']\n",
    "\n",
    "# Step 2: Standardize the Features\n",
    "scaler = StandardScaler()\n",
    "scaled_features = scaler.fit_transform(pca_features)\n",
    "\n",
    "# Step 3: Apply PCA\n",
    "pca = PCA(n_components=2)\n",
    "pca_results = pca.fit_transform(scaled_features)\n",
    "\n",
    "# Step 4: Create a DataFrame for Visualization\n",
    "pca_df = pd.DataFrame(data=pca_results, columns=['Principal Component 1', 'Principal Component 2'])\n",
    "pca_df['Cluster'] = pca_labels.values\n",
    "\n",
    "# Step 5: Plot PCA Results\n",
    "plt.figure(figsize=(12, 6))\n",
    "scatter = plt.scatter(pca_df['Principal Component 1'], pca_df['Principal Component 2'],\n",
    "                      c=pca_df['Cluster'], cmap=cm.jet, s=10)\n",
    "plt.colorbar(scatter, label='Cluster Label')\n",
    "plt.title('PCA Visualization of Clusters (Jet Colormap)')\n",
    "plt.xlabel(f'Principal Component 1 ({pca.explained_variance_ratio_[0]*100:.2f}% Variance)')\n",
    "plt.ylabel(f'Principal Component 2 ({pca.explained_variance_ratio_[1]*100:.2f}% Variance)')\n",
    "plt.grid(True)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import PCA\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import cm\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "\n",
    "# Step 1: Prepare the Data\n",
    "pca_features = filtered_data_with_features[['SOC_batt', 'Hour', 'current_batt', 'Difference']].astype(float).dropna()\n",
    "pca_labels = filtered_data_with_features.loc[pca_features.index, 'HDBSCAN_Cluster_Label']\n",
    "\n",
    "# Step 2: Standardize the Features\n",
    "scaler = StandardScaler()\n",
    "scaled_features = scaler.fit_transform(pca_features)\n",
    "\n",
    "# Step 3: Apply PCA\n",
    "pca = PCA(n_components=3)\n",
    "pca_results = pca.fit_transform(scaled_features)\n",
    "\n",
    "# Step 4: Create a DataFrame for Visualization\n",
    "pca_df = pd.DataFrame(data=pca_results, columns=['PC1', 'PC2', 'PC3'])\n",
    "pca_df['Cluster'] = pca_labels.values\n",
    "\n",
    "# Step 5: Plot PCA Results\n",
    "fig = plt.figure(figsize=(14, 8))\n",
    "ax = fig.add_subplot(111, projection='3d')\n",
    "\n",
    "# Plot the points\n",
    "scatter = ax.scatter(\n",
    "    pca_df['PC1'], pca_df['PC2'], pca_df['PC3'],\n",
    "    c=pca_df['Cluster'], cmap=cm.jet, s=10\n",
    ")\n",
    "\n",
    "# Plot labels and color bar\n",
    "ax.set_title('3D PCA Visualization of Clusters (Jet Colormap)')\n",
    "ax.set_xlabel(f'PC1 ({pca.explained_variance_ratio_[0]*100:.2f}% Variance)')\n",
    "ax.set_ylabel(f'PC2 ({pca.explained_variance_ratio_[1]*100:.2f}% Variance)')\n",
    "ax.set_zlabel(f'PC3 ({pca.explained_variance_ratio_[2]*100:.2f}% Variance)')\n",
    "fig.colorbar(scatter, ax=ax, label='Cluster Label')\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add Hour column for temporal analysis\n",
    "df_temp['Hour'] = pd.to_datetime(df_temp['Timestamp']).dt.hour\n",
    "\n",
    "# Compute the energy savings by hour for baseline and adaptive\n",
    "energy_by_hour = df_temp.groupby('Hour').agg({\n",
    "    'SOC_batt': 'sum',\n",
    "    'SOC_batt_360 sec': 'sum'\n",
    "})\n",
    "\n",
    "# Compute energy savings by hour\n",
    "energy_by_hour['Energy_Savings'] = energy_by_hour['SOC_batt_360 sec'] - energy_by_hour['SOC_batt']\n",
    "\n",
    "# Plot energy savings by hour\n",
    "plt.figure(figsize=(10, 6))\n",
    "energy_by_hour['Energy_Savings'].plot(kind='bar', color='orange')\n",
    "plt.title('Energy Savings by Hour')\n",
    "plt.xlabel('Hour of the Day')\n",
    "plt.ylabel('Energy Savings (SOC)')\n",
    "plt.grid(True)\n",
    "plt.show()\n",
    "\n",
    "# Display the energy savings by hour\n",
    "print(energy_by_hour)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix, classification_report\n",
    "from xgboost import XGBClassifier\n",
    "\n",
    "# Step 1: Remove noise points (label = -1) from the dataset\n",
    "filtered_data_with_features = filtered_data_with_features[filtered_data_with_features['HDBSCAN_Cluster_Label'] != -1]\n",
    "\n",
    "# Step 2: Convert relevant columns to numeric\n",
    "columns_to_convert = ['SOC_batt', 'solar_current', 'current_batt']\n",
    "filtered_data_with_features[columns_to_convert] = filtered_data_with_features[columns_to_convert].apply(pd.to_numeric, errors='coerce')\n",
    "\n",
    "# Step 3: Prepare data for classification\n",
    "X = filtered_data_with_features.drop(['HDBSCAN_Cluster_Label'], axis=1)\n",
    "y = filtered_data_with_features['HDBSCAN_Cluster_Label']\n",
    "\n",
    "# Step 4: Train-test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Step 5: Train XGBoost Classifier\n",
    "xgb_model = XGBClassifier(use_label_encoder=False, eval_metric='mlogloss')\n",
    "xgb_model.fit(X_train, y_train)\n",
    "\n",
    "# Step 6: Model Evaluation\n",
    "y_pred = xgb_model.predict(X_test)\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(f\"Accuracy: {accuracy * 100:.2f}%\")\n",
    "\n",
    "# Step 7: Classification Report\n",
    "class_report = classification_report(y_test, y_pred)\n",
    "print(\"Classification Report:\\n\", class_report)\n",
    "\n",
    "# Step 8: Confusion Matrix\n",
    "conf_matrix = confusion_matrix(y_test, y_pred)\n",
    "plt.figure(figsize=(10, 7))\n",
    "sns.heatmap(conf_matrix, annot=False, fmt='d', cmap='Blues')\n",
    "plt.title('Confusion Matrix')\n",
    "plt.xlabel('Predicted Label')\n",
    "plt.ylabel('True Label')\n",
    "plt.show()\n",
    "\n",
    "# Step 9: Feature Importance Analysis\n",
    "feature_importances = xgb_model.feature_importances_\n",
    "importance_df = pd.DataFrame({\n",
    "    'Feature': X.columns,\n",
    "    'Importance': feature_importances\n",
    "}).sort_values(by='Importance', ascending=False)\n",
    "\n",
    "# Display Feature Importances\n",
    "print(importance_df)\n",
    "\n",
    "# Step 10: Plot Feature Importances\n",
    "plt.figure(figsize=(12, 6))\n",
    "sns.barplot(x='Importance', y='Feature', data=importance_df)\n",
    "plt.title('Feature Importance Analysis (XGBoost)')\n",
    "plt.xlabel('Importance')\n",
    "plt.ylabel('Feature')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "# Step 1: Convert the aggregated label to a new label index\n",
    "y_train_agg = np.array(y_train.copy())\n",
    "y_test_agg = np.array(y_test.copy())\n",
    "\n",
    "# Assign a new label index for small clusters\n",
    "new_label_index = max(set(y_train)) + 1  # This ensures we don't overlap with existing labels\n",
    "y_train_agg = np.where(np.isin(y_train_agg, small_clusters), new_label_index, y_train_agg)\n",
    "y_test_agg = np.where(np.isin(y_test_agg, small_clusters), new_label_index, y_test_agg)\n",
    "\n",
    "# Step 2: Apply Label Encoding to make labels consecutive integers\n",
    "label_encoder = LabelEncoder()\n",
    "y_train_encoded = label_encoder.fit_transform(y_train_agg)\n",
    "y_test_encoded = label_encoder.transform(y_test_agg)\n",
    "\n",
    "# Step 3: Train the XGBoost Classifier again with encoded labels\n",
    "xgb_model_agg = XGBClassifier(use_label_encoder=False, eval_metric='mlogloss')\n",
    "xgb_model_agg.fit(X_train, y_train_encoded)\n",
    "\n",
    "# Step 4: Make predictions with the aggregated model\n",
    "y_pred_encoded = xgb_model_agg.predict(X_test)\n",
    "\n",
    "# Step 5: Decode predictions to original labels\n",
    "y_pred_agg = label_encoder.inverse_transform(y_pred_encoded)\n",
    "\n",
    "# Step 6: Compute the confusion matrix\n",
    "conf_matrix_agg = confusion_matrix(y_test_agg, y_pred_agg)\n",
    "\n",
    "# Step 7: Plot the simplified confusion matrix\n",
    "plt.figure(figsize=(12, 10))\n",
    "sns.heatmap(conf_matrix_agg, annot=False, fmt='d', cmap='Blues')\n",
    "plt.title('Simplified Confusion Matrix (Aggregated Clusters)')\n",
    "plt.xlabel('Predicted Label')\n",
    "plt.ylabel('True Label')\n",
    "plt.show()\n",
    "\n",
    "# Step 8: Visualize Predictions vs Original Clusters\n",
    "\n",
    "# Check if PCA components have proper names\n",
    "if isinstance(pca_df, pd.DataFrame):\n",
    "    pca_df.columns = ['Principal Component 1', 'Principal Component 2', 'Cluster']\n",
    "\n",
    "# Create a new DataFrame containing the test set's indices and predictions\n",
    "predictions_df = pd.DataFrame({\n",
    "    'Predicted_Cluster': y_pred_agg,\n",
    "}, index=X_test.index)\n",
    "\n",
    "# Merge predictions with the original PCA DataFrame\n",
    "pca_df_with_predictions = pca_df.copy()\n",
    "pca_df_with_predictions['Predicted_Cluster'] = predictions_df.reindex(pca_df_with_predictions.index).fillna(-1)['Predicted_Cluster']\n",
    "\n",
    "# Plotting original clusters\n",
    "plt.figure(figsize=(15, 6))\n",
    "\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.scatter(pca_df_with_predictions['Principal Component 1'], pca_df_with_predictions['Principal Component 2'], \n",
    "            c=pca_df_with_predictions['Cluster'], cmap='jet', s=10)\n",
    "plt.title('Original Clusters (PCA Visualization)')\n",
    "plt.xlabel('Principal Component 1')\n",
    "plt.ylabel('Principal Component 2')\n",
    "plt.colorbar(label='Cluster Label')\n",
    "\n",
    "# Plotting predicted clusters\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.scatter(pca_df_with_predictions['Principal Component 1'], pca_df_with_predictions['Principal Component 2'], \n",
    "            c=pca_df_with_predictions['Predicted_Cluster'], cmap='jet', s=10)\n",
    "plt.title('Predicted Clusters (PCA Visualization)')\n",
    "plt.xlabel('Principal Component 1')\n",
    "plt.ylabel('Principal Component 2')\n",
    "plt.colorbar(label='Predicted Cluster Label')\n",
    "\n",
    "plt.show()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import AgglomerativeClustering\n",
    "from scipy.cluster.hierarchy import dendrogram, linkage\n",
    "\n",
    "# Step 11: Hierarchical Clustering for Cluster Aggregation\n",
    "# Prepare the dataset for clustering\n",
    "unique_clusters = np.unique(y)\n",
    "cluster_centroids = X.groupby(y).mean()  # Compute the centroids of each cluster\n",
    "\n",
    "# Perform hierarchical clustering on the cluster centroids\n",
    "linked = linkage(cluster_centroids, method='ward')\n",
    "\n",
    "# Plot the dendrogram to visualize potential clusters to merge\n",
    "plt.figure(figsize=(15, 7))\n",
    "dendrogram(linked, labels=unique_clusters, leaf_rotation=90)\n",
    "plt.title('Dendrogram for Cluster Aggregation')\n",
    "plt.xlabel('Cluster Label')\n",
    "plt.ylabel('Distance')\n",
    "plt.show()\n",
    "\n",
    "# Agglomerative Clustering (Merging similar clusters)\n",
    "agg_clusterer = AgglomerativeClustering(n_clusters=30)  # Reduce to 30 clusters\n",
    "y_aggregated = agg_clusterer.fit_predict(cluster_centroids)\n",
    "\n",
    "# Mapping original labels to aggregated labels\n",
    "label_mapping = {original: aggregated for original, aggregated in zip(unique_clusters, y_aggregated)}\n",
    "y_aggregated_final = y.map(label_mapping)\n",
    "\n",
    "# Step 12: Re-train the XGBoost Classifier with aggregated labels\n",
    "X_train_agg, X_test_agg, y_train_agg, y_test_agg = train_test_split(X, y_aggregated_final, test_size=0.2, random_state=42)\n",
    "xgb_model_agg = XGBClassifier(use_label_encoder=False, eval_metric='mlogloss')\n",
    "xgb_model_agg.fit(X_train_agg, y_train_agg)\n",
    "\n",
    "# Step 13: Evaluate the New Model\n",
    "y_pred_agg = xgb_model_agg.predict(X_test_agg)\n",
    "accuracy_agg = accuracy_score(y_test_agg, y_pred_agg)\n",
    "print(f\"Aggregated Model Accuracy: {accuracy_agg * 100:.2f}%\")\n",
    "\n",
    "# Classification Report for Aggregated Clusters\n",
    "class_report_agg = classification_report(y_test_agg, y_pred_agg)\n",
    "print(\"Aggregated Classification Report:\\n\", class_report_agg)\n",
    "\n",
    "# Confusion Matrix for Aggregated Clusters\n",
    "conf_matrix_agg = confusion_matrix(y_test_agg, y_pred_agg)\n",
    "plt.figure(figsize=(10, 7))\n",
    "sns.heatmap(conf_matrix_agg, annot=True, fmt='d', cmap='Blues')\n",
    "plt.title('Confusion Matrix (Aggregated Clusters)')\n",
    "plt.xlabel('Predicted Label')\n",
    "plt.ylabel('True Label')\n",
    "plt.show()\n",
    "\n",
    "# Step 14: Feature Importance Analysis for Aggregated Model\n",
    "feature_importances_agg = xgb_model_agg.feature_importances_\n",
    "importance_df_agg = pd.DataFrame({\n",
    "    'Feature': X.columns,\n",
    "    'Importance': feature_importances_agg\n",
    "}).sort_values(by='Importance', ascending=False)\n",
    "\n",
    "# Display Feature Importances for Aggregated Model\n",
    "print(importance_df_agg)\n",
    "\n",
    "# Plot Feature Importances for Aggregated Model\n",
    "plt.figure(figsize=(12, 6))\n",
    "sns.barplot(x='Importance', y='Feature', data=importance_df_agg)\n",
    "plt.title('Feature Importance Analysis (Aggregated Model)')\n",
    "plt.xlabel('Importance')\n",
    "plt.ylabel('Feature')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix, classification_report\n",
    "from xgboost import XGBClassifier\n",
    "from imblearn.over_sampling import SMOTE\n",
    "import optuna\n",
    "from optuna.integration import XGBoostPruningCallback\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Step 1: Remove noise points (label = -1) from the dataset\n",
    "filtered_data_with_features = filtered_data_with_features[filtered_data_with_features['HDBSCAN_Cluster_Label'] != -1]\n",
    "\n",
    "# Step 2: Convert relevant columns to numeric\n",
    "columns_to_convert = ['SOC_batt', 'solar_current', 'current_batt']\n",
    "filtered_data_with_features[columns_to_convert] = filtered_data_with_features[columns_to_convert].apply(pd.to_numeric, errors='coerce')\n",
    "\n",
    "# Step 3: Prepare data for classification\n",
    "X = filtered_data_with_features.drop(['HDBSCAN_Cluster_Label'], axis=1)\n",
    "y = filtered_data_with_features['HDBSCAN_Cluster_Label']\n",
    "\n",
    "# Step 4: Scale Features\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "\n",
    "from imblearn.over_sampling import SMOTE, ADASYN\n",
    "\n",
    "# Step 5: Apply SMOTE for Class Balancing (k_neighbors adjusted to handle small clusters)\n",
    "smote = SMOTE(random_state=42, k_neighbors=3)  # Reduced k_neighbors to avoid error\n",
    "X_resampled, y_resampled = smote.fit_resample(X_scaled, y)\n",
    "\n",
    "# If the error persists, try using ADASYN instead of SMOTE\n",
    "# adasyn = ADASYN(random_state=42)\n",
    "# X_resampled, y_resampled = adasyn.fit_resample(X_scaled, y)\n",
    "\n",
    "# Step 6: Train-test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_resampled, y_resampled, test_size=0.2, random_state=42)\n",
    "\n",
    "# Step 7: Define Objective Function for Optuna\n",
    "def objective(trial):\n",
    "    params = {\n",
    "        'n_estimators': trial.suggest_int('n_estimators', 100, 500),\n",
    "        'max_depth': trial.suggest_int('max_depth', 4, 12),\n",
    "        'learning_rate': trial.suggest_float('learning_rate', 0.01, 0.2),\n",
    "        'colsample_bytree': trial.suggest_float('colsample_bytree', 0.5, 1.0),\n",
    "        'subsample': trial.suggest_float('subsample', 0.5, 1.0),\n",
    "        'gamma': trial.suggest_float('gamma', 0.0, 1.0),\n",
    "        'reg_alpha': trial.suggest_float('reg_alpha', 0.0, 1.0),\n",
    "        'reg_lambda': trial.suggest_float('reg_lambda', 0.0, 1.0),\n",
    "    }\n",
    "\n",
    "    # model = XGBClassifier(**params, use_label_encoder=False, eval_metric='mlogloss')\n",
    "    model = XGBClassifier(**best_params,\n",
    "    device='cuda',\n",
    "    use_label_encoder=False,\n",
    "    eval_metric='mlogloss',\n",
    "    tree_method='gpu_hist',\n",
    "    gpu_id=0,\n",
    "    n_jobs=-1  # Allow parallel processing on multiple GPUs if configured properly\n",
    "    )\n",
    "    model.fit(X_train, y_train)\n",
    "\n",
    "    y_pred = model.predict(X_test)\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    return accuracy\n",
    "\n",
    "# Step 8: Optuna Hyperparameter Tuning\n",
    "study = optuna.create_study(direction='maximize')\n",
    "study.optimize(objective, n_trials=50)\n",
    "\n",
    "# Step 9: Train Optimized XGBoost Model\n",
    "best_params = study.best_params\n",
    "# xgb_model = XGBClassifier(**best_params, use_label_encoder=False, eval_metric='mlogloss')\n",
    "xgb_model = XGBClassifier(**best_params,\n",
    "    device='cuda',\n",
    "    use_label_encoder=False,\n",
    "    eval_metric='mlogloss',\n",
    "    tree_method='gpu_hist',\n",
    "    gpu_id=0,\n",
    "    n_jobs=-1  # Allow parallel processing on multiple GPUs if configured properly\n",
    ")\n",
    "xgb_model.fit(X_train, y_train)\n",
    "\n",
    "# Step 10: Model Evaluation\n",
    "y_pred = xgb_model.predict(X_test)\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(f\"Optimized Model Accuracy: {accuracy * 100:.2f}%\")\n",
    "\n",
    "# Step 11: Classification Report\n",
    "class_report = classification_report(y_test, y_pred)\n",
    "print(\"Optimized Classification Report:\\n\", class_report)\n",
    "\n",
    "# Step 12: Confusion Matrix\n",
    "conf_matrix = confusion_matrix(y_test, y_pred)\n",
    "plt.figure(figsize=(12, 8))\n",
    "sns.heatmap(conf_matrix, annot=True, fmt='d', cmap='Blues')\n",
    "plt.title('Optimized Confusion Matrix (Aggregated Clusters)')\n",
    "plt.xlabel('Predicted Label')\n",
    "plt.ylabel('True Label')\n",
    "plt.show()\n",
    "\n",
    "# Step 13: Feature Importance Analysis\n",
    "feature_importances = xgb_model.feature_importances_\n",
    "importance_df = pd.DataFrame({\n",
    "    'Feature': X.columns,\n",
    "    'Importance': feature_importances\n",
    "}).sort_values(by='Importance', ascending=False)\n",
    "\n",
    "# Display Feature Importances\n",
    "print(importance_df)\n",
    "\n",
    "# Step 14: Plot Feature Importances\n",
    "plt.figure(figsize=(12, 6))\n",
    "sns.barplot(x='Importance', y='Feature', data=importance_df)\n",
    "plt.title('Optimized Feature Importance Analysis (XGBoost)')\n",
    "plt.xlabel('Importance')\n",
    "plt.ylabel('Feature')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 12: Confusion Matrix\n",
    "plt.figure(figsize=(12, 8))\n",
    "sns.heatmap(conf_matrix, annot=False, fmt='d', cmap='Reds')\n",
    "plt.title('Optimized Confusion Matrix (Aggregated Clusters)')\n",
    "plt.xlabel('Predicted Label')\n",
    "plt.ylabel('True Label')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Markov-Based Spatial Transition Modeling for Vehicle Movement and Sensor Depletion Analysis**\n",
    "\n",
    "This methodology enables **data-driven mobility prediction** and **battery depletion analysis** using **Markovian dynamics**.\n",
    "\n",
    "## **1. Assigning Vehicles to Grid Cells Using Polygon Containment**\n",
    "Given a dataset of GPS points (`Lat`, `Log`), we spatially bin the data into a **120m x 120m** grid using polygon containment:\n",
    "\n",
    "- Convert each GPS point into a **GeoDataFrame** (`df_gdf`).\n",
    "- Each point is assigned to its corresponding grid cell using the spatial containment function:\n",
    "\n",
    "  $$\n",
    "  G(i) = \\arg\\max_j \\mathbb{1}(p_i \\in P_j)\n",
    "  $$\n",
    "\n",
    "  where:\n",
    "  - $ p_i $ is the point geometry of record $ i $,\n",
    "  - $ P_j $ is the polygon geometry of grid cell $ j $,\n",
    "  - $ \\mathbb{1}(p_i \\in P_j) $ is an indicator function that is **1** if $ p_i $ is inside $ P_j $, else **0**.\n",
    "\n",
    "- Any points that **do not match** a grid cell are treated as **outliers** and removed.\n",
    "\n",
    "## **2. Temporal Sorting for Transition Analysis**\n",
    "To analyze transitions between grid cells, the dataset is **sorted** by:\n",
    "\n",
    "$$\n",
    "\\text{Sort}(df, \\text{by}=[\\text{deviceID}, \\text{Timestamp}])\n",
    "$$\n",
    "\n",
    "where:\n",
    "- `deviceID` ensures sorting is done **per vehicle**,\n",
    "- `Timestamp` orders data points **chronologically**.\n",
    "\n",
    "## **3. Identifying Sensor Depletion Events**\n",
    "A **battery depletion threshold** is defined as:\n",
    "\n",
    "$$\n",
    "SOC_{\\text{batt}} < 50\\%\n",
    "$$\n",
    "\n",
    "where **State of Charge (SOC)** is the batteryâ€™s remaining capacity. We create a binary indicator:\n",
    "\n",
    "$$\n",
    "D_i = \n",
    "\\begin{cases} \n",
    "1, & SOC_{\\text{batt}, i} < 50\\% \\\\\n",
    "0, & \\text{otherwise}\n",
    "\\end{cases}\n",
    "$$\n",
    "\n",
    "- **Pre-Depletion Data**: $ D_i = 0 $ (battery level above threshold).\n",
    "- **Post-Depletion Data**: $ D_i = 1 $ (battery level below threshold).\n",
    "\n",
    "## **4. Constructing Grid Cell Transitions**\n",
    "To model **spatial movement**, we define a transition as:\n",
    "\n",
    "$$\n",
    "T_i = (G_i, G_{i+1})\n",
    "$$\n",
    "\n",
    "where:\n",
    "- $ G_i $ is the grid cell at time $ t_i $,\n",
    "- $ G_{i+1} $ is the **next** grid cell at $ t_{i+1} $.\n",
    "\n",
    "We compute the **state transitions** for each vehicle:\n",
    "\n",
    "$$\n",
    "\\text{Next\\_Grid\\_Cell} = G_{i+1} = \\text{shift}(G_i, -1)\n",
    "$$\n",
    "\n",
    "Dropping the last row for each vehicle ensures only **valid transitions** are included.\n",
    "\n",
    "## **5. Constructing the Markov Transition Matrix**\n",
    "A **first-order Markov model** is constructed, where each transition probability is estimated as:\n",
    "\n",
    "$$\n",
    "P(G_{i+1} | G_i) = \\frac{N(G_i \\to G_{i+1})}{\\sum_{G_j} N(G_i \\to G_j)}\n",
    "$$\n",
    "\n",
    "where:\n",
    "- $ N(G_i \\to G_{i+1}) $ is the **count** of observed transitions from $ G_i $ to $ G_{i+1} $,\n",
    "- The denominator sums over **all possible** next states.\n",
    "\n",
    "The **transition matrix** $ P $ is structured as:\n",
    "\n",
    "$$\n",
    "P = \\begin{bmatrix}\n",
    "P(G_1 | G_1) & P(G_2 | G_1) & \\dots & P(G_n | G_1) \\\\\n",
    "P(G_1 | G_2) & P(G_2 | G_2) & \\dots & P(G_n | G_2) \\\\\n",
    "\\vdots & \\vdots & \\ddots & \\vdots \\\\\n",
    "P(G_1 | G_n) & P(G_2 | G_n) & \\dots & P(G_n | G_n)\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "This is a **stochastic matrix**, where each row sums to **1**:\n",
    "\n",
    "$$\n",
    "\\sum_{G_{i+1}} P(G_{i+1} | G_i) = 1, \\quad \\forall G_i\n",
    "$$\n",
    "\n",
    "## **6. Predicting the Most Likely Next Grid Cell**\n",
    "For a given grid cell $ G_i $, the predicted **next location** is:\n",
    "\n",
    "$$\n",
    "G_{\\text{predicted}} = \\arg\\max_{G_{i+1}} P(G_{i+1} | G_i)\n",
    "$$\n",
    "\n",
    "This follows a **greedy decision rule**, selecting the most probable transition based on historical data.\n",
    "\n",
    "## **7. Validation Against Post-Depletion Data**\n",
    "For vehicles that **experienced battery depletion**:\n",
    "\n",
    "- The predicted transition is compared to the **actual** next grid cell.\n",
    "- A **correct prediction** is counted if:\n",
    "\n",
    "  $$\n",
    "  G_{\\text{predicted}} = G_{\\text{actual}}\n",
    "  $$\n",
    "\n",
    "- The **prediction accuracy** is computed as:\n",
    "\n",
    "  $$\n",
    "  \\text{Accuracy} = \\frac{\\sum \\mathbb{1}(G_{\\text{predicted}} = G_{\\text{actual}})}{N_{\\text{post-depletion}}}\n",
    "  $$\n",
    "\n",
    "where $ N_{\\text{post-depletion}} $ is the total number of post-depletion data points.\n",
    "\n",
    "## **8. Output and Insights**\n",
    "- The **transition matrix** is saved for further analysis.\n",
    "- The **post-depletion validation results** are stored.\n",
    "- The **top 10 most likely transitions** are displayed, providing insight into dominant movement patterns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## 1ST ORDER MC\n",
    "\n",
    "# Step 1: Assign Vehicles to Grid Cells Using Polygon Containment\n",
    "#df_gdf = gpd.GeoDataFrame(df, geometry=gpd.points_from_xy(df['Log'], df['Lat']), crs=\"EPSG:4326\")\n",
    "\n",
    "# Function to find which grid cell a point belongs to\n",
    "def find_grid_cell(point, grid_gdf):\n",
    "    match = grid_gdf.contains(point.geometry)\n",
    "    if match.any():\n",
    "        return match.idxmax()  # Return index of matching grid cell\n",
    "    else:\n",
    "        return None  # No match found\n",
    "\n",
    "# Apply function to assign each GPS point to a grid cell\n",
    "df_gdf['Grid_Cell'] = df_gdf.apply(lambda row: find_grid_cell(row, grid_gdf), axis=1)\n",
    "\n",
    "# Drop rows where no grid cell was matched (outliers)\n",
    "df_gdf = df_gdf.dropna(subset=['Grid_Cell'])\n",
    "\n",
    "# Step 2: Sort by deviceID and Timestamp for Transition Analysis\n",
    "df_gdf = df_gdf.sort_values(by=['deviceID', 'Timestamp'])\n",
    "\n",
    "# Step 3: Identify Sensor Depletion (SOC_batt < 30)\n",
    "df_gdf['Depleted'] = df_gdf['SOC_batt'] < df_gdf['Safe_SOC_Threshold']\n",
    "\n",
    "# Step 4: Separate Pre- and Post-Depletion Data\n",
    "df_pre_depletion = df_gdf[~df_gdf['Depleted']].copy()\n",
    "df_post_depletion = df_gdf[df_gdf['Depleted']].copy()\n",
    "\n",
    "# Step 5: Create Transitions (from one grid cell to the next)\n",
    "df_pre_depletion['Next_Grid_Cell'] = df_pre_depletion.groupby('deviceID')['Grid_Cell'].shift(-1)\n",
    "\n",
    "# Drop last row per vehicle (no next transition available)\n",
    "df_transitions = df_pre_depletion.dropna(subset=['Next_Grid_Cell'])\n",
    "\n",
    "# Step 6: Build the Markov Transition Matrix\n",
    "transition_counts = (\n",
    "    df_transitions.groupby(['Grid_Cell', 'Next_Grid_Cell'])\n",
    "    .size()\n",
    "    .unstack(fill_value=0)\n",
    ")\n",
    "transition_probabilities = transition_counts.div(transition_counts.sum(axis=1), axis=0)  # Normalize to probabilities\n",
    "\n",
    "# Step 7: Define a Function to Predict the Next Grid Cell\n",
    "def predict_next_grid(current_grid, transition_matrix):\n",
    "    if current_grid in transition_matrix.index:\n",
    "        return transition_matrix.loc[current_grid].idxmax()  # Most likely transition\n",
    "    else:\n",
    "        return None  # No transition data available\n",
    "\n",
    "# Step 8: Validate Predictions Using Post-Depletion Data\n",
    "df_post_depletion['Predicted_Grid_Cell'] = df_post_depletion['Grid_Cell'].apply(\n",
    "    lambda x: predict_next_grid(x, transition_probabilities)\n",
    ")\n",
    "\n",
    "# Step 9: Measure Prediction Accuracy\n",
    "df_post_depletion['Correct_Prediction'] = df_post_depletion['Predicted_Grid_Cell'] == df_post_depletion['Grid_Cell']\n",
    "accuracy = df_post_depletion['Correct_Prediction'].mean()\n",
    "\n",
    "print(f\"Prediction Accuracy: {accuracy:.2%}\")\n",
    "\n",
    "# # Step 10: Save the Transition Matrix and Post-Depletion Validation\n",
    "# output_path_transition_matrix = \"/Users/mayar/Desktop/MIT/Research_Fellow/ENERGY_SENSING/DATA/Transition_Matrix.xlsx\"\n",
    "# transition_probabilities.to_excel(output_path_transition_matrix)\n",
    "\n",
    "# output_path_post_depletion = \"/Users/mayar/Desktop/MIT/Research_Fellow/ENERGY_SENSING/DATA/Post_Depletion_Validation.xlsx\"\n",
    "# df_post_depletion.to_excel(output_path_post_depletion, index=False)\n",
    "\n",
    "# # Step 11: Analyze and Print the Top 10 Most Likely Transitions\n",
    "# most_likely_transitions = (\n",
    "#     transition_probabilities.stack()\n",
    "#     .reset_index()\n",
    "#     .rename(columns={0: 'Probability', 'level_0': 'From_Grid', 'level_1': 'To_Grid'})\n",
    "#     .sort_values(by='Probability', ascending=False)\n",
    "# )\n",
    "\n",
    "# print(\"Top 10 Most Likely Transitions:\")\n",
    "# print(most_likely_transitions.head(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# **ðŸ”¹ Use Spatial Indexing for Fast Grid Cell Lookup**\n",
    "grid_sindex = grid_gdf.sindex\n",
    "\n",
    "def find_grid_cell(point):\n",
    "    possible_matches = list(grid_sindex.intersection(point.bounds))  # âœ… FIXED: Use `.bounds`\n",
    "    for match in possible_matches:\n",
    "        if grid_gdf.iloc[match].geometry.contains(point):  # âœ… FIXED: Use `point` directly\n",
    "            return grid_gdf.index[match]  # Return grid cell index\n",
    "    return None\n",
    "\n",
    "# **ðŸ”¹ Apply Fast Lookup in Parallel**\n",
    "df_gdf['Grid_Cell'] = df_gdf['geometry'].map(find_grid_cell)  # âœ… FIXED: Ensure correct `.map()` usage\n",
    "\n",
    "# Drop rows where no grid cell was matched\n",
    "df_gdf.dropna(subset=['Grid_Cell'], inplace=True)\n",
    "\n",
    "# Step 2: Sort by deviceID and Timestamp for Transition Analysis\n",
    "df_gdf.sort_values(by=['deviceID', 'Timestamp'], inplace=True)\n",
    "\n",
    "# Step 3: Identify Sensor Depletion \n",
    "depletion_threshold = df_gdf['Safe_SOC_Threshold']\n",
    "df_gdf['Depleted'] = df_gdf['SOC_batt'] < depletion_threshold\n",
    "\n",
    "# Step 4: Separate Pre- and Post-Depletion Data\n",
    "df_pre_depletion = df_gdf[~df_gdf['Depleted']].copy()\n",
    "df_post_depletion = df_gdf[df_gdf['Depleted']].copy()\n",
    "\n",
    "# Step 5: Compute Transitions Efficiently\n",
    "df_pre_depletion['Next_Grid_Cell'] = df_pre_depletion.groupby('deviceID')['Grid_Cell'].shift(-1)\n",
    "df_transitions = df_pre_depletion.dropna(subset=['Next_Grid_Cell'])\n",
    "\n",
    "# **ðŸ”¹ Fast Markov Transition Matrix Computation (Numba-Optimized)**\n",
    "@numba.njit(parallel=True)\n",
    "def compute_transition_matrix(transitions):\n",
    "    unique_states = np.unique(transitions)  # Extract unique states\n",
    "    num_states = len(unique_states)\n",
    "    \n",
    "    # **Numba-Compatible Mapping (Replace Dictionary)**\n",
    "    state_map = {state: i for i, state in enumerate(unique_states)}  # Index mapping\n",
    "    \n",
    "    # **Initialize Transition Matrix**\n",
    "    matrix = np.zeros((num_states, num_states), dtype=np.float32)\n",
    "\n",
    "    # **Compute State Transitions in Parallel**\n",
    "    for i in numba.prange(len(transitions) - 1):\n",
    "        if transitions[i] in state_map and transitions[i + 1] in state_map:\n",
    "            from_idx = state_map[transitions[i]]\n",
    "            to_idx = state_map[transitions[i + 1]]\n",
    "            matrix[from_idx, to_idx] += 1  # Increment count\n",
    "\n",
    "    # **Normalize Matrix (Convert to Probabilities)**\n",
    "    row_sums = matrix.sum(axis=1)\n",
    "    for i in range(num_states):\n",
    "        if row_sums[i] > 0:\n",
    "            matrix[i, :] /= row_sums[i]\n",
    "\n",
    "    return matrix, unique_states  # Return transition matrix & corresponding states\n",
    "\n",
    "# **ðŸ”¹ Convert transitions to NumPy for Fast Processing**\n",
    "transitions_np = df_transitions[['Grid_Cell', 'Next_Grid_Cell']].to_numpy().flatten()\n",
    "\n",
    "# **ðŸš€ Compute Transition Matrix Using Numba**\n",
    "transition_matrix, state_list = compute_transition_matrix(transitions_np)\n",
    "\n",
    "# **Convert Back to Pandas DataFrame**\n",
    "transition_df = pd.DataFrame(transition_matrix, index=state_list, columns=state_list)\n",
    "\n",
    "# **ðŸ”¹ Step 7: Optimized Prediction Function**\n",
    "def predict_next_grid_batch(grid_cells, transition_df):\n",
    "    return [transition_df.loc[cell].idxmax() if cell in transition_df.index else None for cell in grid_cells]\n",
    "\n",
    "# **Parallel Prediction**\n",
    "df_post_depletion['Predicted_Grid_Cell'] = predict_next_grid_batch(df_post_depletion['Grid_Cell'], transition_df)\n",
    "\n",
    "# **Step 8: Measure Prediction Accuracy**\n",
    "df_post_depletion['Correct_Prediction'] = df_post_depletion['Predicted_Grid_Cell'] == df_post_depletion['Grid_Cell']\n",
    "accuracy = df_post_depletion['Correct_Prediction'].mean()\n",
    "\n",
    "print(f\"Optimized Prediction Accuracy: {accuracy:.2%}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# **Step 1: Convert GPS Data to Geospatial Format**\n",
    "df_gdf = gpd.GeoDataFrame(df, geometry=gpd.points_from_xy(df['Log'], df['Lat']), crs=\"EPSG:4326\")\n",
    "\n",
    "# **ðŸ”¹ Use Spatial Indexing for Fast Grid Cell Lookup**\n",
    "grid_sindex = grid_gdf.sindex\n",
    "\n",
    "def find_grid_cell(point):\n",
    "    possible_matches = list(grid_sindex.intersection(point.bounds))\n",
    "    for match in possible_matches:\n",
    "        if grid_gdf.iloc[match].geometry.contains(point):\n",
    "            return grid_gdf.index[match]  # Return grid cell index\n",
    "    return None\n",
    "\n",
    "# **ðŸ”¹ Apply Fast Lookup in Parallel**\n",
    "df_gdf['Grid_Cell'] = df_gdf['geometry'].map(find_grid_cell)\n",
    "df_gdf.dropna(subset=['Grid_Cell'], inplace=True)\n",
    "\n",
    "# **Step 2: Merge Computed Safe SOC Thresholds**\n",
    "df_gdf = df_gdf.merge(safe_soc_thresholds_df, on=['deviceID', 'Timestamp'], how='left')\n",
    "\n",
    "# **Step 3: Sort by deviceID and Timestamp for Transition Analysis**\n",
    "df_gdf.sort_values(by=['deviceID', 'Timestamp'], inplace=True)\n",
    "\n",
    "# **Step 4: Identify Sensor Depletion Using Dynamic Safe SOC**\n",
    "df_gdf['Depleted'] = df_gdf['SOC_batt'] < df_gdf['Safe_SOC_Threshold']  # âœ… Replaced static threshold\n",
    "\n",
    "# **Step 5: Separate Pre- and Post-Depletion Data**\n",
    "df_pre_depletion = df_gdf[~df_gdf['Depleted']].copy()\n",
    "df_post_depletion = df_gdf[df_gdf['Depleted']].copy()\n",
    "\n",
    "# **Step 6: Compute Next Unique Grid Cell for the Entire Dataset**\n",
    "def find_next_different(series):\n",
    "    \"\"\"Finds the next different grid cell for each row within a deviceID group.\"\"\"\n",
    "    next_values = series.to_numpy()\n",
    "    result = np.full_like(next_values, fill_value=np.nan, dtype=np.float64)\n",
    "\n",
    "    for i in range(len(next_values) - 1):  \n",
    "        for j in range(i + 1, len(next_values)):  # Look ahead to find the next different value\n",
    "            if next_values[j] != next_values[i]:  \n",
    "                result[i] = next_values[j]  # Assign the first different value found\n",
    "                break  \n",
    "\n",
    "    return pd.Series(result, index=series.index)  # Ensure index remains unchanged\n",
    "\n",
    "# âœ… Apply Next Unique Grid Cell to the Entire Dataset (NOT JUST df_pre_depletion)\n",
    "df_gdf['Next_Grid_Cell'] = df_gdf.groupby('deviceID')['Grid_Cell'].transform(find_next_different)\n",
    "df_transitions = df_gdf.dropna(subset=['Next_Grid_Cell'])  # Train on full dataset\n",
    "\n",
    "# **ðŸ”¹ Fast Markov Transition Matrix Computation (Numba-Optimized)**\n",
    "@numba.njit(parallel=True)\n",
    "def compute_transition_matrix(transitions):\n",
    "    unique_states = np.unique(transitions)  \n",
    "    num_states = len(unique_states)\n",
    "\n",
    "    # **Numba-Compatible Mapping**\n",
    "    state_map = {state: i for i, state in enumerate(unique_states)}\n",
    "\n",
    "    # **Initialize Transition Matrix**\n",
    "    matrix = np.zeros((num_states, num_states), dtype=np.float32)\n",
    "\n",
    "    # **Compute State Transitions in Parallel**\n",
    "    for i in numba.prange(len(transitions) - 1):\n",
    "        if transitions[i] in state_map and transitions[i + 1] in state_map:\n",
    "            from_idx = state_map[transitions[i]]\n",
    "            to_idx = state_map[transitions[i + 1]]\n",
    "            matrix[from_idx, to_idx] += 1  # Increment count\n",
    "\n",
    "    # **Normalize Matrix (Convert to Probabilities)**\n",
    "    row_sums = matrix.sum(axis=1)\n",
    "    for i in range(num_states):\n",
    "        if row_sums[i] > 0:\n",
    "            matrix[i, :] /= row_sums[i]\n",
    "\n",
    "    return matrix, unique_states  \n",
    "\n",
    "# **ðŸ”¹ Convert transitions to NumPy for Fast Processing**\n",
    "transitions_np = df_transitions[['Grid_Cell', 'Next_Grid_Cell']].to_numpy().flatten()\n",
    "\n",
    "# **ðŸš€ Compute Transition Matrix Using Numba**\n",
    "transition_matrix, state_list = compute_transition_matrix(transitions_np)\n",
    "\n",
    "# **Convert Back to Pandas DataFrame**\n",
    "transition_df = pd.DataFrame(transition_matrix, index=state_list, columns=state_list)\n",
    "\n",
    "# **ðŸ”¹ Step 7: Optimized Prediction Function**\n",
    "def predict_next_grid_batch(grid_cells, transition_df):\n",
    "    return [transition_df.loc[cell].idxmax() if cell in transition_df.index else None for cell in grid_cells]\n",
    "\n",
    "# **Use `Next_Grid_Cell` from Full Dataset for Predictions**\n",
    "df_post_depletion['Next_Grid_Cell'] = df_post_depletion.groupby('deviceID')['Grid_Cell'].transform(find_next_different)\n",
    "\n",
    "# **Parallel Prediction**\n",
    "df_post_depletion['Predicted_Grid_Cell'] = predict_next_grid_batch(df_post_depletion['Next_Grid_Cell'], transition_df)\n",
    "\n",
    "# âœ… Reorder Columns to Place Next_Grid_Cell After Depleted\n",
    "column_order = df_post_depletion.columns.tolist()\n",
    "column_order.remove('Next_Grid_Cell')  # Remove if exists\n",
    "depleted_index = column_order.index('Depleted')\n",
    "column_order.insert(depleted_index + 1, 'Next_Grid_Cell')  # Insert after Depleted\n",
    "df_post_depletion = df_post_depletion[column_order]  # Apply new column order\n",
    "\n",
    "# **Step 8: Measure Prediction Accuracy**\n",
    "df_post_depletion['Correct_Prediction'] = (\n",
    "    df_post_depletion['Predicted_Grid_Cell'].fillna(\"\").astype(str)\n",
    "    == df_post_depletion['Grid_Cell'].fillna(\"\").astype(str)\n",
    ")\n",
    "accuracy = df_post_depletion['Correct_Prediction'].mean()\n",
    "\n",
    "print(f\"Optimized Prediction Accuracy: {accuracy:.2%}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save DataFrame to Excel file\n",
    "df_post_depletion.to_excel(\"/workspace/data/df_post_depletion_1.xlsx\", index=False)\n",
    "df_pre_depletion.to_excel(\"/workspace/data/df_pre_depletion_1.xlsx\", index=False)\n",
    "\n",
    "# Return file path for download\n",
    "file_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# **Step 1: Convert GPS Data to Geospatial Format**\n",
    "df_gdf = gpd.GeoDataFrame(df, geometry=gpd.points_from_xy(df['Log'], df['Lat']), crs=\"EPSG:4326\")\n",
    "\n",
    "# **Step 2: Assign Vehicles to Grid Cells Using Spatial Index**\n",
    "grid_sindex = grid_gdf.sindex  \n",
    "\n",
    "def find_grid_cell(point):\n",
    "    \"\"\"Find the grid cell containing the given point.\"\"\"\n",
    "    possible_matches = list(grid_sindex.intersection(point.bounds))\n",
    "    for match in possible_matches:\n",
    "        if grid_gdf.iloc[match].geometry.contains(point):\n",
    "            return grid_gdf.index[match]  \n",
    "    return None  \n",
    "\n",
    "df_gdf['Grid_Cell'] = df_gdf['geometry'].map(find_grid_cell)\n",
    "df_gdf.dropna(subset=['Grid_Cell'], inplace=True)\n",
    "\n",
    "# **Step 3: Merge Safe SOC Thresholds**\n",
    "df_gdf = df_gdf.merge(safe_soc_thresholds_df, on=['deviceID', 'Timestamp'], how='left')\n",
    "\n",
    "# **Step 4: Sort and Identify Sensor Depletion**\n",
    "df_gdf.sort_values(by=['deviceID', 'Timestamp'], inplace=True)\n",
    "df_gdf['Depleted'] = df_gdf['SOC_batt'] < df_gdf['Safe_SOC_Threshold']\n",
    "\n",
    "# **Step 5: Separate Pre- and Post-Depletion Data**\n",
    "df_pre_depletion = df_gdf[~df_gdf['Depleted']].copy()\n",
    "df_post_depletion = df_gdf[df_gdf['Depleted']].copy()\n",
    "\n",
    "# **Step 6: Compute Transition Matrices**\n",
    "max_transitions_per_device = df_pre_depletion.groupby('deviceID')['Grid_Cell'].count().min()\n",
    "MAX_ORDER = max(1, min(max_transitions_per_device - 1, 3))  \n",
    "print(f\"ðŸ”¹ Automatically Set MAX_ORDER = {MAX_ORDER}\")\n",
    "\n",
    "def find_next_different(series):\n",
    "    \"\"\"Finds the next different grid cell for each row within a deviceID group.\"\"\"\n",
    "    next_values = series.to_numpy()  # Convert to NumPy array for efficiency\n",
    "    result = np.full_like(next_values, fill_value=np.nan, dtype=np.float64)  # Initialize result\n",
    "\n",
    "    # Loop through each value in the series\n",
    "    for i in range(len(next_values) - 1):  \n",
    "        for j in range(i + 1, len(next_values)):  # Look ahead to find the next different value\n",
    "            if next_values[j] != next_values[i]:  \n",
    "                result[i] = next_values[j]  # Assign the first different value found\n",
    "                break  \n",
    "\n",
    "    return pd.Series(result, index=series.index)  # Ensure index remains unchanged\n",
    "\n",
    "# âœ… Apply it safely without breaking the DataFrame index\n",
    "df_pre_depletion['Next_Grid_Cell'] = df_pre_depletion.groupby('deviceID')['Grid_Cell'].transform(find_next_different)\n",
    "\n",
    "def find_previous_different(series):\n",
    "    \"\"\"Finds the last different grid cell for each row within a deviceID group.\"\"\"\n",
    "    prev_values = series.to_numpy()  # Convert to NumPy array for efficiency\n",
    "    result = np.full_like(prev_values, fill_value=np.nan, dtype=np.float64)  # Initialize result\n",
    "\n",
    "    for i in range(1, len(prev_values)):  \n",
    "        for j in range(i - 1, -1, -1):  # Look backward to find the last different value\n",
    "            if prev_values[j] != prev_values[i]:  \n",
    "                result[i] = prev_values[j]  # Assign the first different value found\n",
    "                break  \n",
    "\n",
    "    return pd.Series(result, index=series.index)\n",
    "\n",
    "# âœ… Apply for the first order\n",
    "df_pre_depletion['Prev_Grid_Cell_1'] = df_pre_depletion.groupby('deviceID')['Grid_Cell'].transform(find_previous_different)\n",
    "\n",
    "# âœ… Build higher orders iteratively, stopping when history runs out\n",
    "for order in range(2, MAX_ORDER + 1):\n",
    "    prev_col = f'Prev_Grid_Cell_{order - 1}'  # Previous order column\n",
    "    new_col = f'Prev_Grid_Cell_{order}'\n",
    "\n",
    "    # Only compute next order for rows where the previous order is NOT NaN\n",
    "    df_pre_depletion[new_col] = df_pre_depletion.groupby('deviceID')[prev_col].transform(\n",
    "        lambda x: find_previous_different(x) if x.notna().any() else np.nan\n",
    "    )\n",
    "\n",
    "# **Step 7: Compute Transition Matrices for Each Order with Laplace Smoothing**\n",
    "alpha = 0.01  \n",
    "transition_probabilities = {}\n",
    "\n",
    "# âœ… Ensure First-Order Matches Standalone Model\n",
    "transition_probabilities[1] = transition_df.copy()  # Directly use standalone first-order model\n",
    "\n",
    "# âœ… Compute Higher-Order Matrices\n",
    "for order in range(2, MAX_ORDER + 1):\n",
    "    prev_cols = [f'Prev_Grid_Cell_{i}' for i in range(order, 0, -1)]\n",
    "    required_cols = prev_cols + ['Grid_Cell', 'Next_Grid_Cell']\n",
    "    df_transitions = df_pre_depletion.dropna(subset=required_cols)\n",
    "\n",
    "    if not df_transitions.empty:\n",
    "        transition_counts = df_transitions.groupby(required_cols).size().unstack(fill_value=0)\n",
    "        row_sums = transition_counts.sum(axis=1)\n",
    "        transition_probabilities[order] = (transition_counts).div(row_sums, axis=0).fillna(0)\n",
    "\n",
    "# **Step 8: Define Hybrid N-th Order Prediction Function**\n",
    "def predict_next_grid_n_order(prev_grids, current_grid, transition_probabilities):\n",
    "    \"\"\"Predict the next grid cell using the highest available Markov order, gradually falling back.\"\"\"\n",
    "    print(f\"ðŸ” Predicting for {current_grid} with history: {prev_grids}\")\n",
    "\n",
    "    # Try the highest available order first\n",
    "    for order in range(len(prev_grids), 0, -1):  \n",
    "        key = (current_grid,) if order == 1 else tuple(prev_grids[-order:]) + (current_grid,)\n",
    "\n",
    "        if order in transition_probabilities and key in transition_probabilities[order].index:\n",
    "            print(f\"âœ… Used Order {order} for {current_grid} (Key: {key})\")\n",
    "            return transition_probabilities[order].loc[key].idxmax()\n",
    "\n",
    "    # Fall back to first-order correctly\n",
    "    if 1 in transition_probabilities and current_grid in transition_probabilities[1].index:\n",
    "        print(f\"âš ï¸ Falling back to Order 1 for {current_grid} (Key: ({current_grid},))\")\n",
    "        return transition_probabilities[1].loc[current_grid].idxmax()\n",
    "\n",
    "    # If everything fails, use most frequent transition\n",
    "    if 1 in transition_probabilities and not transition_probabilities[1].empty:\n",
    "        most_common_transition = transition_probabilities[1].sum(axis=1).idxmax()\n",
    "        print(f\"ðŸ”¹ Choosing most frequent transition for {current_grid}: {most_common_transition}\")\n",
    "        return most_common_transition\n",
    "\n",
    "    print(f\"ðŸš¨ No prediction found for {current_grid}. Returning Unknown.\")\n",
    "    return \"Unknown\"\n",
    "\n",
    "# **Step 9: Apply Hybrid N-th Order Prediction to Post-Depletion Data**\n",
    "prev_grid_columns = [f'Prev_Grid_Cell_{order}' for order in range(1, MAX_ORDER + 1)]\n",
    "# **Step 9: Apply Hybrid N-th Order Prediction to Post-Depletion Data**\n",
    "prev_grid_columns = [f'Prev_Grid_Cell_{order}' for order in range(1, MAX_ORDER + 1)]\n",
    "\n",
    "# âœ… Apply first order correctly\n",
    "df_post_depletion['Prev_Grid_Cell_1'] = df_post_depletion.groupby('deviceID')['Grid_Cell'].transform(find_previous_different)\n",
    "\n",
    "# âœ… Apply higher orders while ensuring correct stopping\n",
    "for order in range(2, MAX_ORDER + 1):\n",
    "    prev_col = f'Prev_Grid_Cell_{order - 1}'  # Previous order column\n",
    "    new_col = f'Prev_Grid_Cell_{order}'\n",
    "\n",
    "    # Only compute for rows where the previous order is NOT NaN\n",
    "    df_post_depletion[new_col] = df_post_depletion.groupby('deviceID')[prev_col].transform(\n",
    "        lambda x: find_previous_different(x) if x.notna().any() else np.nan\n",
    "    )\n",
    "\n",
    "def safe_predict(row):\n",
    "    prev_grids = [row[col] for col in prev_grid_columns if pd.notna(row[col])]\n",
    "\n",
    "    if not prev_grids:  \n",
    "        return np.nan\n",
    "\n",
    "    predicted = predict_next_grid_n_order(prev_grids, row['Grid_Cell'], transition_probabilities)\n",
    "    return predicted if isinstance(predicted, (int, str, float)) else \"Unknown\"\n",
    "\n",
    "# âœ… Debugging Function\n",
    "def debug_prediction(prev_grids, current_grid, transition_probabilities):\n",
    "    for order in range(len(prev_grids), 0, -1):\n",
    "        key = (current_grid,) if order == 1 else tuple(prev_grids[-order:]) + (current_grid,)\n",
    "\n",
    "        if order in transition_probabilities and key in transition_probabilities[order].index:\n",
    "            print(f\"âœ… Order {order} used for {current_grid} (Key: {key})\")\n",
    "            return transition_probabilities[order].loc[key].idxmax()\n",
    "\n",
    "    print(f\"âš ï¸ No prediction found for {current_grid} with {prev_grids}. Falling back...\")\n",
    "    return None\n",
    "\n",
    "# **Apply Prediction**\n",
    "df_post_depletion['Predicted_Grid_Cell_Hybrid'] = df_post_depletion.apply(\n",
    "    lambda row: safe_predict(row), axis=1\n",
    ")\n",
    "df_post_depletion['Predicted_Grid_Cell_Hybrid'].fillna(\"Unknown\", inplace=True)\n",
    "\n",
    "# **Step 10: Measure Prediction Accuracy**\n",
    "df_post_depletion['Correct_Prediction_Hybrid'] = df_post_depletion['Predicted_Grid_Cell_Hybrid'].eq(df_post_depletion['Grid_Cell'])\n",
    "accuracy_hybrid = df_post_depletion['Correct_Prediction_Hybrid'].mean()\n",
    "\n",
    "print(f\"Hybrid N-th Order Markov Chain Prediction Accuracy: {accuracy_hybrid:.2%}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save DataFrame to Excel file\n",
    "df_post_depletion.to_excel(\"/workspace/data/df_post_depletion.xlsx\", index=False)\n",
    "df_pre_depletion.to_excel(\"/workspace/data/df_pre_depletion.xlsx\", index=False)\n",
    "\n",
    "# Return file path for download\n",
    "file_path\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check how many transitions exist at each order\n",
    "for order in transition_probabilities:\n",
    "    print(f\"ðŸ“Š Order {order}: {len(transition_probabilities[order])} transitions recorded\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Hybrid N-th Order Markov Chain Prediction Accuracy: {accuracy_hybrid:.2%}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract entropy values\n",
    "orders = list(transition_probabilities.keys())\n",
    "entropy_values = []\n",
    "\n",
    "for order in orders:\n",
    "    probs = transition_probabilities[order]\n",
    "    entropy = -np.nansum(probs * np.log2(probs))\n",
    "    entropy_values.append(entropy)\n",
    "    print(f\"ðŸ”¹ Entropy (Order {order}): {entropy:.4f}\")\n",
    "\n",
    "# Convert orders and entropy values to numpy arrays\n",
    "orders_array = np.array(orders)\n",
    "entropy_array = np.array(entropy_values)\n",
    "\n",
    "# Normalize the values for better numerical stability\n",
    "orders_norm = (orders_array - orders_array.min()) / (orders_array.max() - orders_array.min())\n",
    "entropy_norm = (entropy_array - entropy_array.min()) / (entropy_array.max() - entropy_array.min())\n",
    "\n",
    "# Define the line from the first to the last point\n",
    "line_vec = np.array([orders_norm[-1] - orders_norm[0], entropy_norm[-1] - entropy_norm[0]])\n",
    "line_vec_norm = line_vec / np.linalg.norm(line_vec)\n",
    "\n",
    "# Compute distances of each point from the line\n",
    "distances = []\n",
    "for i in range(len(orders_norm)):\n",
    "    point_vec = np.array([orders_norm[i] - orders_norm[0], entropy_norm[i] - entropy_norm[0]])\n",
    "    proj = np.dot(point_vec, line_vec_norm) * line_vec_norm\n",
    "    dist_vec = point_vec - proj\n",
    "    distances.append(np.linalg.norm(dist_vec))\n",
    "\n",
    "# Find the elbow point as the max distance from the line\n",
    "elbow_index = np.argmax(distances)\n",
    "elbow_order = orders[elbow_index]\n",
    "\n",
    "# Plot entropy vs. Markov order with elbow point\n",
    "plt.figure(figsize=(8, 5))\n",
    "plt.plot(orders, entropy_values, marker='o', linestyle='-', color='b', label=\"Entropy\")\n",
    "plt.axvline(x=elbow_order, color='r', linestyle='--', label=f'Elbow at N={elbow_order}')\n",
    "plt.xlabel(\"Markov Chain Order (N)\")\n",
    "plt.ylabel(\"Entropy\")\n",
    "plt.title(\"Entropy vs. Markov Order with Corrected Elbow Point\")\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()\n",
    "\n",
    "# Return the detected elbow order\n",
    "print(f\"ðŸ”¹ Optimal order is {elbow_order}\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import scipy.stats as stats\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import mutual_info_score\n",
    "\n",
    "# **Step 1: Compute Mutual Information (MI)**\n",
    "def compute_mutual_information(df, prev_grid_col, next_grid_col='Next_Grid_Cell'):\n",
    "    valid_df = df.dropna(subset=[prev_grid_col, next_grid_col])\n",
    "    return mutual_info_score(valid_df[prev_grid_col], valid_df[next_grid_col])\n",
    "\n",
    "# **Step 2: Compute Conditional Entropy (Fixed)**\n",
    "def compute_conditional_entropy(df, prev_grid_col, next_grid_col='Next_Grid_Cell'):\n",
    "    contingency_table = pd.crosstab(df[prev_grid_col], df[next_grid_col])\n",
    "    probs = contingency_table.div(contingency_table.sum(axis=1), axis=0)\n",
    "\n",
    "    # ðŸ”¹ Fix: Replace zero values with NaN to avoid log(0) issues\n",
    "    probs = probs.replace(0, np.nan)\n",
    "    \n",
    "    entropy = -np.nansum(probs * np.log2(probs))  # Ignore NaN values in computation\n",
    "    return entropy\n",
    "\n",
    "# **Step 3: Chi-Square Test for Independence**\n",
    "def chi_square_test(df, prev_grid_col, next_grid_col='Next_Grid_Cell'):\n",
    "    contingency_table = pd.crosstab(df[prev_grid_col], df[next_grid_col])\n",
    "    chi2_stat, p_value, _, _ = stats.chi2_contingency(contingency_table)\n",
    "    return chi2_stat, p_value\n",
    "\n",
    "# Assuming df_pre_depletion is already defined in the environment\n",
    "orders = list(range(1, 31))\n",
    "\n",
    "# Initialize dictionaries to store values\n",
    "mi_scores = {}\n",
    "conditional_entropy_scores = {}\n",
    "chi_square_results = {}\n",
    "\n",
    "for order in orders:\n",
    "    prev_col = f'Prev_Grid_Cell_{order}'\n",
    "\n",
    "    mi_scores[order] = compute_mutual_information(df_pre_depletion, prev_col)\n",
    "    conditional_entropy_scores[order] = compute_conditional_entropy(df_pre_depletion, prev_col)\n",
    "    chi2_stat, p_value = chi_square_test(df_pre_depletion, prev_col)\n",
    "    chi_square_results[order] = (chi2_stat, p_value)\n",
    "\n",
    "# Convert results into lists for plotting\n",
    "mi_values = list(mi_scores.values())\n",
    "conditional_entropy_values = list(conditional_entropy_scores.values())\n",
    "chi_square_values = [chi_square_results[o][0] for o in orders]\n",
    "\n",
    "# **Step 5: Plot the Results with Dual Y-Axis**\n",
    "fig, ax1 = plt.subplots(figsize=(10, 5))\n",
    "\n",
    "# Plot Mutual Information on secondary Y-axis\n",
    "ax2 = ax1.twinx()\n",
    "ax2.plot(orders, mi_values, marker='o', linestyle='-', color='b', label=\"Mutual Information\")\n",
    "ax2.set_ylabel(\"Mutual Information\", color='b')\n",
    "ax2.tick_params(axis='y', labelcolor='b')\n",
    "\n",
    "# Plot Conditional Entropy and Chi-Square Statistic on primary Y-axis\n",
    "ax1.plot(orders, chi_square_values, marker='^', linestyle='-', color='r', label=\"Chi-Square Statistic\")\n",
    "\n",
    "ax1.set_xlabel(\"Markov Chain Order (N)\")\n",
    "ax1.set_ylabel(\"Score (Entropy & Chi-Square)\", color='r')\n",
    "ax1.tick_params(axis='y', labelcolor='r')\n",
    "\n",
    "# Tertiary Y-Axis (Conditional Entropy)\n",
    "ax3 = ax1.twinx()\n",
    "ax3.spines['right'].set_position(('outward', 60))  # Move third axis outward\n",
    "ax3.plot(orders, conditional_entropy_values, marker='s', linestyle='-', color='g', label=\"Conditional Entropy\")\n",
    "ax3.set_ylabel(\"Conditional Entropy\", color='g')\n",
    "ax3.tick_params(axis='y', labelcolor='g')\n",
    "\n",
    "# Title and Legend\n",
    "fig.suptitle(\"MI, Conditional Entropy, and Chi-Square vs. Markov Order\")\n",
    "fig.legend(loc=\"upper right\", bbox_to_anchor=(1,1), bbox_transform=ax1.transAxes)\n",
    "\n",
    "plt.grid(True)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **GPU-Accelerated Q-Learning for Grid-Based Mobility Prediction**\n",
    "\n",
    "This implementation leverages **GPU-accelerated computing** for **spatial binning, sensor depletion analysis, and reinforcement learning-based mobility prediction**. The core objective is to **train an agent to predict the next grid cell occupied by a vehicle after battery depletion**, based on historical transitions.\n",
    "\n",
    "## **1. Data Loading and Conversion to cuDF for GPU Acceleration**\n",
    "To facilitate **large-scale spatiotemporal data processing**, the dataset is converted into a **cuDF DataFrame**, enabling computations on NVIDIA GPUs via **RAPIDS cuDF**. \n",
    "\n",
    "Given an original DataFrame **$df$** containing numerical and categorical attributes, each numerical column is converted to **floating-point representation** while categorical variables ($deviceID$ and $Date$) remain as strings. The transformation ensures efficient memory alignment for GPU operations.\n",
    "\n",
    "## **2. Grid Binning Using cuSpatial**\n",
    "A **spatial discretization strategy** is applied to **map GPS coordinates into a structured 120m Ã— 120m grid**. The Earth's curvature necessitates an **adaptive longitude resolution**, computed dynamically based on latitude.\n",
    "\n",
    "### **Latitude and Longitude Transformation**\n",
    "For a given latitude $ \\text{Lat}_i $ and longitude $ \\text{Log}_i $, the coordinates are mapped into grid indices as follows:\n",
    "\n",
    "$$\n",
    "\\text{Grid}_X = \\left\\lfloor \\frac{(\\text{Log}_i - \\text{Log}_{\\min}) \\cdot \\pi / 180 \\cdot R_E \\cdot \\cos(\\text{Lat}_i)}{\\text{grid size}} \\right\\rfloor\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\text{Grid}_Y = \\left\\lfloor \\frac{(\\text{Lat}_i - \\text{Lat}_{\\min}) \\cdot \\pi / 180 \\cdot R_E}{\\text{grid size}} \\right\\rfloor\n",
    "$$\n",
    "\n",
    "where:\n",
    "- $ R_E = 6371000 $ m is Earth's radius,\n",
    "- The **longitude transformation** incorporates the **cosine of latitude** to account for Earth's curvature,\n",
    "- The floor function ensures that values are discretized into **integer grid indices**.\n",
    "\n",
    "Each spatial point is **hashed into a grid cell identifier**:\n",
    "\n",
    "$$\n",
    "\\text{Grid\\_Cell}_i = (\\text{Grid}_X, \\text{Grid}_Y)\n",
    "$$\n",
    "\n",
    "## **3. Sensor Depletion Detection**\n",
    "A **binary depletion flag** is assigned to each vehicle record:\n",
    "\n",
    "$$\n",
    "D_i =\n",
    "\\begin{cases} \n",
    "1, & \\text{if } SOC_{\\text{batt}, i} < 20\\% \\\\\n",
    "0, & \\text{otherwise}\n",
    "\\end{cases}\n",
    "$$\n",
    "\n",
    "where **$ SOC_{\\text{batt}} $** is the state of charge. The dataset is split into:\n",
    "- **Pre-depletion** data **$P$**, containing normal mobility patterns.\n",
    "- **Post-depletion** data **$Q$**, capturing movement after battery exhaustion.\n",
    "\n",
    "The next grid cell for each pre-depletion record is assigned using a **time-ordered shift operation**:\n",
    "\n",
    "$$\n",
    "G_{i+1} = \\text{shift}(G_i, -1)\n",
    "$$\n",
    "\n",
    "ensuring that transitions are correctly captured.\n",
    "\n",
    "## **4. Reinforcement Learning (Q-Learning) for Mobility Prediction**\n",
    "\n",
    "The goal of this reinforcement learning (RL) framework is to train an agent that learns **optimal movement patterns** based on past trajectories and predicts the most probable **next grid cell** a vehicle will occupy after battery depletion. The agent is trained using **Q-learning**, a model-free reinforcement learning algorithm that iteratively updates **Q-values** representing the expected reward for selecting an action (i.e., moving to a new grid cell) from a given state.\n",
    "\n",
    "### **State and Action Representation**\n",
    "- The **state space** consists of all possible **grid cells** $ s \\in S $, where each grid cell is a **120m Ã— 120m spatial unit** indexed as $(X, Y)$.  \n",
    "- The **action space** consists of transitions to **neighboring grid cells**, corresponding to potential movements between states.  \n",
    "\n",
    "Each transition is extracted from **pre-depletion data**, where each record consists of:\n",
    "1. **Current grid cell** $ G_i $\n",
    "2. **Next observed grid cell** $ G_{i+1} $\n",
    "3. **State of charge (SOC)** and depletion flag\n",
    "\n",
    "For each **state-action pair** $(s, a)$, we maintain a **Q-value** $ Q(s, a) $, which represents the estimated cumulative reward expected when selecting action $ a $ from state $ s $.\n",
    "\n",
    "### **Q-Value Update Rule**\n",
    "The agent updates its **Q-values** iteratively using the Bellman equation:\n",
    "\n",
    "$$\n",
    "Q(s, a) = (1 - \\alpha) Q(s, a) + \\alpha \\left( r + \\gamma \\max_{a'} Q(s', a') \\right)\n",
    "$$\n",
    "\n",
    "where:\n",
    "- **$ \\alpha $ (learning rate)** determines how much the newly acquired information overrides the existing Q-value.\n",
    "- **$ \\gamma $ (discount factor)** determines the importance of future rewards.\n",
    "- **$ r $ (reward function)** assigns a numerical value to each transition, encouraging movement patterns that match realistic trajectories.\n",
    "- **$ \\max_{a'} Q(s', a') $** represents the highest Q-value of possible actions in the next state $ s' $, guiding the agent toward high-reward decisions.\n",
    "\n",
    "### **Reward Function**\n",
    "To ensure realistic movement, the **reward function** incorporates both **empirical transition frequency** and **spatial coherence**:\n",
    "\n",
    "$$\n",
    "r = \\log (N(s, a) + 1) + 0.01 + 0.5 \\cdot e^{-\\frac{||s - a||}{\\text{grid size}}}\n",
    "$$\n",
    "\n",
    "where:\n",
    "- $ N(s, a) $ is the number of observed transitions from $ s $ to $ a $ in the dataset.\n",
    "- The **logarithmic term** prevents highly frequent transitions from dominating the learning process.\n",
    "- The **exponential decay term** penalizes large jumps, encouraging spatially coherent movement.\n",
    "\n",
    "### **Exploration vs. Exploitation Policy**\n",
    "The agent follows an **$ \\epsilon $-greedy policy**, balancing exploration (random movement selection) and exploitation (selecting the best known action):\n",
    "\n",
    "$$\n",
    "a =\n",
    "\\begin{cases} \n",
    "\\text{random action}, & \\text{with probability } \\epsilon \\\\\n",
    "\\arg\\max_{a} Q(s, a), & \\text{otherwise}\n",
    "\\end{cases}\n",
    "$$\n",
    "\n",
    "where **$ \\epsilon $** is the exploration probability, which **decays over time** to prioritize exploitation:\n",
    "\n",
    "$$\n",
    "\\epsilon = \\max(0.05, \\epsilon \\cdot \\text{decay factor})\n",
    "$$\n",
    "\n",
    "### **Batch Training Strategy**\n",
    "Instead of updating Q-values one transition at a time, **mini-batch updates** are applied using **vectorized operations**:\n",
    "- A batch size of **1000 transitions** is selected per iteration.\n",
    "- Transitions are processed in parallel using **GPU acceleration**.\n",
    "- Each batch computes transition counts and updates the **Q-table** accordingly.\n",
    "\n",
    "## **5. Hyperparameter Optimization Using Bayesian Search**\n",
    "To optimize **Q-learning performance**, we conduct **Bayesian optimization** over the following hyperparameters:\n",
    "- **$ \\alpha $ (learning rate) in $ [0.1, 0.5] $**: Controls how aggressively Q-values are updated.\n",
    "- **$ \\gamma $ (discount factor) in $ [0.5, 0.9] $**: Adjusts how much future rewards impact current decisions.\n",
    "- **$ \\epsilon $ (exploration probability) in $ [0.1, 0.3] $**: Governs the randomness of action selection.\n",
    "- **$ \\epsilon_{\\text{decay}} $ in $ [0.98, 0.999] $**: Ensures a smooth transition from exploration to exploitation.\n",
    "- **Number of training iterations in $ [2,10] $**: Determines how long the agent learns from past data.\n",
    "\n",
    "The optimization process **minimizes the loss function**:\n",
    "\n",
    "$$\n",
    "\\mathcal{L} = -\\text{Accuracy}\n",
    "$$\n",
    "\n",
    "where accuracy is computed as:\n",
    "\n",
    "$$\n",
    "\\text{Accuracy} = \\frac{\\sum \\mathbb{1} (G_{\\text{predicted}} = G_{\\text{actual}})}{N_{\\text{post-depletion}}}\n",
    "$$\n",
    "\n",
    "Bayesian search uses **tree-structured Parzen estimators (TPE)** to efficiently explore the hyperparameter space.\n",
    "\n",
    "## **6. RL-Based Mobility Prediction**\n",
    "Once training is complete, the **Q-table is used for inference** to predict the most likely **next grid cell** after depletion:\n",
    "\n",
    "$$\n",
    "G_{\\text{predicted}} = \\arg\\max_{a} Q(G_{\\text{current}}, a)\n",
    "$$\n",
    "\n",
    "A final evaluation step compares the predicted post-depletion locations with the actual recorded locations. The **model terminates training** if:\n",
    "\n",
    "$$\n",
    "\\text{Accuracy} \\geq 85\\%\n",
    "$$\n",
    "\n",
    "ensuring that the agent achieves a **sufficiently high prediction accuracy**.\n",
    "\n",
    "## **7. Conclusion**\n",
    "This approach leverages:\n",
    "- **Q-learning with batch training**, enabling efficient large-scale learning.\n",
    "- **GPU acceleration via RAPIDS cuDF**, significantly reducing training time.\n",
    "- **Bayesian hyperparameter tuning**, optimizing Q-learning efficiency.\n",
    "- **Adaptive exploration-exploitation balance**, refining the model over iterations.\n",
    "\n",
    "The final model provides **high-accuracy mobility predictions**, supporting **real-time sensor deployment planning and energy-aware urban mobility analysis**.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The below is the final RL framework"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import cudf\n",
    "import cupy as cp\n",
    "import cuspatial\n",
    "import random\n",
    "from hyperopt import fmin, tpe, hp, STATUS_OK, Trials\n",
    "import shapely.geometry\n",
    "import geopandas as gpd\n",
    "\n",
    "# ==========================\n",
    "# ðŸ”¹ Load and Convert Data to cuDF\n",
    "# ==========================\n",
    "dfcopied = df.copy()\n",
    "\n",
    "# Convert numeric columns\n",
    "for col in dfcopied.columns:\n",
    "    if dfcopied[col].dtype == 'object' and col not in ['deviceID', 'Date']:  \n",
    "        dfcopied[col] = pd.to_numeric(dfcopied[col], errors='coerce')\n",
    "\n",
    "dfcopied['deviceID'] = dfcopied['deviceID'].astype(str)  \n",
    "dfcopied['Date'] = dfcopied['Date'].astype(str)  \n",
    "\n",
    "# Convert to cuDF for GPU processing\n",
    "df_gdf = cudf.DataFrame(dfcopied)\n",
    "\n",
    "# Ensure deviceID and Date remain strings\n",
    "df_gdf['deviceID'] = df_gdf['deviceID'].astype('str')\n",
    "df_gdf['Date'] = df_gdf['Date'].astype('str')\n",
    "\n",
    "# ==========================\n",
    "# ðŸ”¹ Define Grid Binning Using cuSpatial\n",
    "# ==========================\n",
    "# Define spatial bounding box\n",
    "min_x, max_x = df_gdf['Log'].min(), df_gdf['Log'].max()\n",
    "min_y, max_y = df_gdf['Lat'].min(), df_gdf['Lat'].max()\n",
    "\n",
    "EARTH_RADIUS = 6371000  # Earth radius in meters\n",
    "\n",
    "# Convert lat/lon degrees to meters using Haversine formula approximation\n",
    "df_gdf['Grid_X'] = ((df_gdf['Log'] - min_x) * (np.pi/180) * EARTH_RADIUS * np.cos(np.radians(df_gdf['Lat']))).astype(int) // grid_size\n",
    "df_gdf['Grid_Y'] = ((df_gdf['Lat'] - min_y) * (np.pi/180) * EARTH_RADIUS).astype(int) // grid_size\n",
    "\n",
    "df_gdf['Grid_Cell'] = df_gdf['Grid_X'].astype(str) + \"_\" + df_gdf['Grid_Y'].astype(str)\n",
    "\n",
    "# Sort by deviceID and Timestamp for Transition Analysis\n",
    "df_gdf = df_gdf.sort_values(by=['deviceID', 'Timestamp'])\n",
    "\n",
    "# ==========================\n",
    "# ðŸ”¹ Identify Sensor Depletion\n",
    "# ==========================\n",
    "depletion_threshold = 20\n",
    "df_gdf['Depleted'] = df_gdf['SOC_batt'] < depletion_threshold\n",
    "\n",
    "df_pre_depletion = df_gdf[~df_gdf['Depleted']].copy()\n",
    "df_post_depletion = df_gdf[df_gdf['Depleted']].copy()\n",
    "\n",
    "df_pre_depletion['Next_Grid_Cell'] = df_pre_depletion.groupby('deviceID')['Grid_Cell'].shift(-1)\n",
    "df_transitions = cudf.concat([df_pre_depletion, df_post_depletion]).dropna(subset=['Next_Grid_Cell'])\n",
    "\n",
    "# ðŸ›  Debugging Step: Check if df_transitions has data\n",
    "print(f\"Total transitions available for training: {len(df_transitions)}\")\n",
    "\n",
    "# ==========================\n",
    "# ðŸ”¹ 4ï¸âƒ£ Initialize Q-Table & Tracking Lists\n",
    "# ==========================\n",
    "q_table = {}\n",
    "q_table_convergence = []\n",
    "bellman_errors = []\n",
    "policy_consistency = []\n",
    "reward_per_episode = []\n",
    "training_accuracies = []\n",
    "validation_accuracies = []\n",
    "hyperparam_values = []\n",
    "accuracy_values = []\n",
    "\n",
    "# ==========================\n",
    "# ðŸ”¹ 5ï¸âƒ£ Helper Functions\n",
    "# ==========================\n",
    "def parse_grid_cell(grid_cell_str):\n",
    "    try:\n",
    "        x, y = map(int, grid_cell_str.split('_'))\n",
    "        return np.array([x, y])\n",
    "    except ValueError:\n",
    "        return np.array([0, 0])\n",
    "\n",
    "def track_q_table_convergence(prev_q_table, new_q_table):\n",
    "    \"\"\"Compute change in Q-table values between iterations.\"\"\"\n",
    "    delta_q = sum(abs(new_q_table.get(s, {}).get(a, 0) - prev_q_table.get(s, {}).get(a, 0))\n",
    "                  for s in new_q_table for a in new_q_table[s])\n",
    "    q_table_convergence.append(delta_q)\n",
    "\n",
    "def compute_policy_consistency(q_table, df_transitions):\n",
    "    \"\"\"Check how often the same best action is chosen for a state.\"\"\"\n",
    "    correct_choices = 0\n",
    "    total_choices = 0\n",
    "\n",
    "    for _, row in df_transitions.to_pandas().iterrows():\n",
    "        state, next_state = row['Grid_Cell'], row['Next_Grid_Cell']\n",
    "        if state in q_table and next_state in q_table[state]:\n",
    "            best_action = max(q_table[state], key=q_table[state].get)\n",
    "            if best_action == next_state:\n",
    "                correct_choices += 1\n",
    "            total_choices += 1\n",
    "\n",
    "    policy_consistency.append(correct_choices / total_choices if total_choices > 0 else 0)\n",
    "\n",
    "def compute_expected_reward(rewards_per_episode, gamma):\n",
    "    \"\"\"Compute expected cumulative reward using discounted sum formula.\"\"\"\n",
    "    total_reward = sum((gamma**t) * r for t, r in enumerate(rewards_per_episode))\n",
    "    reward_per_episode.append(total_reward)\n",
    "\n",
    "# ==========================\n",
    "# ðŸ”¹ Initialize Tracking Variables\n",
    "# ==========================\n",
    "hyperparam_values = []\n",
    "accuracy_values = []\n",
    "gamma_values = []\n",
    "\n",
    "# Store best Q-table across trials\n",
    "best_q_table = {}\n",
    "best_trial = {'accuracy': -np.inf, 'policy': [], 'rewards': [], 'q_table': {}}  # Store best trial\n",
    "\n",
    "# ==========================\n",
    "# ðŸ”¹ GPU-Accelerated Q-Learning (Final Stability-Optimized Version)\n",
    "# ==========================\n",
    "def train_rl(params):\n",
    "    global hyperparam_values, accuracy_values, gamma_values, best_q_table, best_trial   # Store hyperparameters and best Q-table\n",
    "\n",
    "    # Extract parameters\n",
    "    alpha_init = params['alpha']\n",
    "    gamma = params['gamma']\n",
    "    epsilon_init = 0.1  # Start with reduced randomness\n",
    "    epsilon_decay = params['epsilon_decay']\n",
    "    training_iterations = 40  # Increased training iterations\n",
    "\n",
    "    # Track hyperparameters\n",
    "    gamma_values.append(gamma)\n",
    "    hyperparam_values.append(params)\n",
    "\n",
    "    # Initialize Q-table (reuse best Q-table if available)\n",
    "    q_table = best_q_table.copy() if best_q_table else {}\n",
    "\n",
    "    # Initialize alpha and epsilon decay parameters\n",
    "    alpha = alpha_init\n",
    "    epsilon = epsilon_init\n",
    "    alpha_decay = 0.002  # Slower decay to allow stable learning\n",
    "    epsilon_min = 0.05\n",
    "    epsilon_lambda = 0.005  # Even smoother transition from exploration to exploitation\n",
    "\n",
    "    # Rolling average for policy consistency\n",
    "    policy_consistency_window = []\n",
    "    q_table_sizes = []  # Track Q-table growth\n",
    "    # Initialize prev_q_table before training\n",
    "    prev_q_table = {}  \n",
    "\n",
    "    # RL Training with Vectorized Operations\n",
    "    batch_size = 2000  # Large batch size stabilizes learning\n",
    "\n",
    "    for t in range(training_iterations):\n",
    "        for batch_start in range(0, len(df_transitions), batch_size):\n",
    "            batch = df_transitions.iloc[batch_start:batch_start + batch_size].to_pandas()  # Convert batch to Pandas\n",
    "\n",
    "            states = batch['Grid_Cell'].values\n",
    "            actions = batch['Next_Grid_Cell'].values\n",
    "\n",
    "            # Compute transition counts efficiently\n",
    "            unique_pairs, counts = np.unique(list(zip(states, actions)), axis=0, return_counts=True)\n",
    "\n",
    "            for (state, action), count in zip(unique_pairs, counts):\n",
    "                if state not in q_table:\n",
    "                    q_table[state] = {}\n",
    "\n",
    "                # Ensure each state has at least one valid action with a default Q-value\n",
    "                if action not in q_table[state]:\n",
    "                    q_table[state][action] = np.random.uniform(0.01, 0.1)  # Small random Q-value\n",
    "\n",
    "                # Convert Grid Cells to numerical coordinates\n",
    "                state_coords = parse_grid_cell(state)\n",
    "                action_coords = parse_grid_cell(action)\n",
    "\n",
    "                # Compute Euclidean distance penalty (More balanced)\n",
    "                distance_penalty = np.exp(-np.linalg.norm(state_coords - action_coords) / grid_size) * 0.2\n",
    "\n",
    "                # 1ï¸âƒ£ **Final Adjusted Reward Function (More Stability)**\n",
    "                reward = np.log(count + 20) / 5 + 0.03 + distance_penalty - 0.008  # Smoother scaling\n",
    "\n",
    "                # âœ… **Regularized Q-learning update with stronger stability**\n",
    "                max_future_q = max(q_table[state].values(), default=0)\n",
    "                q_update = reward + gamma * max_future_q - q_table[state][action]\n",
    "                q_table[state][action] += alpha * q_update - 0.0015 * abs(q_table[state][action])  # Stronger Q-value regularization\n",
    "\n",
    "        # âœ… Preserve learned policies between training iterations\n",
    "        if t > 0:\n",
    "            prev_q_table = q_table.copy()\n",
    "\n",
    "        # âœ… Track Q-Table Convergence **AFTER each training iteration**\n",
    "        track_q_table_convergence(prev_q_table, q_table)\n",
    "        q_table_sizes.append(len(q_table))  # Log Q-table growth\n",
    "\n",
    "        # âœ… Compute Expected Reward Growth\n",
    "        compute_expected_reward([q_table[state][action] for state in q_table for action in q_table[state]], gamma)\n",
    "\n",
    "        # âœ… Adaptive Epsilon Decay (Slower and more stable)\n",
    "        epsilon = epsilon_min + (epsilon_init - epsilon_min) * np.exp(-epsilon_lambda * t)\n",
    "\n",
    "        # âœ… Adaptive Alpha Decay for Controlled Learning Rate\n",
    "        alpha = max(0.005, alpha_init / (1 + alpha_decay * t))  # Lower bound prevents sharp drops\n",
    "\n",
    "        # # âœ… **Final Stability Enhancements**\n",
    "        # # ðŸ”¹ Force Exploitation in Last 5% of Training Iterations\n",
    "        # if t > training_iterations * 0.95:\n",
    "        #     epsilon = 0.01  # Minimal exploration, forcing exploitation\n",
    "        \n",
    "        # # ðŸ”¹ Soft Lock on Policy Updates in the Last 3% of Iterations\n",
    "        # if t > training_iterations * 0.97:\n",
    "        #     alpha = 0.003  # Lock learning with very small updates\n",
    "\n",
    "        # # ðŸ”¹ **Final Q-Value Freezing Near Convergence**\n",
    "        # if t > training_iterations * 0.98:\n",
    "        #     for state in q_table:\n",
    "        #         for action in q_table[state]:\n",
    "        #             q_table[state][action] *= 0.999  # More gradual freezing\n",
    "\n",
    "    # âœ… Store best Q-table after training\n",
    "    best_q_table = q_table.copy()\n",
    "\n",
    "    # ðŸ›  Debugging Step: Check if Q-table was updated\n",
    "    print(f\"Total states in Q-table: {len(q_table)}\")\n",
    "\n",
    "    # ==========================\n",
    "    # ðŸ”¹ RL Prediction (Using Decayed Epsilon)\n",
    "    # ==========================\n",
    "    def predict_next_grid(state):\n",
    "        if state in q_table and q_table[state]:  # Ensure Q-values exist\n",
    "            if random.uniform(0, 1) < epsilon:\n",
    "                return random.choice(list(q_table[state].keys()))  # Explore\n",
    "            return max(q_table[state], key=q_table[state].get)  # Exploit\n",
    "        return random.choice(list(q_table.keys())) if q_table else None\n",
    "\n",
    "    df_post_depletion['Predicted_Grid_Cell_RL'] = df_post_depletion['Grid_Cell'].to_pandas().map(predict_next_grid)\n",
    "    df_post_depletion['Correct_Prediction_RL'] = df_post_depletion['Predicted_Grid_Cell_RL'] == df_post_depletion['Grid_Cell']\n",
    "    accuracy = df_post_depletion['Correct_Prediction_RL'].mean()\n",
    "    accuracy_values.append(accuracy)  # Store for later validation\n",
    "\n",
    "    if accuracy > best_trial['accuracy']:\n",
    "        best_trial['accuracy'] = accuracy\n",
    "        best_trial['policy'] = policy_consistency.copy()\n",
    "        best_trial['rewards'] = reward_per_episode.copy()\n",
    "        best_trial['q_table'] = q_table.copy()\n",
    "        best_q_table = q_table.copy()  # Store best Q-table permanently\n",
    "\n",
    "    # ðŸ›  Debugging Step: Check if predictions are being made\n",
    "    print(f\"Total predictions made: {df_post_depletion['Predicted_Grid_Cell_RL'].notna().sum()}\")\n",
    "    print(f\"Trial Accuracy: {accuracy:.2%} | Params: {params}\")\n",
    "\n",
    "    return {'loss': -accuracy, 'status': STATUS_OK}\n",
    "\n",
    "\n",
    "# ==========================\n",
    "# ðŸ”¹ Hyperparameter Optimization (Final Fine-Tuning)\n",
    "# ==========================\n",
    "param_space = {\n",
    "    'alpha': hp.uniform('alpha', 0.18, 0.23),  # Narrowed for smoother learning\n",
    "    'gamma': hp.uniform('gamma', 0.68, 0.75),  # Focused on long-term reward stability\n",
    "    'epsilon_decay': hp.uniform('epsilon_decay', 0.987, 0.993),  # Optimized for better convergence\n",
    "}\n",
    "\n",
    "trials = Trials()\n",
    "best_params = fmin(fn=train_rl, space=param_space, algo=tpe.suggest, max_evals=4, trials=trials)  # Reduced max_evals\n",
    "\n",
    "\n",
    "print(f\"Best RL Parameters: {best_params}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Set visualization style\n",
    "sns.set(style=\"whitegrid\")\n",
    "\n",
    "\n",
    "\n",
    "# âœ… 2ï¸âƒ£ Policy Consistency Over Time\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.plot(policy_consistency, label=\"Policy Consistency\", color='green', linewidth=2)\n",
    "plt.xlabel(\"Training Iteration\")\n",
    "plt.ylabel(\"Consistency (%)\")\n",
    "plt.title(\"Policy Consistency Over Training\")\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "# âœ… 4ï¸âƒ£ Expected Reward Growth (Checks if Learning is Improving)\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.plot(reward_per_episode, label=\"Expected Reward\", color='purple', marker=\"o\", linewidth=2)\n",
    "plt.xlabel(\"Training Iteration\")\n",
    "plt.ylabel(\"Total Discounted Reward\")\n",
    "plt.title(\"Cumulative Expected Reward Over Training\")\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "# âœ… 5ï¸âƒ£ Accuracy Evolution Across Trials\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.plot(accuracy_values, label=\"Accuracy\", marker=\"o\", color='orange', linewidth=2)\n",
    "plt.xlabel(\"Trial Number\")\n",
    "plt.ylabel(\"Prediction Accuracy (%)\")\n",
    "plt.title(\"RL Model Accuracy Over Trials\")\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "# âœ… 6ï¸âƒ£ Hyperparameter Sensitivity Analysis\n",
    "fig, axes = plt.subplots(1, 3, figsize=(18, 5))\n",
    "\n",
    "# ðŸŽ¯ Alpha vs Accuracy\n",
    "axes[0].scatter([p[\"alpha\"] for p in hyperparam_values], accuracy_values, color='blue', alpha=0.6)\n",
    "axes[0].set_xlabel(\"Alpha (Learning Rate)\")\n",
    "axes[0].set_ylabel(\"Prediction Accuracy (%)\")\n",
    "axes[0].set_title(\"Effect of Alpha on Accuracy\")\n",
    "\n",
    "# ðŸŽ¯ Gamma vs Accuracy\n",
    "axes[1].scatter(gamma_values, accuracy_values, color='red', alpha=0.6)\n",
    "axes[1].set_xlabel(\"Gamma (Discount Factor)\")\n",
    "axes[1].set_ylabel(\"Prediction Accuracy (%)\")\n",
    "axes[1].set_title(\"Effect of Gamma on Accuracy\")\n",
    "\n",
    "# ðŸŽ¯ Epsilon vs Accuracy\n",
    "# axes[2].scatter([p[\"epsilon\"] for p in hyperparam_values], accuracy_values, color='green', alpha=0.6)\n",
    "# axes[2].set_xlabel(\"Epsilon (Exploration Rate)\")\n",
    "# axes[2].set_ylabel(\"Prediction Accuracy (%)\")\n",
    "# axes[2].set_title(\"Effect of Epsilon on Accuracy\")\n",
    "\n",
    "# plt.tight_layout()\n",
    "# plt.show()\n",
    "\n",
    "# Apply rolling average smoothing for reward per episode\n",
    "# reward_per_episode_smoothed = pd.Series(reward_per_episode).rolling(window=10, min_periods=1).mean()\n",
    "\n",
    "# # âœ… 4ï¸âƒ£ Expected Reward Growth (Smoothed Version)\n",
    "# plt.figure(figsize=(10, 5))\n",
    "# plt.plot(reward_per_episode_smoothed, label=\"Smoothed Expected Reward\", color='purple', marker=\"o\", linewidth=2)\n",
    "# plt.xlabel(\"Training Iteration\")\n",
    "# plt.ylabel(\"Total Discounted Reward\")\n",
    "# plt.title(\"Smoothed Cumulative Expected Reward Over Training\")\n",
    "# plt.legend()\n",
    "# plt.show()\n",
    "\n",
    "\n",
    "# ==========================\n",
    "# ðŸ”¹ Plot Best Trial Results\n",
    "# ==========================\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.plot(best_trial['policy'], label=\"Best Policy Consistency\", color='green', linewidth=2)\n",
    "plt.xlabel(\"Training Iteration\")\n",
    "plt.ylabel(\"Consistency (%)\")\n",
    "plt.title(\"Policy Consistency Over Training (Best Trial)\")\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "\n",
    "plt.figure(figsize=(10, 5))\n",
    "colors =plt.cm.viridis(np.linspace(0, 1, len(rewards_per_eval)))  # Unique colors\n",
    "\n",
    "for i, (eval_id, rewards) in enumerate(rewards_per_eval.items()):\n",
    "    plt.plot(rewards, label=f\"Eval {eval_id + 1}\", color=colors[i], linewidth=2)\n",
    "\n",
    "plt.xlabel(\"Training Iteration\")\n",
    "plt.ylabel(\"Total Discounted Reward\")\n",
    "plt.title(\"Cumulative Expected Reward Over Training (All Evaluations)\")\n",
    "plt.legend()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Mathematical Formulation of Post-Depletion Trajectory Analysis**\n",
    "\n",
    "This analysis enables a detailed comparison of **actual vs predicted movement** following battery depletion, revealing:\n",
    "- How vehicles move after depletion.\n",
    "- The effectiveness of the **Markov-based prediction model**.\n",
    "- Patterns in vehicle trajectory shifts post-depletion.\n",
    "\n",
    "## **1. Extracting Unique Days of Battery Depletion**\n",
    "We define a unique day $ d $ as a calendar date where at least one vehicle experienced battery depletion:\n",
    "\n",
    "$$\n",
    "D = \\{ d \\mid \\exists i, SOC_{\\text{batt}, i} < 50\\%, \\text{on day } d \\}\n",
    "$$\n",
    "\n",
    "where $ D $ is the set of all days in which a depletion event occurred.\n",
    "\n",
    "## **2. Filtering Data for Each Depletion Day**\n",
    "For each $ d \\in D $, we extract:\n",
    "\n",
    "- **Post-depletion records**: \n",
    "  $$\n",
    "  X_d = \\{ x_i \\mid x_i \\in X, \\text{Timestamp}(x_i) = d, SOC_{\\text{batt}, i} < 50\\% \\}\n",
    "  $$\n",
    "\n",
    "- **Pre-depletion records**:\n",
    "  $$\n",
    "  Y_d = \\{ y_i \\mid y_i \\in Y, \\text{Timestamp}(y_i) = d, SOC_{\\text{batt}, i} \\geq 50\\% \\}\n",
    "  $$\n",
    "\n",
    "where:\n",
    "- $ X_d $ represents all vehicle records **after depletion**.\n",
    "- $ Y_d $ represents all vehicle records **before depletion**.\n",
    "\n",
    "## **3. Mapping Vehicles to Grid Cells**\n",
    "For each vehicle's recorded GPS point $ p_i $ on day $ d $:\n",
    "\n",
    "$$\n",
    "G_i = \\arg\\max_j \\mathbb{1}(p_i \\in P_j)\n",
    "$$\n",
    "\n",
    "where:\n",
    "- $ G_i $ is the assigned grid cell.\n",
    "- $ P_j $ represents the grid cells.\n",
    "- $ \\mathbb{1}(p_i \\in P_j) $ is an indicator function that is **1** if $ p_i $ is inside $ P_j $.\n",
    "\n",
    "Thus, we define:\n",
    "\n",
    "$$\n",
    "G_d^{\\text{actual}} = \\{ G_i \\mid x_i \\in X_d \\}\n",
    "$$\n",
    "\n",
    "$$\n",
    "G_d^{\\text{pre}} = \\{ G_i \\mid y_i \\in Y_d \\}\n",
    "$$\n",
    "\n",
    "## **4. Extracting Predicted Grid Cells**\n",
    "The predicted post-depletion grid cells for each vehicle are computed from the **Markov Transition Model**:\n",
    "\n",
    "$$\n",
    "G_{\\text{predicted}, i} = \\arg\\max_{G_j} P(G_j | G_i)\n",
    "$$\n",
    "\n",
    "for each vehicle location $ G_i $ before depletion.\n",
    "\n",
    "The predicted set is:\n",
    "\n",
    "$$\n",
    "G_d^{\\text{predicted}} = \\{ G_{\\text{predicted}, i} \\mid x_i \\in X_d \\}\n",
    "$$\n",
    "\n",
    "## **5. Visualizing the Spatial Trajectories**\n",
    "We generate a geospatial plot for each day $ d $:\n",
    "\n",
    "- **Pre-Depletion Trajectory** $ G_d^{\\text{pre}} $ (Black)\n",
    "- **Actual Post-Depletion Trajectory** $ G_d^{\\text{actual}} $ (Red)\n",
    "- **Predicted Trajectory** $ G_d^{\\text{predicted}} $ (Blue)\n",
    "\n",
    "Each grid cell is represented as a polygon, where:\n",
    "\n",
    "$$\n",
    "P_j = \\{ (x_k, y_k) \\mid k = 1,2,3,4 \\}\n",
    "$$\n",
    "\n",
    "and plotted based on its category:\n",
    "\n",
    "$$\n",
    "\\text{Color}(P_j) =\n",
    "\\begin{cases} \n",
    "\\text{black}, & P_j \\in G_d^{\\text{pre}} \\\\\n",
    "\\text{red}, & P_j \\in G_d^{\\text{actual}} \\\\\n",
    "\\text{blue}, & P_j \\in G_d^{\\text{predicted}}\n",
    "\\end{cases}\n",
    "$$\n",
    "\n",
    "## **6. Overlaying the OpenStreetMap Basemap**\n",
    "The plotted grid cells are projected onto a real-world **OpenStreetMap** (OSM) basemap with coordinate reference system:\n",
    "\n",
    "$$\n",
    "\\text{CRS} = \\text{EPSG:4326}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract unique days where depletion occurred\n",
    "depleted_days = df_post_depletion['Timestamp'].dt.date.unique()\n",
    "\n",
    "# Loop through each depleted day and generate a plot\n",
    "for day in depleted_days:\n",
    "    # Filter data for the current day\n",
    "    df_day = df_post_depletion[df_post_depletion['Timestamp'].dt.date == day]\n",
    "    df_pre_depletion_day = df_pre_depletion[df_pre_depletion['Timestamp'].dt.date == day]\n",
    "\n",
    "    # Convert actual, predicted, and pre-depletion data into GeoDataFrames\n",
    "    gdf_actual = grid_gdf.loc[grid_gdf.index.isin(df_day['Grid_Cell'])].copy()\n",
    "    gdf_actual['Color'] = 'red'\n",
    "\n",
    "    predicted_grid_cells = df_day['Predicted_Grid_Cell_Hybrid'].dropna().unique()\n",
    "    gdf_predicted = grid_gdf.loc[grid_gdf.index.isin(predicted_grid_cells)].copy()\n",
    "    gdf_predicted['Color'] = 'blue'\n",
    "\n",
    "    pre_depletion_grid_cells = df_pre_depletion_day['Grid_Cell'].unique()\n",
    "    gdf_pre_depletion = grid_gdf.loc[grid_gdf.index.isin(pre_depletion_grid_cells)].copy()\n",
    "    gdf_pre_depletion['Color'] = 'black'\n",
    "\n",
    "    # Skip plotting if all GeoDataFrames are empty for the day\n",
    "    if gdf_actual.empty and gdf_predicted.empty and gdf_pre_depletion.empty:\n",
    "        print(f\"Skipping {day}: No valid data for plotting.\")\n",
    "        continue\n",
    "\n",
    "    # Create the plot for the current day\n",
    "    fig, ax = plt.subplots(figsize=(12, 12))\n",
    "\n",
    "    # Plot Grid Cells\n",
    "    grid_gdf.plot(ax=ax, color='lightgrey', alpha=0.2)\n",
    "\n",
    "    # Plot Pre-Depletion Trajectory (Black) if not empty\n",
    "    if not gdf_pre_depletion.empty:\n",
    "        gdf_pre_depletion.plot(ax=ax, color='black', alpha=0.5, label=\"Pre-Depletion Trajectory\")\n",
    "\n",
    "    # Plot Actual Trajectory (Red) if not empty\n",
    "    if not gdf_actual.empty:\n",
    "        gdf_actual.plot(ax=ax, color='red', alpha=0.5, label=\"Actual Trajectory\")\n",
    "\n",
    "    # Plot Predicted Trajectory (Blue) if not empty\n",
    "    if not gdf_predicted.empty:\n",
    "        gdf_predicted.plot(ax=ax, color='blue', alpha=0.5, label=\"Predicted Trajectory\")\n",
    "\n",
    "    # Add Basemap\n",
    "    try:\n",
    "        ctx.add_basemap(ax, crs=grid_gdf.crs.to_string(), source=ctx.providers.CartoDB.Positron)\n",
    "    except Exception as e:\n",
    "        print(f\"Basemap Error on {day}: {e}\")\n",
    "\n",
    "    # Formatting\n",
    "    ax.set_title(f\"Actual vs Predicted Trajectory (Post-Depletion) - {day}\", fontsize=14)\n",
    "    ax.legend()\n",
    "    ax.set_axis_off()\n",
    "\n",
    "    # Show plot\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_temp = df.copy().reset_index(drop=True)  # Ensure indices are sequential\n",
    "df_temp=df_temp.sort_values(by=['Timestamp']).reset_index(drop=True) \n",
    "\n",
    "# Define different time thresholds to compare\n",
    "time_thresholds = {\n",
    "    # \"3 sec\": 3,\n",
    "    \"12 sec\": 12\n",
    "}\n",
    "\n",
    "# Create a dictionary to store SOC and sensor states for each threshold\n",
    "soc_depletion_results = {}\n",
    "\n",
    "# Iterate over different time thresholds\n",
    "for label, TIME_THRESHOLD in time_thresholds.items():\n",
    "\n",
    "    # Track last sensed timestamp, and stored energy during OFF periods\n",
    "    last_sensed_time = {}\n",
    "    stored_energy = {}\n",
    "\n",
    "    # Previous date\n",
    "    prev_date=None\n",
    "    \n",
    "    for i in range(len(df_temp)):\n",
    "        row = df_temp.iloc[i]\n",
    "        grid_key = (row['Lat_Grid'], row['Log_Grid'])  # Unique grid identifier\n",
    "        current_time = row['Timestamp']\n",
    "        current_date = row['Date']\n",
    "        device= row['deviceID']\n",
    "\n",
    "        # Initialise inter-row differences when OFF\n",
    "        d_diff_prev=0 \n",
    "        \n",
    "        # Reset stored energy at the start of a new day\n",
    "        if prev_date is not None and current_date != prev_date:\n",
    "            stored_energy={}  # Reset stored energy for all grid cells\n",
    "            df_temp.loc[i:, f'Energy_Saved_{label}'] = 0  # Reset energy savings for the new day\n",
    "            print(f\"[RESET] Reset stored energy for new day: {current_date}\")\n",
    "\n",
    "        prev_date = current_date  # Update previous date tracker\n",
    "\n",
    "        if df_temp.loc[i, 'SOC_batt']>99:\n",
    "            stored_energy[grid_key]=0\n",
    "            df_temp.loc[i:, f'Energy_Saved_{label}']=0\n",
    "\n",
    "        # If the grid was sensed recently (within the threshold), turn OFF the sensor\n",
    "        if grid_key in last_sensed_time and (current_time - last_sensed_time[grid_key]).total_seconds() < TIME_THRESHOLD:\n",
    "            df_temp.at[i, f'Sensor_ON_{label}'] = False   \n",
    "\n",
    "            # Accumulate stored energy\n",
    "            if i > 0 and pd.notna(df_temp.iloc[i - 1]['SOC_batt']) and pd.notna(row['SOC_batt']):\n",
    "            \n",
    "                # Find the last preceding row for this device\n",
    "                if device == df_temp.iloc[i-1]['deviceID']:\n",
    "                    d_diff = max(0, df_temp.iloc[i - 1]['SOC_batt'] - row['SOC_batt']) #current inter-row difference\n",
    "                    diff = max(d_diff_prev, d_diff)\n",
    "                    stored_energy[grid_key] = df_temp.loc[i-1, f'Energy_Saved_{label}']\n",
    "                    stored_energy[grid_key] += diff\n",
    "                    \n",
    "                    if diff != 0: \n",
    "                        df_temp.loc[i:, f'Energy_Saved_{label}'] = stored_energy[grid_key]\n",
    "                    else:\n",
    "                        df_temp.loc[i:, f'Energy_Saved_{label}'] = df_temp.loc[i-1, f'Energy_Saved_{label}']\n",
    "\n",
    "                    d_diff_prev=diff\n",
    "                    print(f\"[OFF]: Accumulated {diff:.2f}% for device {device}. Total stored: {stored_energy[grid_key]:.2f}%\")\n",
    "\n",
    "                else:\n",
    "                    d_diff = max(0, df_temp.loc[df_temp.deviceID == device, :]['SOC_batt'].iloc[-1] - row['SOC_batt']) #current inter-row difference\n",
    "                    diff = max(d_diff_prev, d_diff)\n",
    "                    stored_energy[grid_key] = df_temp.loc[df_temp.deviceID == device, :][f'Energy_Saved_{label}'].iloc[-1]\n",
    "                    stored_energy[grid_key] += diff\n",
    "                    \n",
    "                    if diff != 0: \n",
    "                        df_temp.loc[i:, f'Energy_Saved_{label}'] = stored_energy[grid_key]\n",
    "                    else:\n",
    "                        df_temp.loc[i:, f'Energy_Saved_{label}'] = df_temp.loc[df_temp.deviceID == device, :][f'Energy_Saved_{label}'].iloc[-1]\n",
    "\n",
    "                    d_diff_prev=diff\n",
    "                    print(f\"CHANGE [OFF]: Accumulated {diff:.2f}% for device {device}. Total stored: {stored_energy[grid_key]:.2f}%\")\n",
    "\n",
    "        else:\n",
    "            # Update last sensed time when the sensor turns ON\n",
    "            last_sensed_time[grid_key] = current_time\n",
    "            df_temp.at[i, f'Sensor_ON_{label}'] = True\n",
    "            d_diff_prev=0\n",
    "\n",
    "            if device == df_temp.iloc[i-1]['deviceID']:\n",
    "\n",
    "                # Ensure stored_energy is initialized per grid cell without overwriting previous values\n",
    "                if grid_key not in stored_energy:\n",
    "                    if i > 0:\n",
    "                        #Carry forward the stored energy from the last known row\n",
    "                        stored_energy[grid_key] = df_temp.loc[i-1, f'Energy_Saved_{label}']\n",
    "                        df_temp.loc[i:, f'Energy_Saved_{label}'] = stored_energy[grid_key]\n",
    "                    elif i == 0:\n",
    "                        stored_energy[grid_key] = 0  # First iteration, no prior energy \n",
    "                        df_temp.loc[i:, f'Energy_Saved_{label}'] = stored_energy[grid_key]  \n",
    "                print(f\"[ON]: device {device}. Total stored: {stored_energy[grid_key]:.2f}%\")\n",
    "\n",
    "            else:\n",
    "                 # Ensure stored_energy is initialized per grid cell without overwriting previous values\n",
    "                if grid_key not in stored_energy:\n",
    "                    if i > 0:\n",
    "                        #Carry forward the stored energy from the last known row\n",
    "                        stored_energy[grid_key] = df_temp.loc[df_temp.deviceID == device, :][f'Energy_Saved_{label}'].iloc[-1]\n",
    "                        df_temp.loc[i:, f'Energy_Saved_{label}'] = stored_energy[grid_key]\n",
    "                    elif i == 0:\n",
    "                        stored_energy[grid_key] = 0  # First iteration, no prior energy \n",
    "                        df_temp.loc[i:, f'Energy_Saved_{label}'] = stored_energy[grid_key]                 \n",
    "                \n",
    "                print(f\"CHANGE [ON]: device {device}. Total stored: {stored_energy[grid_key]:.2f}%\")\n",
    "\n",
    "    \n",
    "    # Compute new SOC_batt with savings\n",
    "    df_temp[f'SOC_batt_{label}'] = df_temp['SOC_batt'] + df_temp[f'Energy_Saved_{label}']\n",
    "    df_temp[f'SOC_batt_{label}'] = df_temp[f'SOC_batt_{label}'].clip(upper=100)\n",
    "\n",
    "    # Compute SOC depletion for this threshold\n",
    "    daily_soc = df_temp.groupby(['Date', 'deviceID'])[f'SOC_batt_{label}'].mean()\n",
    "    soc_depletion_results[label] = daily_soc\n",
    "\n",
    "\n",
    "# Baseline: Compute SOC depletion without constraints\n",
    "soc_depletion_results[\"Baseline\"] = df_temp.groupby(['Date', 'deviceID'])['SOC_batt'].mean()\n",
    "\n",
    "# Convert results to a DataFrame for plotting\n",
    "soc_depletion_df = pd.DataFrame(soc_depletion_results)\n",
    "\n",
    "# Save the updated dataset with sensor states and energy savings for each threshold\n",
    "output_path = \"/workspace/data/updated_SOC_batt_with_energy_savings.xlsx\"\n",
    "df_temp.to_excel(output_path, index=False)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define line styles and transparency levels for each threshold\n",
    "line_styles = {\n",
    "    \"Baseline\": \"--\",\n",
    "    # \"3 sec\": \"--\",\n",
    "    \"12 sec\": \"-\"\n",
    "}\n",
    "\n",
    "# Transparency levels for each threshold\n",
    "alpha_values = {\n",
    "    \"Baseline\": 0.5,  # 70% transparent\n",
    "    # \"3 sec\": 0.7,  # 30% transparent\n",
    "    \"12 sec\": 1   #Fully visible\n",
    "}\n",
    "\n",
    "# Predefined colors for devices\n",
    "predefined_colors = ['#007FFF', '#DC143C', '#FF4500','#39FF14', '#800080']\n",
    "device_ids = set()\n",
    "\n",
    "for soc_series in soc_depletion_results.values():\n",
    "    device_ids.update(soc_series.index.get_level_values('deviceID').unique())\n",
    "\n",
    "# Create a color map using predefined colors\n",
    "color_map = {device_id: predefined_colors[i % len(predefined_colors)] for i, device_id in enumerate(sorted(device_ids))}\n",
    "\n",
    "# Plot SOC depletion for different devices and thresholds\n",
    "plt.figure(figsize=(12, 6))\n",
    "\n",
    "# Iterate over thresholds and plot per device\n",
    "for label, soc_series in soc_depletion_results.items():  # soc_series is a MultiIndexed Series\n",
    "    for device_id in soc_series.index.get_level_values('deviceID').unique():  # Get unique devices\n",
    "        device_data = soc_series[soc_series.index.get_level_values('deviceID') == device_id]\n",
    "        plt.plot(\n",
    "            device_data.index.get_level_values('Date'),  # X-axis: Dates\n",
    "            device_data.values,  # Y-axis: SOC values\n",
    "            linestyle=line_styles[label],\n",
    "            color=color_map[device_id],  # Use predefined color for the device\n",
    "            # marker='o',\n",
    "            # markersize=3,\n",
    "            alpha=alpha_values[label],  # Apply transparency per threshold\n",
    "            label=f\"Device {device_id} - {label}\"\n",
    "        )\n",
    "\n",
    "plt.xlabel('Date')\n",
    "plt.ylabel('Mean SOC (%)')\n",
    "plt.title('SOC Depletion Comparison Across Devices and Time Constraints')\n",
    "\n",
    "# Place the legend outside the plot\n",
    "plt.legend(\n",
    "    bbox_to_anchor=(1.05, 1),  # Place legend to the right of the plot\n",
    "    loc='upper left',          # Align legend to the top-left of the bounding box\n",
    "    borderaxespad=0.           # Reduce spacing between the legend and the plot\n",
    ")\n",
    "plt.grid(True)\n",
    "plt.xticks(rotation=45)\n",
    "\n",
    "# Adjust layout to make room for the legend\n",
    "plt.tight_layout()\n",
    "\n",
    "# Show plot\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute the count of times the sensor was turned OFF for each constraint scenario\n",
    "off_counts = {}\n",
    "\n",
    "# Iterate over different time thresholds\n",
    "for label, TIME_THRESHOLD in time_thresholds.items():\n",
    "    df_copy = df.copy()  # Work on a copy of the dataset\n",
    "    df_copy['Sensor_ON'] = True  # Default: Sensor is ON\n",
    "\n",
    "    # Track last sensed timestamp per grid cell\n",
    "    last_sensed_time = {}\n",
    "    off_count = 0\n",
    "\n",
    "    for i, row in df_copy.iterrows():\n",
    "        grid_key = (row['Lat_Grid'], row['Log_Grid'])  # Unique grid identifier\n",
    "        current_time = row['Timestamp']\n",
    "\n",
    "        # If the grid was sensed recently (within the threshold), turn OFF the sensor\n",
    "        if grid_key in last_sensed_time and (current_time - last_sensed_time[grid_key]).total_seconds() < TIME_THRESHOLD:\n",
    "            df_copy.at[i, 'Sensor_ON'] = False\n",
    "            off_count += 1\n",
    "        else:\n",
    "            # Update last sensed time when the sensor turns ON\n",
    "            last_sensed_time[grid_key] = current_time\n",
    "\n",
    "    off_counts[label] = off_count\n",
    "\n",
    "# Convert to DataFrame for better visualization\n",
    "off_counts_df = pd.DataFrame.from_dict(off_counts, orient='index', columns=['Sensor OFF Count'])\n",
    "\n",
    "# Display results\n",
    "off_counts_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract unique days where depletion occurred\n",
    "depleted_days = df_post_depletion['Timestamp'].dt.date.unique()\n",
    "\n",
    "# Loop through each depleted day for visualization\n",
    "for day in depleted_days:\n",
    "    # Filter data for the current depleted day\n",
    "    df_day_pre = df_pre_coverage[df_pre_coverage['Date'] == day]\n",
    "    df_day_new = df_new_coverage_only[df_new_coverage_only['Date'] == day]\n",
    "    df_day_actual = df_post_depletion[df_post_depletion['Timestamp'].dt.date == day]\n",
    "    df_day_predicted = df_post_depletion[df_post_depletion['Timestamp'].dt.date == day]\n",
    "\n",
    "    # Convert to GeoDataFrames\n",
    "    gdf_pre = grid_gdf.loc[grid_gdf.index.isin(df_day_pre['Grid_Cell'])].copy()\n",
    "    gdf_pre['Color'] = 'black'  # Pre-depletion trajectory\n",
    "\n",
    "    gdf_new = grid_gdf.loc[grid_gdf.index.isin(df_day_new['Grid_Cell'])].copy()\n",
    "    gdf_new['Color'] = 'green'  # Newly sensed due to 10min rule\n",
    "\n",
    "    gdf_actual = grid_gdf.loc[grid_gdf.index.isin(df_day_actual['Grid_Cell'])].copy()\n",
    "    gdf_actual['Color'] = 'red'  # Actual trajectory after depletion\n",
    "\n",
    "    predicted_grid_cells = df_day_predicted['Predicted_Grid_Cell'].dropna().unique()\n",
    "    gdf_predicted = grid_gdf.loc[grid_gdf.index.isin(predicted_grid_cells)].copy()\n",
    "    gdf_predicted['Color'] = 'blue'  # Predicted trajectory\n",
    "\n",
    "    # Skip if no relevant data for the day\n",
    "    if gdf_pre.empty and gdf_new.empty and gdf_actual.empty and gdf_predicted.empty:\n",
    "        print(f\"Skipping {day}: No valid data for plotting.\")\n",
    "        continue\n",
    "\n",
    "    # Create the plot\n",
    "    fig, ax = plt.subplots(figsize=(12, 12))\n",
    "\n",
    "    # Plot Grid Cells (Background)\n",
    "    grid_gdf.plot(ax=ax, color='lightgrey', edgecolor='grey', alpha=0.2)\n",
    "\n",
    "    # Plot Pre-Depletion Trajectory (Black)\n",
    "    if not gdf_pre.empty:\n",
    "        gdf_pre.plot(ax=ax, color='black', alpha=0.5, edgecolor='black', label=\"Pre-Depletion Trajectory\")\n",
    "\n",
    "    # Plot Actual Post-Depletion Trajectory (Red)\n",
    "    if not gdf_actual.empty:\n",
    "        gdf_actual.plot(ax=ax, color='red', alpha=0.5, edgecolor='red', label=\"Actual Trajectory (Post-Depletion)\")\n",
    "\n",
    "    # Plot Predicted Post-Depletion Trajectory (Blue)\n",
    "    if not gdf_predicted.empty:\n",
    "        gdf_predicted.plot(ax=ax, color='blue', alpha=0.5, edgecolor='blue', label=\"Predicted Trajectory\")\n",
    "\n",
    "    # Plot Newly Sensed Cells Due to 10-Minute Rule (Green)\n",
    "    if not gdf_new.empty:\n",
    "        gdf_new.plot(ax=ax, color='green', alpha=0.5, edgecolor='green', label=\"Newly Sensed Cells (10-min Interval)\")\n",
    "\n",
    "    # Add Basemap\n",
    "    try:\n",
    "        ctx.add_basemap(ax, crs=grid_gdf.crs.to_string(), source=ctx.providers.OpenStreetMap.Mapnik)\n",
    "    except Exception as e:\n",
    "        print(f\"Basemap Error on {day}: {e}\")\n",
    "\n",
    "    # Formatting\n",
    "    ax.set_title(f\"Trajectory Visualization with 10-Minute Sensing Constraint - {day}\", fontsize=14)\n",
    "    ax.legend()\n",
    "    ax.set_axis_off()\n",
    "\n",
    "    # Show plot\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dynamically get the bounds from the data\n",
    "min_lat, max_lat = df['Lat'].min(), df['Lat'].max()\n",
    "min_lon, max_lon = df['Log'].min(), df['Log'].max()\n",
    "\n",
    "# Define grid size (120x120 meters)\n",
    "grid_size = 120\n",
    "lat_resolution = grid_size / 111320  # Convert meters to latitude degrees\n",
    "lon_resolution_at_lat = lambda lat: grid_size / (111320 * np.cos(np.radians(lat)))\n",
    "\n",
    "# Generate grid covering the dataset area\n",
    "grid = []\n",
    "lat = min_lat\n",
    "while lat < max_lat:\n",
    "    lon = min_lon\n",
    "    while lon < max_lon:\n",
    "        lon_res = lon_resolution_at_lat(lat)\n",
    "        grid.append(Polygon([\n",
    "            (lon, lat),\n",
    "            (lon + lon_res, lat),\n",
    "            (lon + lon_res, lat + lat_resolution),\n",
    "            (lon, lat + lat_resolution)\n",
    "        ]))\n",
    "        lon += lon_res\n",
    "    lat += lat_resolution\n",
    "\n",
    "# Create an empty GeoDataFrame for the grid\n",
    "grid_gdf = gpd.GeoDataFrame({'geometry': grid, 'Count': 0}, crs=\"EPSG:4326\")\n",
    "\n",
    "# Create a GeoDataFrame for the data points\n",
    "df_gdf = gpd.GeoDataFrame(df, geometry=gpd.points_from_xy(df['Log'], df['Lat']), crs=\"EPSG:4326\")\n",
    "\n",
    "# Assign each measurement to a grid square\n",
    "for index, point in df_gdf.iterrows():\n",
    "    match = grid_gdf.contains(point.geometry)\n",
    "    if match.any():\n",
    "        grid_gdf.loc[match.idxmax(), 'Count'] += 1\n",
    "\n",
    "# Apply Fractional Power Scaling\n",
    "gamma = 0.3  # Adjust for visibility\n",
    "grid_gdf['Scaled_Count'] = (grid_gdf['Count'] + 1) ** gamma\n",
    "\n",
    "# Normalize values for color mapping\n",
    "norm = Normalize(vmin=grid_gdf['Scaled_Count'].min(), vmax=grid_gdf['Scaled_Count'].max())\n",
    "cmap = plt.get_cmap('jet')\n",
    "\n",
    "# Convert scaled values to hex colors\n",
    "grid_gdf['Color'] = grid_gdf['Scaled_Count'].apply(lambda x: to_hex(cmap(norm(x))))\n",
    "\n",
    "# Create Folium map centered on Stockholm\n",
    "m = folium.Map(location=[df['Lat'].mean(), df['Log'].mean()], zoom_start=12, tiles='Cartodb dark_matter')\n",
    "\n",
    "# Function to color the grid based on scaled counts\n",
    "def style_function(feature):\n",
    "    color = feature['properties']['Color']  # Get precomputed color\n",
    "    return {\n",
    "        'fillColor': color,\n",
    "        'color': 'black',\n",
    "        'weight': 0.1,\n",
    "        'fillOpacity': 0.4\n",
    "    }\n",
    "\n",
    "# Add grid layer to Folium\n",
    "folium.GeoJson(\n",
    "    grid_gdf,\n",
    "    name=\"Measurement Grid\",\n",
    "    style_function=style_function,\n",
    "    tooltip=folium.GeoJsonTooltip(fields=['Count'], aliases=[\"Measurements:\"])\n",
    ").add_to(m)\n",
    "\n",
    "# Add layer control\n",
    "folium.LayerControl().add_to(m)\n",
    "\n",
    "# Display the map\n",
    "m\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

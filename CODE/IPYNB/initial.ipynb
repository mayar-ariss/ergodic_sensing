{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Open poweshell in new terminal and run\n",
    "\n",
    "docker build -t sensing-whale \"C:\\Users\\mayar\\OneDrive - Massachusetts Institute of Technology\\Desktop\\energy-aware\"\n",
    "\n",
    "After building the image, use -v to mount the local DATA directory inside /workspace/data/ in the container:\n",
    "\n",
    "docker run -it --gpus all --shm-size=1g --ulimit memlock=-1 --ulimit stack=67108864 -v \"C:\\Users\\mayar\\OneDrive - Massachusetts Institute of Technology\\Desktop\\energy-aware\\DATA:/workspace/data\" -p 8888:8888 sensing-whale"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "to detect docker containers run: docker ps\n",
    "\n",
    "to stop docker container: docker stop 'insert container name'\n",
    "\n",
    "to delete docker container: docker rm 'insert container name'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "print(os.getcwd())  \n",
    "print(os.listdir())  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cudf\n",
    "import cuml\n",
    "import cupy as cp\n",
    "import cuspatial\n",
    "\n",
    "!nvidia-smi\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imported Libraries Overview\n",
    "\n",
    "This cell imports essential Python libraries for geospatial analysis, data visualization, and statistical modeling:\n",
    "\n",
    "- **Data Handling & Computation**: `numpy` (`np`), `pandas` (`pd`), `collections.defaultdict`\n",
    "- **Geospatial Analysis**: `geopandas` (`gpd`), `shapely.geometry` (`Polygon`, `Point`), `contextily` (`ctx`), `folium`\n",
    "- **Visualization**: `matplotlib.pyplot` (`plt`), `seaborn` (`sns`), `matplotlib.colors` (`Normalize`, `to_hex`)\n",
    "- **Statistical Analysis**: `scipy.stats`, `scipy.optimize.curve_fit`, `kstest`\n",
    "- **Date & Time**: `datetime.datetime`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "import contextily as ctx\n",
    "from datetime import datetime\n",
    "import folium\n",
    "import geopandas as gpd\n",
    "from matplotlib.colors import Normalize, to_hex\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import scipy.stats as stats\n",
    "from scipy.optimize import curve_fit\n",
    "from scipy.stats import kstest\n",
    "from shapely.geometry import Polygon, Point\n",
    "import seaborn as sns\n",
    "import shapely\n",
    "import branca.colormap as cm\n",
    "import matplotlib.colors as mcolors\n",
    "import branca.colormap as cm\n",
    "import matplotlib.colors as mcolors\n",
    "import cudf\n",
    "import cupy as cp\n",
    "import cuspatial\n",
    "import random\n",
    "from hyperopt import fmin, tpe, hp, STATUS_OK, Trials\n",
    "from tqdm import tqdm  # Import tqdm for the progress bar\n",
    "import xgboost as xgb\n",
    "import optuna\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error\n",
    "import logging\n",
    "import numba\n",
    "import multiprocessing as mp\n",
    "from shapely.geometry import Point\n",
    "import optuna.visualization\n",
    "\n",
    "print(\"RAPIDS & required libraries loaded successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Cleaning and Preprocessing\n",
    "\n",
    "Loads and process multi-sheet Excel data\n",
    "\n",
    "1. **File Loading**: Reads all sheets from `2022_vitals.xlsx` without headers.\n",
    "2. **Column Naming**: Assigns predefined column names for consistency.\n",
    "3. **Data Alignment**: \n",
    "   - Fixes misaligned rows by detecting valid `deviceID`.\n",
    "   - Ensures all rows have the correct number of columns.\n",
    "4. **Filtering**:\n",
    "   - Removes invalid or duplicate header rows.\n",
    "   - Drops rows with zero values for latitude (`Lat`) and longitude (`Log`).\n",
    "5. **Indexing**: Resets the index and assigns a sequential 1-based index.\n",
    "6. **Output**: Saves the cleaned data to `2022_vitals_cleaned.xlsx` and previews it."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Mount the local data directory to Docker;\n",
    "docker run -it --gpus all --shm-size=1g --ulimit memlock=-1 --ulimit stack=67108864 -p 8888:8888 `\n",
    "    -v \"C:\\Users\\mayar\\OneDrive - Massachusetts Institute of Technology\\Desktop\\energy-aware\\DATA:/workspace/data\" `\n",
    "    rapids-custom-container\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adjusted file path for Docker (mounted volume)\n",
    "file_path = \"/workspace/data/2022_vitals.xlsx\"\n",
    "output_path = \"/workspace/data/2022_vitals_cleaned.xlsx\"\n",
    "\n",
    "# Specify the column names explicitly\n",
    "column_names = [\n",
    "    \"deviceID\", \"Timestamp\", \"Lat\", \"Log\", \"SOC_batt\", \"temp_batt\", \"volatge_batt\",\n",
    "    \"voltage_particle\", \"current_batt\", \"isCharging\", \"isCharginS\", \"isCharged\",\n",
    "    \"Temp_int\", \"Hum_int\", \"solar_current\", \"Cellular_signal_strength\", \"index\"\n",
    "]\n",
    "\n",
    "# Load all sheets into a dictionary\n",
    "sheets_dict = pd.read_excel(file_path, sheet_name=None, header=None)  # No header initially\n",
    "\n",
    "# Process each sheet\n",
    "processed_sheets = []\n",
    "for sheet_name, sheet_data in sheets_dict.items():\n",
    "    # Ensure the number of columns matches the expected number\n",
    "    sheet_data = sheet_data.iloc[:, :len(column_names)]\n",
    "\n",
    "    # Fix misaligned rows where the first column is invalid\n",
    "    def fix_alignment(row):\n",
    "        # Convert the row to a list\n",
    "        row_list = row.tolist()\n",
    "\n",
    "        # Find the first valid `deviceID` (assumes valid `deviceID` has > 5 characters)\n",
    "        for i, value in enumerate(row_list):\n",
    "            if isinstance(value, str) and len(value) > 5:  # Valid `deviceID` found\n",
    "                # Align the row starting from the valid `deviceID`\n",
    "                aligned_row = row_list[i:i + len(column_names)]\n",
    "                # Ensure the row is padded or trimmed to match `column_names`\n",
    "                return aligned_row + [None] * (len(column_names) - len(aligned_row))\n",
    "\n",
    "        # If no valid `deviceID` is found, return a row of NaN\n",
    "        return [None] * len(column_names)\n",
    "\n",
    "    # Apply alignment fix to all rows\n",
    "    sheet_data = sheet_data.apply(fix_alignment, axis=1, result_type=\"expand\")\n",
    "    \n",
    "    # Assign column names\n",
    "    sheet_data.columns = column_names\n",
    "\n",
    "    # Drop rows where 'deviceID' is still invalid or starts with \"deviceID\"\n",
    "    sheet_data = sheet_data[sheet_data['deviceID'].notna()]\n",
    "    sheet_data = sheet_data[sheet_data['deviceID'] != \"deviceID\"]  # Remove rows starting with \"deviceID\"\n",
    "\n",
    "    # Append processed sheet\n",
    "    processed_sheets.append(sheet_data)\n",
    "\n",
    "# Concatenate all sheets into one DataFrame\n",
    "df = pd.concat(processed_sheets, ignore_index=True)\n",
    "\n",
    "# Drop rows where Lat or Log is 0\n",
    "df = df[(df['Lat'] != 0) & (df['Log'] != 0)]\n",
    "\n",
    "# Correct the indexing column to start at 1 and increment sequentially\n",
    "df.reset_index(drop=True, inplace=True)  # Reset pandas index\n",
    "df['index'] = df.index + 1  # Create a 1-based index\n",
    "\n",
    "# Save the cleaned data back to Excel\n",
    "df.to_excel(output_path, index=False)\n",
    "\n",
    "# Print the cleaned data preview\n",
    "print(\"âœ… Data cleaning completed. Saved to:\", output_path)\n",
    "print(df.head())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Spatiotemporal Binning and Stationary Period Detection**\n",
    "\n",
    "This enables **spatial binning**, **stationary period detection**, and **temporal filtering** for robust movement analysis.\n",
    "\n",
    "## **1. Timestamp Conversion**\n",
    "The Unix timestamp $ T_i $ is converted into a standard datetime format:\n",
    "\n",
    "$$\n",
    "T_i^{\\text{datetime}} = T_i^{\\text{unix}} \\times \\frac{1}{86400} + \\text{epoch}\n",
    "$$\n",
    "\n",
    "where:\n",
    "- $ T_i^{\\text{unix}} $ is the raw Unix timestamp in **seconds**,\n",
    "- $ 86400 $ seconds = **1 day**,\n",
    "- **Epoch** is the reference starting time (January 1, 1970).\n",
    "\n",
    "## **2. Ensuring Numeric Latitude and Longitude**\n",
    "We enforce that latitude ($ \\text{Lat} $) and longitude ($ \\text{Log} $) are real-valued:\n",
    "\n",
    "$$\n",
    "\\text{Lat}, \\text{Log} \\in \\mathbb{R}\n",
    "$$\n",
    "\n",
    "Non-numeric values are coerced to **NaN**.\n",
    "\n",
    "## **3. Discretization into a 120m Grid**\n",
    "### **3.1 Latitude Grid Resolution**\n",
    "Since the **Earth's meridional circumference** is approximately **40,030 km**, the degree-to-meter conversion near the equator is:\n",
    "\n",
    "$$\n",
    "1^\\circ \\approx 111,320 \\text{ meters}\n",
    "$$\n",
    "\n",
    "Thus, the spatial resolution of a **120m grid** in latitude is:\n",
    "\n",
    "$$\n",
    "\\Delta \\text{Lat} = \\frac{120}{111320}\n",
    "$$\n",
    "\n",
    "The **grid-aligned latitude** is computed as:\n",
    "\n",
    "$$\n",
    "\\text{Lat\\_Grid} = \\left\\lfloor \\frac{\\text{Lat}}{\\Delta \\text{Lat}} \\right\\rfloor \\times \\Delta \\text{Lat}\n",
    "$$\n",
    "\n",
    "### **3.2 Longitude Grid Resolution**\n",
    "Unlike latitude, **longitude spacing** varies with latitude due to Earthâ€™s curvature. The **longitude degree-to-meter conversion** is:\n",
    "\n",
    "$$\n",
    "1^\\circ \\approx 111320 \\times \\cos(\\text{Lat})\n",
    "$$\n",
    "\n",
    "Thus, the **longitude resolution** at a given latitude is:\n",
    "\n",
    "$$\n",
    "\\Delta \\text{Log} = \\frac{120}{111320 \\cos(\\text{Lat})}\n",
    "$$\n",
    "\n",
    "The **grid-aligned longitude** is computed as:\n",
    "\n",
    "$$\n",
    "\\text{Log\\_Grid} = \\left\\lfloor \\frac{\\text{Log}}{\\Delta \\text{Log}} \\right\\rfloor \\times \\Delta \\text{Log}\n",
    "$$\n",
    "\n",
    "## **4. Sorting by Time and Device**\n",
    "To track movement **chronologically** for each vehicle, we sort:\n",
    "\n",
    "$$\n",
    "\\text{Sort}(df, \\text{by}=[\\text{deviceID}, \\text{Timestamp}])\n",
    "$$\n",
    "\n",
    "## **5. Identifying Stationary Periods**\n",
    "For each vehicle, we determine if it remained in the same grid cell over consecutive timestamps:\n",
    "\n",
    "$$\n",
    "\\text{Same\\_Grid}_i =\n",
    "\\begin{cases} \n",
    "1, & (\\text{Lat\\_Grid}_i = \\text{Lat\\_Grid}_{i-1}) \\land (\\text{Log\\_Grid}_i = \\text{Log\\_Grid}_{i-1}) \\\\\n",
    "0, & \\text{otherwise}\n",
    "\\end{cases}\n",
    "$$\n",
    "\n",
    "where:\n",
    "- $ \\text{Same\\_Grid}_i = 1 $ means no movement occurred.\n",
    "- $ \\text{Same\\_Grid}_i = 0 $ means movement occurred.\n",
    "\n",
    "## **6. Computing Time Spent in a Grid Cell**\n",
    "The time difference between consecutive records within the same grid is:\n",
    "\n",
    "$$\n",
    "\\Delta t_i = T_i - T_{i-1}\n",
    "$$\n",
    "\n",
    "The **total duration** a vehicle spends within a specific grid cell before moving is:\n",
    "\n",
    "$$\n",
    "\\text{Cumulative\\_Time}_{i} = \\sum_{k=1}^{i} \\Delta t_k\n",
    "$$\n",
    "\n",
    "where:\n",
    "- The summation continues **until movement occurs**.\n",
    "\n",
    "## **7. Assigning a Group ID to Each Stationary Period**\n",
    "A **unique group identifier** is assigned to each stationary period using a cumulative sum:\n",
    "\n",
    "$$\n",
    "\\text{Group}_i =\n",
    "\\sum_{j=1}^{i} (1 - \\text{Same\\_Grid}_j)\n",
    "$$\n",
    "\n",
    "Each transition into a **new grid cell** increments the group ID.\n",
    "\n",
    "## **8. Removing Prolonged Stationary Vehicles**\n",
    "Vehicles remaining in the **same grid for over 3 hours** (10,800 seconds) are excluded:\n",
    "\n",
    "$$\n",
    "\\text{Remove } i \\text{ if } \\text{Cumulative\\_Time}_i \\geq 10,800 \\text{ sec}\n",
    "$$\n",
    "\n",
    "## **9. Cleanup**\n",
    "All intermediate columns used for calculations are dropped to optimize storage.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert Unix timestamp to datetime\n",
    "df['Timestamp'] = pd.to_datetime(df['Timestamp'], unit='s')\n",
    "\n",
    "# Ensure 'Lat' and 'Log' are numeric\n",
    "df['Lat'] = pd.to_numeric(df['Lat'], errors='coerce')\n",
    "df['Log'] = pd.to_numeric(df['Log'], errors='coerce')\n",
    "\n",
    "# Spatial Resolution: 40m grid\n",
    "grid_size=40\n",
    "lat_resolution = grid_size / 111320 \n",
    "df['Lat_Grid'] = (df['Lat'] // lat_resolution) * lat_resolution\n",
    "\n",
    "# Longitude resolution depends on latitude\n",
    "df['Lon_Resolution'] = grid_size / (111320 * np.cos(np.radians(df['Lat'])))\n",
    "df['Log_Grid'] = (df['Log'] // df['Lon_Resolution']) * df['Lon_Resolution']\n",
    "\n",
    "# Drop auxiliary column\n",
    "df = df.drop(columns=['Lon_Resolution'])\n",
    "\n",
    "# Step 1: Sort by deviceID and Timestamp\n",
    "df = df.sort_values(by=['deviceID', 'Timestamp'])\n",
    "\n",
    "# Step 2: Detect continuous stationary periods\n",
    "df['Prev_Lat_Grid'] = df.groupby('deviceID')['Lat_Grid'].shift(1)\n",
    "df['Prev_Log_Grid'] = df.groupby('deviceID')['Log_Grid'].shift(1)\n",
    "df['Prev_Timestamp'] = df.groupby('deviceID')['Timestamp'].shift(1)\n",
    "\n",
    "# Step 3: Identify whether the taxi has stayed in the same grid\n",
    "df['Same_Grid'] = (df['Lat_Grid'] == df['Prev_Lat_Grid']) & (df['Log_Grid'] == df['Prev_Log_Grid'])\n",
    "\n",
    "# Step 4: Compute time spent in the grid continuously\n",
    "df['Time_Diff'] = (df['Timestamp'] - df['Prev_Timestamp']).dt.total_seconds()\n",
    "\n",
    "# Step 5: Assign a group ID that resets when the taxi leaves a grid\n",
    "df['Group'] = (~df['Same_Grid']).cumsum()\n",
    "\n",
    "# Step 6: Compute total time spent in each visit to the grid\n",
    "df['Cumulative_Time'] = df.groupby(['deviceID', 'Lat_Grid', 'Log_Grid', 'Group'])['Time_Diff'].cumsum()\n",
    "\n",
    "# Step 7: Remove vehicles that stayed continuously in the same grid for more than 3hours (10800 sec)\n",
    "df = df[~(df['Cumulative_Time'] >= 10800)]\n",
    "\n",
    "# Drop helper columns\n",
    "df = df.drop(columns=['Prev_Lat_Grid', 'Prev_Log_Grid', 'Prev_Timestamp', 'Same_Grid', 'Time_Diff', 'Group', 'Cumulative_Time'])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Spatial Grid Aggregation\n",
    "\n",
    "This script generates a **30m x 30m geospatial grid** and counts the number of data points within each grid cell:\n",
    "\n",
    "1. **Dynamic Boundary Definition**: \n",
    "   - Extracts min/max latitude and longitude from the dataset.\n",
    "2. **Grid Construction**:\n",
    "   - Defines **30m resolution** for latitude and dynamically calculates longitude resolution.\n",
    "   - Iterates over the spatial extent to generate **polygonal grid cells**.\n",
    "3. **GeoDataFrame Creation**:\n",
    "   - Converts the grid into a `GeoDataFrame` (`grid_gdf`).\n",
    "   - Converts data points into a `GeoDataFrame` (`df_gdf`).\n",
    "4. **Spatial Aggregation**:\n",
    "   - Checks which points fall within each grid cell.\n",
    "   - Increments the count of data points within corresponding grid polygons."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dynamically define the bounds from the DataFrame\n",
    "min_lat = df['Lat'].min()\n",
    "max_lat = df['Lat'].max()\n",
    "min_lon = df['Log'].min()\n",
    "max_lon = df['Log'].max()\n",
    "\n",
    "# Define grid size (40x40 meters)\n",
    "lon_resolution_at_lat = lambda lat: grid_size / (111320 * np.cos(np.radians(lat)))\n",
    "\n",
    "# Generate grid of polygons\n",
    "grid = []\n",
    "lat = min_lat\n",
    "while lat < max_lat:\n",
    "    lon = min_lon\n",
    "    while lon < max_lon:\n",
    "        lon_res = lon_resolution_at_lat(lat)\n",
    "        grid.append(Polygon([\n",
    "            (lon, lat),\n",
    "            (lon + lon_res, lat),\n",
    "            (lon + lon_res, lat + lat_resolution),\n",
    "            (lon, lat + lat_resolution)\n",
    "        ]))\n",
    "        lon += lon_res\n",
    "    lat += lat_resolution\n",
    "\n",
    "# Create an empty GeoDataFrame for the grid\n",
    "grid_gdf = gpd.GeoDataFrame({'geometry': grid, 'Count': 0}, crs=\"EPSG:4326\")\n",
    "\n",
    "# Create a GeoDataFrame for the points in df\n",
    "df_gdf = gpd.GeoDataFrame(df, geometry=gpd.points_from_xy(df['Log'], df['Lat']), crs=\"EPSG:4326\")\n",
    "\n",
    "# Perform spatial join (vectorized operation)\n",
    "joined = gpd.sjoin(df_gdf, grid_gdf, how=\"left\", predicate=\"within\")\n",
    "\n",
    "# Count points per grid cell\n",
    "grid_gdf['Count'] = joined.groupby(joined.index_right).size()\n",
    "\n",
    "# Fill NaN with 0 for empty grid cells\n",
    "grid_gdf['Count'].fillna(0, inplace=True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Adaptive Gamma Correction for Spatial Data Visualization**\n",
    "\n",
    "This implementation refines the **visual representation of geospatial density** by applying **adaptive gamma scaling** to the recorded data counts within a **120m Ã— 120m grid**. The goal is to **enhance contrast** in data representation by dynamically adjusting the transformation based on the **coefficient of variation (CV)**.\n",
    "\n",
    "## **1. Coefficient of Variation (CV) and Adaptive Gamma**\n",
    "To ensure that the **scaling transformation** is adaptive to the datasetâ€™s variability, the **coefficient of variation (CV)** is computed:\n",
    "\n",
    "$$\n",
    "CV = \\frac{\\sigma}{\\mu}\n",
    "$$\n",
    "\n",
    "where $ \\mu $ is the **mean** count of data points across all grid cells, and $ \\sigma $ is the **standard deviation** of these counts.\n",
    "\n",
    "To prevent division by zero, an exception is handled: if the mean is zero, $ CV $ is set to 1.\n",
    "\n",
    "An **adaptive gamma correction factor** is then defined:\n",
    "\n",
    "$$\n",
    "\\gamma_{\\text{opt}} = \\frac{1}{1 + CV}\n",
    "$$\n",
    "\n",
    "Gamma correction adjusts the **nonlinear contrast enhancement**, where:\n",
    "- If **data variance is high** ($ CV \\gg 1 $), the gamma correction **reduces distortion** and avoids over-enhancement of outliers.\n",
    "- If **data variance is low** ($ CV \\approx 0 $), the correction behaves linearly, maintaining **uniform scaling**.\n",
    "\n",
    "The **gamma-transformed scaled count** is computed as:\n",
    "\n",
    "$$\n",
    "S_i = (\\text{Count}_i + 1)^{\\gamma_{\\text{opt}}}\n",
    "$$\n",
    "\n",
    "where $ S_i $ is the **scaled count** for grid cell $ i $, and **adding 1** prevents the transformation from collapsing near zero.\n",
    "\n",
    "## **2. Colormap Design and Visualization**\n",
    "A **color scale** is applied using `branca.colormap`, ranging from **dark blue to orange-red**, where:\n",
    "- **Dark blue** represents **low densities** of recorded data.\n",
    "- **Cyan and yellow** indicate **moderate densities**.\n",
    "- **Orange-red** signifies **high-density regions**.\n",
    "\n",
    "## **3. Folium Map with Geospatial Overlay**\n",
    "A **dark-themed base map** (`CartoDB dark_matter`) is initialized at the dataset's mean latitude and longitude. The **scaled count values** are mapped using a `GeoJson` overlay, applying the **adaptive color scale dynamically** based on grid cell properties.\n",
    "\n",
    "## **4. Custom Legend Implementation**\n",
    "To improve interpretability, a **custom legend** is integrated into the map UI. This legend:\n",
    "- Uses **white text** for readability against the dark background.\n",
    "- Matches the **color scale mapping** with corresponding intensity levels (Low, Medium-Low, Medium-High, High).\n",
    "- Ensures a **fixed position** on the interface for clear reference.\n",
    "\n",
    "The final map displays **spatial density variations** with adaptive contrast enhancement, enabling **better pattern recognition** in data distributions.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grid_gdf_baseline = grid_gdf.copy()\n",
    "\n",
    "grid_gdf_baseline['Count_Baseline'] = np.log1p(grid_gdf_baseline['Count'])\n",
    "\n",
    "mean_count_baseline = grid_gdf_baseline['Count_Baseline'].mean()\n",
    "std_count_baseline = grid_gdf_baseline['Count_Baseline'].std()\n",
    "CV_baseline = std_count_baseline / mean_count_baseline if mean_count_baseline > 0 else 1\n",
    "baseline_gamma = 1 / (1 + CV_baseline)\n",
    "\n",
    "# Apply gamma scaling\n",
    "grid_gdf_baseline['Scaled_Count_Baseline'] = (grid_gdf_baseline['Count_Baseline'] + 1) ** baseline_gamma\n",
    "\n",
    "print(f\"Adaptive Optimal Gamma: {baseline_gamma:.4f}\")\n",
    "\n",
    "# Define a colormap\n",
    "colormap = cm.LinearColormap(\n",
    "    colors=['darkblue', 'cyan', 'yellow', 'orangered'],\n",
    "    vmin=grid_gdf_baseline['Scaled_Count_Baseline'].min(),\n",
    "    vmax=grid_gdf_baseline['Scaled_Count_Baseline'].max()\n",
    ")\n",
    "\n",
    "# Create folium map with a dark basemap\n",
    "m = folium.Map(location=[df['Lat'].mean(), df['Log'].mean()], zoom_start=12, tiles='Cartodb dark_matter')\n",
    "\n",
    "# Add GeoJSON overlay\n",
    "geojson = folium.GeoJson(\n",
    "    grid_gdf_baseline,\n",
    "    style_function=lambda feature: {\n",
    "        'fillColor': colormap(feature['properties']['Scaled_Count_Baseline']),\n",
    "        'weight': 0.0,\n",
    "        'fillOpacity': 0.4\n",
    "    }\n",
    ").add_to(m)\n",
    "\n",
    "colormap.caption = \"Log Scaled Count Intensity\"\n",
    "colormap.add_to(m)  # Attach to the map\n",
    "\n",
    "# Show map\n",
    "m\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sampling Frequency Analysis in High-Density Grid Cells\n",
    "\n",
    "This script analyzes the **sampling frequency** in the **top 10 most frequently sensed grid cells**:\n",
    "\n",
    "1. **Identify High-Activity Areas**:\n",
    "   - Selects the **top 10 grid cells** with the highest measurement counts.\n",
    "\n",
    "2. **Filter Relevant Data**:\n",
    "   - Extracts sensor readings from these **top 10 grid cells**.\n",
    "\n",
    "3. **Compute Time Differences**:\n",
    "   - Calculates the time intervals between consecutive measurements per `deviceID` within each grid cell.\n",
    "\n",
    "4. **Estimate Sampling Frequency**:\n",
    "   - Computes the **mean sampling interval** per grid cell.\n",
    "   - Converts it to **sampling frequency (Hz)** as **1 / mean time difference**.\n",
    "\n",
    "5. **Extract Top Sampling Frequencies**:\n",
    "   - Selects the **10 highest sampling frequencies** for analysis.\n",
    "   - Converts results into a structured DataFrame for better readability.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select the top 10 most frequently sensed grid cells\n",
    "top_10_cells = grid_gdf.nlargest(10, 'Count')\n",
    "\n",
    "# Filter the data for points within these top 10 grid cells\n",
    "df_top_cells = df_gdf[df_gdf.geometry.apply(lambda point: any(top_10_cells.contains(point)))].copy()  # Make a copy\n",
    "\n",
    "# Compute the time differences for each device in each grid cell\n",
    "df_top_cells['Time_Diff'] = df_top_cells.groupby(['Lat_Grid', 'Log_Grid', 'deviceID'])['Timestamp'].diff().dt.total_seconds()\n",
    "\n",
    "# Calculate the mean sampling frequency (1 / mean time difference) per grid cell\n",
    "sampling_frequency = df_top_cells.groupby(['Lat_Grid', 'Log_Grid'])['Time_Diff'].mean().dropna().apply(lambda x: 1 / x)\n",
    "top_10_frequencies = sampling_frequency.nlargest(10)\n",
    "\n",
    "# Convert to DataFrame for better visualization\n",
    "top_10_frequencies_df = top_10_frequencies.reset_index()\n",
    "top_10_frequencies_df.columns = ['Lat_Grid', 'Log_Grid', 'Sampling_Frequency (Hz)']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Battery Depletion\n",
    "\n",
    "Identify the **number of unique days** where at least one deviceâ€™s battery **(SOC_batt)** dropped below **50%**:\n",
    "\n",
    "1. **Extract Date Information**:\n",
    "   - Converts the timestamp to **date-only format**.\n",
    "\n",
    "2. **Filter for Battery Depletion Events**:\n",
    "   - Selects records where `SOC_batt` is below **50%** (can be changed).\n",
    "\n",
    "3. **Count Unique Affected Days**:\n",
    "   - Computes the number of distinct days where a depletion event occurred."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assuming df is already loaded with the necessary data\n",
    "# Identify unique days where at least one device's SOC_batt dropped below 50%\n",
    "df['Date'] = df['Timestamp'].dt.date  # Extract the date\n",
    "days_with_depletion = df[df['SOC_batt'] < 50]['Date'].nunique()\n",
    "\n",
    "# Display the number of days with a battery drop below 50%\n",
    "days_with_depletion\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **XGBoost-Based SOC Forecasting with Optuna Hyperparameter Optimization**\n",
    "\n",
    "This implementation builds a **data-driven model** for **predicting battery State of Charge (SOC) depletion** in sensor-equipped vehicles. The model:\n",
    "1. **Extracts spatiotemporal and power-related features** from historical sensor data.\n",
    "2. **Trains an XGBoost regressor** to predict future SOC values.\n",
    "3. **Optimizes model hyperparameters** using Bayesian search via **Optuna**.\n",
    "4. **Performs multi-step forecasting**, predicting SOC for the next **seven time steps**.\n",
    "5. **Quantifies predictive uncertainty** and **adjusts dynamic SOC thresholds** for safety monitoring.\n",
    "\n",
    "## **1. Data Preprocessing and Feature Engineering**\n",
    "\n",
    "The dataset contains time-series measurements for multiple vehicles, each identified by a **deviceID**. The dataset is first sorted **chronologically** for each device:\n",
    "\n",
    "$$\n",
    "\\text{Sort}(df, \\text{by}=[\\text{deviceID}, \\text{Timestamp}])\n",
    "$$\n",
    "\n",
    "SOC and related features are converted to **numeric types** to ensure proper mathematical operations:\n",
    "\n",
    "$$\n",
    "X = \\{\\text{SOC\\_batt}, \\text{current\\_batt}, \\text{solar\\_current}, \\text{voltage\\_batt}, \\text{charging\\_status} \\}\n",
    "$$\n",
    "\n",
    "### **Feature Engineering**\n",
    "New predictive features are created, capturing both **short-term trends** and **time-based influences**:\n",
    "\n",
    "- **Hourly time representation:** $ \\text{Hour} = \\text{Timestamp.hour} $, capturing **daily charge-discharge patterns**.\n",
    "- **Lagged values:** Previous SOC and power readings are used as predictors:\n",
    "\n",
    "$$\n",
    "\\text{Prev\\_SOC}_t = \\text{SOC\\_batt}_{t-1}, \\quad \\text{Prev\\_Current}_t = \\text{current\\_batt}_{t-1}\n",
    "$$\n",
    "\n",
    "- **Rolling depletion rate:** Defined as the moving average of SOC depletion over a 5-step window:\n",
    "\n",
    "$$\n",
    "\\text{Depletion\\_Rate}_t = \\frac{1}{5} \\sum_{i=t-4}^{t} (\\text{SOC\\_batt}_i - \\text{SOC\\_batt}_{i-1})\n",
    "$$\n",
    "\n",
    "where $ \\text{Depletion\\_Rate}_t $ estimates **battery discharge trends**.\n",
    "\n",
    "The final **feature matrix** is:\n",
    "\n",
    "$$\n",
    "X = \\{ \\text{Hour}, \\text{Prev\\_SOC}, \\text{Prev\\_Current}, \\text{Prev\\_Solar\\_Current}, \\text{Prev\\_Voltage}, \\text{Prev\\_Charging}, \\text{Depletion\\_Rate} \\}\n",
    "$$\n",
    "\n",
    "and the target variable is:\n",
    "\n",
    "$$\n",
    "y = \\text{SOC\\_batt}\n",
    "$$\n",
    "\n",
    "## **2. Training the XGBoost Model with Bayesian Optimization**\n",
    "XGBoost, a **gradient-boosted tree regressor**, is trained to minimize **SOC prediction error**. Hyperparameter tuning is performed using **Optuna**, a Bayesian optimization framework.\n",
    "\n",
    "### **Optimization Objective**\n",
    "The model is optimized to **minimize the Mean Absolute Error (MAE)**:\n",
    "\n",
    "$$\n",
    "\\mathcal{L} = \\frac{1}{N} \\sum_{i=1}^{N} | y_i - \\hat{y}_i |\n",
    "$$\n",
    "\n",
    "where:\n",
    "- $ y_i $ is the **actual SOC value**,\n",
    "- $ \\hat{y}_i $ is the **predicted SOC**,\n",
    "- $ N $ is the number of test samples.\n",
    "\n",
    "### **Search Space for Hyperparameter Tuning**\n",
    "The following hyperparameters are optimized:\n",
    "- **Number of boosting rounds**: $ n_{\\text{estimators}} \\in [100, 500] $\n",
    "- **Learning rate**: $ \\eta \\in [0.06, 0.12] $ (log-uniform)\n",
    "- **Tree depth**: $ d \\in [4,6] $\n",
    "- **Subsampling ratio**: $ \\text{subsample} \\in [0.5,1.0] $\n",
    "- **Column sampling ratio**: $ \\text{colsample\\_bytree} \\in [0.5,1.0] $\n",
    "- **Regularization parameters**: $ \\lambda, \\alpha \\in [0.001, 10] $\n",
    "\n",
    "The Bayesian search selects hyperparameters that minimize **validation MAE**.\n",
    "\n",
    "## **3. Multi-Step Forecasting**\n",
    "To predict future SOC depletion, the trained model is used iteratively for **seven future time steps**.\n",
    "\n",
    "For each step $ t $, the next SOC is predicted as:\n",
    "\n",
    "$$\n",
    "\\hat{y}_t = f(X_t, \\theta^*)\n",
    "$$\n",
    "\n",
    "where:\n",
    "- $ f $ is the trained **XGBoost model**,\n",
    "- $ X_t $ contains the latest **SOC, charging state, and depletion rate**,\n",
    "- $ \\theta^* $ represents the **optimal hyperparameters**.\n",
    "\n",
    "Each prediction is fed back into the model, allowing **rolling forecasts**:\n",
    "\n",
    "$$\n",
    "X_{t+1} \\gets X_t, \\quad X_{t+1}[\\text{Prev\\_SOC}] = \\hat{y}_t\n",
    "$$\n",
    "\n",
    "ensuring **dynamic simulation of SOC depletion**.\n",
    "\n",
    "## **4. Uncertainty Estimation and Dynamic Thresholding**\n",
    "To ensure **safe operation**, a **dynamic SOC threshold** is computed for each future step, adjusting based on:\n",
    "- **Historical depletion rates**\n",
    "- **Charging behavior**\n",
    "- **Battery degradation uncertainty**\n",
    "\n",
    "A **Bayesian prior** is set on SOC:\n",
    "\n",
    "$$\n",
    "\\mu_{\\text{prior}} = \\mathbb{E}[y_{\\text{test}}], \\quad \\sigma_{\\text{prior}} = \\text{std}(y_{\\text{test}})\n",
    "$$\n",
    "\n",
    "Thresholds are adjusted based on:\n",
    "1. **Failure probability**:\n",
    "\n",
    "$$\n",
    "P_{\\text{failure}} = \\Phi\\left(\\frac{10 - \\mu_{\\text{prior}}}{\\sigma_{\\text{prior}}} \\right)\n",
    "$$\n",
    "\n",
    "where $ \\Phi $ is the **cumulative normal distribution**, modeling the probability of SOC dropping below 10%.\n",
    "\n",
    "2. **Dynamic SOC Threshold Computation**:\n",
    "\n",
    "$$\n",
    "\\text{Safe\\_SOC}_t = 30 + (10 \\cdot P_{\\text{failure}}) + (5 \\cdot \\sigma_t)\n",
    "$$\n",
    "\n",
    "where:\n",
    "- The **base threshold is 30% SOC**.\n",
    "- **Additional margins** are added based on **failure probability** and **rolling standard deviation**.\n",
    "\n",
    "Charging and solar effects further refine the threshold:\n",
    "\n",
    "$$\n",
    "\\text{Safe\\_SOC}_t = \\text{Safe\\_SOC}_t + 5 \\cdot \\mathbb{1}(\\text{Charging}) - 3 \\cdot \\mathbb{1}(\\text{Solar\\_High})\n",
    "$$\n",
    "\n",
    "ensuring **dynamic safety monitoring**.\n",
    "\n",
    "## **5. Evaluation Metrics**\n",
    "Final model performance is evaluated using:\n",
    "- **Mean Absolute Error (MAE)**:\n",
    "\n",
    "$$\n",
    "\\text{MAE} = \\frac{1}{N} \\sum_{i=1}^{N} | y_i - \\hat{y}_i |\n",
    "$$\n",
    "\n",
    "- **Root Mean Squared Error (RMSE)**:\n",
    "\n",
    "$$\n",
    "\\text{RMSE} = \\sqrt{\\frac{1}{N} \\sum_{i=1}^{N} (y_i - \\hat{y}_i)^2}\n",
    "$$\n",
    "\n",
    "- **Symmetric Mean Absolute Percentage Error (SMAPE)**:\n",
    "\n",
    "$$\n",
    "\\text{SMAPE} = 100 \\cdot \\frac{1}{N} \\sum_{i=1}^{N} \\frac{| y_i - \\hat{y}_i |}{( | y_i | + | \\hat{y}_i | ) / 2}\n",
    "$$\n",
    "\n",
    "Residual analysis confirms the **distribution of errors**, and rolling error plots show **stability over time**.\n",
    "\n",
    "## **6. Feature Importance Analysis Using XGBoost**\n",
    "\n",
    "Understanding **feature importance** is crucial in machine learning models to **interpret predictive behavior** and **assess model reliability**. In this study, we employ **XGBoostâ€™s feature importance analysis** to quantify the impact of each input variable on **SOC (State of Charge) prediction**.\n",
    "\n",
    "XGBoost provides an **intrinsic feature ranking mechanism**, which assigns **importance scores** to each predictor based on how frequently and effectively it contributes to **minimizing prediction error**.\n",
    "\n",
    "## **7. Conclusion**\n",
    "This model integrates:\n",
    "âœ… **Gradient-boosted decision trees** for SOC prediction.  \n",
    "âœ… **Bayesian hyperparameter tuning** for performance optimization.  \n",
    "âœ… **Multi-step forecasting** to predict SOC **seven steps ahead**.  \n",
    "âœ… **Uncertainty estimation** and **dynamic thresholding** for real-time monitoring.  \n",
    "\n",
    "This methodology enables **accurate, data-driven SOC forecasting**, supporting **energy-efficient urban mobility** and **predictive battery management**.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from graphviz import Digraph\n",
    "\n",
    "# Create a directed graph for Safe SOC computation\n",
    "safe_soc_flowchart = Digraph(\"Safe_SOC_Computation\", format=\"png\")\n",
    "\n",
    "# Define nodes\n",
    "safe_soc_flowchart.node(\"Start\", \"Start\", shape=\"oval\", style=\"filled\", fillcolor=\"lightgreen\")\n",
    "safe_soc_flowchart.node(\"InitStats\", \"Compute Initial SOC Statistics\\n(prior_mean, prior_std)\", shape=\"parallelogram\")\n",
    "safe_soc_flowchart.node(\"ComputeFailure\", \"Compute Failure Probability:\\nP_failure = CDF(10, prior_mean, prior_std)\", shape=\"parallelogram\")\n",
    "\n",
    "safe_soc_flowchart.node(\"PredictSOC\", \"Predict Future SOC for t+1 to t+7\\nÅ·_t = f(X_t, Î¸*)\", shape=\"parallelogram\")\n",
    "safe_soc_flowchart.node(\"RollingDepletion\", \"Compute Rolling SOC Depletion Rate:\\nDepletion_Rate[t] = Mean(SOC[t-5:t] - SOC[t-6:t-1])\", shape=\"parallelogram\")\n",
    "safe_soc_flowchart.node(\"ComputeUncertainty\", \"Compute Prediction Uncertainty:\\nÏƒ_pred = StdDev(Å·[t-5:t])\", shape=\"parallelogram\")\n",
    "\n",
    "safe_soc_flowchart.node(\"BaseSafeSOC\", \"Compute Base Safe SOC:\\nSafe_SOC = 30 + (10 * P_failure) + (5 * Ïƒ_pred)\", shape=\"parallelogram\")\n",
    "safe_soc_flowchart.node(\"AdjustForCharging\", \"Adjust for Charging:\\nIf Charging[t] â†’ Safe_SOC += 5\", shape=\"diamond\")\n",
    "safe_soc_flowchart.node(\"AdjustForSolar\", \"Adjust for Solar Input:\\nIf Solar[t] > 0 â†’ Safe_SOC -= 3\", shape=\"diamond\")\n",
    "\n",
    "safe_soc_flowchart.node(\"ApplyBounds\", \"Clip Safe SOC to [15, 45]%\\nSafe_SOC = min(45, max(15, Safe_SOC))\", shape=\"parallelogram\")\n",
    "safe_soc_flowchart.node(\"End\", \"End\", shape=\"oval\", style=\"filled\", fillcolor=\"red\")\n",
    "\n",
    "# Define edges with calculations\n",
    "safe_soc_flowchart.edge(\"Start\", \"InitStats\")\n",
    "safe_soc_flowchart.edge(\"InitStats\", \"ComputeFailure\", label=\"Compute P_failure\")\n",
    "safe_soc_flowchart.edge(\"ComputeFailure\", \"PredictSOC\", label=\"Predict Future SOC\")\n",
    "safe_soc_flowchart.edge(\"PredictSOC\", \"RollingDepletion\", label=\"Compute Depletion Rate\")\n",
    "safe_soc_flowchart.edge(\"RollingDepletion\", \"ComputeUncertainty\", label=\"Estimate Variability\")\n",
    "safe_soc_flowchart.edge(\"ComputeUncertainty\", \"BaseSafeSOC\", label=\"Compute Safe SOC Baseline\")\n",
    "\n",
    "safe_soc_flowchart.edge(\"BaseSafeSOC\", \"AdjustForCharging\", label=\"Check Charging Status\")\n",
    "safe_soc_flowchart.edge(\"AdjustForCharging\", \"AdjustForSolar\", label=\"If Charging â†’ Increase Safe SOC\", constraint=\"false\")\n",
    "safe_soc_flowchart.edge(\"AdjustForSolar\", \"ApplyBounds\", label=\"If Solar Input â†’ Reduce Safe SOC\", constraint=\"false\")\n",
    "\n",
    "safe_soc_flowchart.edge(\"ApplyBounds\", \"End\", label=\"Ensure Safe SOC in Range\")\n",
    "\n",
    "# Render and display the flowchart\n",
    "safe_soc_flowchart_path = \"/workspace/data/safe_soc_curve_flowchart\"\n",
    "safe_soc_flowchart.render(safe_soc_flowchart_path, format=\"png\", cleanup=True)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load and preprocess data\n",
    "df_threshold = df.copy()\n",
    "\n",
    "# Sort and prepare dataset\n",
    "df_threshold = df_threshold.sort_values(by=['deviceID', 'Timestamp'])\n",
    "df_threshold['Timestamp'] = pd.to_datetime(df_threshold['Timestamp'])\n",
    "df_threshold.set_index('Timestamp', inplace=True)\n",
    "\n",
    "# Select a single device\n",
    "device_id = df_threshold['deviceID'].unique()[0]\n",
    "df_device = df_threshold[df_threshold['deviceID'] == device_id].copy()\n",
    "\n",
    "# Convert SOC and related features to numeric\n",
    "for col in ['SOC_batt', 'current_batt', 'solar_current', 'isCharginS', 'volatge_batt', 'isCharging']:\n",
    "    df_device[col] = pd.to_numeric(df_device[col], errors='coerce')\n",
    "\n",
    "df_device.dropna(inplace=True)\n",
    "\n",
    "# **ðŸ“Œ Feature Engineering**\n",
    "df_device['Hour'] = df_device.index.hour\n",
    "df_device['Prev_SOC'] = df_device['SOC_batt'].shift(1)\n",
    "df_device['Prev_Current'] = df_device['current_batt'].shift(1)\n",
    "df_device['Prev_Solar_Current'] = df_device['solar_current'].shift(1)\n",
    "df_device['Prev_Solar'] = df_device['isCharginS'].shift(1)\n",
    "df_device['Prev_Voltage'] = df_device['volatge_batt'].shift(1)\n",
    "df_device['Prev_Charging'] = df_device['isCharging'].shift(1)\n",
    "\n",
    "# Compute rolling depletion rate\n",
    "df_device['Depletion_Rate'] = df_device['SOC_batt'].diff().rolling(window=5).mean()\n",
    "df_device['Depletion_Rate'].fillna(0, inplace=True)\n",
    "\n",
    "df_device.dropna(inplace=True)\n",
    "\n",
    "# Define input features and target\n",
    "features = ['Hour', 'Prev_SOC', 'Prev_Current', 'Prev_Solar_Current', 'Prev_Solar', 'Prev_Voltage', 'Prev_Charging', 'Depletion_Rate']\n",
    "X = df_device[features]\n",
    "y = df_device['SOC_batt']\n",
    "\n",
    "# Train-test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, shuffle=False)\n",
    "\n",
    "# **ðŸ”¹ Hyperparameter Tuning with Optuna**\n",
    "def objective(trial):\n",
    "    params = {\n",
    "        'objective': 'reg:squarederror',\n",
    "        'n_estimators': trial.suggest_int('n_estimators', 100, 500),\n",
    "        'learning_rate': trial.suggest_float('learning_rate', 0.07, 0.12, log=True),  # Narrow range\n",
    "        'max_depth': trial.suggest_int('max_depth', 4, 6),  # Prevent deep overfitting\n",
    "        'subsample': trial.suggest_float('subsample', 0.5, 1.0),\n",
    "        'colsample_bytree': trial.suggest_float('colsample_bytree', 0.5, 1.0),\n",
    "        'gamma': trial.suggest_float('gamma', 0.01, 1.0, log=True),\n",
    "        'reg_alpha': trial.suggest_float('reg_alpha', 0.001, 10, log=True),\n",
    "        'reg_lambda': trial.suggest_float('reg_lambda', 0.001, 10, log=True)\n",
    "    }\n",
    "    model = xgb.XGBRegressor(**params)\n",
    "    model.fit(X_train, y_train)\n",
    "    y_pred = model.predict(X_test)\n",
    "    return mean_absolute_error(y_test, y_pred)\n",
    "\n",
    "# Run Optuna optimization\n",
    "# Define total trials\n",
    "N_TRIALS = 400\n",
    "STARTUP_TRIALS = 10\n",
    "\n",
    "# Suppress excessive Optuna logging\n",
    "optuna.logging.set_verbosity(optuna.logging.WARNING)  # Show only important messages\n",
    "\n",
    "# **ðŸ”¹ Initialize tqdm progress bar**\n",
    "pbar = tqdm(total=N_TRIALS, desc=\"Optimization Progress\", position=0, leave=False, dynamic_ncols=True)\n",
    "\n",
    "# **ðŸ”¹ Callback function to update progress smoothly**\n",
    "def progress_callback(study, trial):\n",
    "    pbar.update(1)  # Increment progress bar by 1\n",
    "    if study.best_value is not None:\n",
    "        pbar.set_postfix({\"Best MAE\": f\"{study.best_value:.4f}\"})  # Update in progress bar instead of printing\n",
    "\n",
    "# **ðŸ”¹ Run Optuna optimization**\n",
    "study = optuna.create_study(\n",
    "    sampler=optuna.samplers.TPESampler(n_startup_trials=STARTUP_TRIALS),\n",
    "    direction='minimize'\n",
    ")\n",
    "study.optimize(objective, n_trials=N_TRIALS, callbacks=[progress_callback])\n",
    "\n",
    "# **ðŸ”¹ Close tqdm progress bar after completion**\n",
    "pbar.close()\n",
    "\n",
    "# Train final model with best parameters\n",
    "best_params = study.best_params\n",
    "xgb_model = xgb.XGBRegressor(**best_params)\n",
    "xgb_model.fit(X_train, y_train)\n",
    "\n",
    "# Predict SOC\n",
    "y_pred = xgb_model.predict(X_test)\n",
    "\n",
    "# **ðŸ”¹ Multi-Step Forecasting (Predict Next 7 Steps)**\n",
    "future_steps = 7\n",
    "X_future = X_test.copy()  # Copy of the last known test input\n",
    "predictions = np.zeros((len(X_test), future_steps))  # Shape: (test_size, future_steps)\n",
    "dynamic_thresholds = np.zeros((len(X_test), future_steps))  # Shape: (test_size, future_steps)\n",
    "\n",
    "# Initialize Bayesian prior for SOC distribution\n",
    "prior_mean = np.mean(y_test)  # Initial mean SOC\n",
    "prior_std = np.std(y_test)  # Initial standard deviation\n",
    "\n",
    "# **Precompute failure probability for efficiency**\n",
    "failure_prob_cache = stats.norm.cdf(10, loc=prior_mean, scale=prior_std)\n",
    "\n",
    "for step in range(future_steps):\n",
    "    y_future = xgb_model.predict(X_future)  # Predict future SOC\n",
    "    predictions[:, step] = y_future  # Store predictions\n",
    "\n",
    "    # **Vectorized Depletion Rate Calculation**\n",
    "    if step >= 5:\n",
    "        depletion_factor = np.mean(np.diff(predictions[:, max(0, step-5):step]), axis=1)\n",
    "    else:\n",
    "        depletion_factor = np.mean(np.diff(predictions[:, :step+1]), axis=1) if step > 0 else np.zeros(len(y_future))\n",
    "\n",
    "    # **Rolling Standard Deviation Instead of Monte Carlo Simulations**\n",
    "    prediction_uncertainty = np.std(predictions[:, max(0, step-5):step], axis=1)\n",
    "\n",
    "    # **Dynamic Threshold Adjustment (Vectorized)**\n",
    "    safe_soc = 30 + (10 * failure_prob_cache) + (5 * prediction_uncertainty)\n",
    "\n",
    "    # **Charging & Solar Adjustments**\n",
    "    safe_soc += (X_future['Prev_Charging'].values * 5)  # Add 5% if charging\n",
    "    safe_soc -= (X_future['Prev_Solar_Current'].values > 0) * 3  # Reduce by 3% if solar is strong\n",
    "\n",
    "    # **Apply Bounds**\n",
    "    safe_soc = np.clip(safe_soc, 10, 100)  # Keep in range\n",
    "    dynamic_thresholds[:, step] = safe_soc  # Store computed thresholds\n",
    "\n",
    "    # **Update Prior for Next Step**\n",
    "    prior_mean = np.mean(y_future)\n",
    "    prior_std = np.std(y_future)\n",
    "\n",
    "    # **Update X_future for next step**\n",
    "    X_future = X_future.copy()\n",
    "    X_future['Prev_SOC'] = y_future  # Use predicted SOC as input for next step\n",
    "\n",
    "\n",
    "# **ðŸ“Œ Compute Evaluation Metrics**\n",
    "mae = mean_absolute_error(y_test, y_pred)\n",
    "rmse = np.sqrt(mean_squared_error(y_test, y_pred))\n",
    "mape = np.mean(np.abs((y_test - y_pred) / y_test)) * 100\n",
    "\n",
    "# **Function to Compute SMAPE**\n",
    "def smape(y_true, y_pred):\n",
    "    return 100 * np.mean(np.abs(y_pred - y_true) / ((np.abs(y_true) + np.abs(y_pred)) / 2))\n",
    "\n",
    "smape_score = smape(y_test, y_pred)\n",
    "\n",
    "print(f\"ðŸ“Œ Validation Metrics:\")\n",
    "print(f\"âœ… Mean Absolute Error (MAE): {mae:.2f}%\")\n",
    "print(f\"âœ… Root Mean Square Error (RMSE): {rmse:.2f}%\")\n",
    "print(f\"âœ… Mean Absolute Percentage Error (MAPE): {mape:.2f}%\")\n",
    "print(f\"âœ… Symmetric Mean Absolute Percentage Error (SMAPE): {smape_score:.2f}%\")\n",
    "\n",
    "# **ðŸ”¹ Residual Analysis**\n",
    "residuals = y_test - y_pred\n",
    "plt.figure(figsize=(12, 6))\n",
    "sns.histplot(residuals, bins=50, kde=True, color=\"purple\")\n",
    "plt.axvline(residuals.mean(), color='red', linestyle='dashed', label=f\"Mean Residual: {residuals.mean():.2f}\")\n",
    "plt.title(\"Residual Distribution (y_test - y_pred)\")\n",
    "plt.xlabel(\"Residual\")\n",
    "plt.ylabel(\"Frequency\")\n",
    "plt.legend()\n",
    "plt.grid()\n",
    "plt.show()\n",
    "\n",
    "# **ðŸ”¹ Rolling Error Analysis (50-step window)**\n",
    "rolling_window = 50\n",
    "rolling_mae = np.convolve(np.abs(residuals), np.ones(rolling_window)/rolling_window, mode='valid')\n",
    "rolling_rmse = np.convolve(np.square(residuals), np.ones(rolling_window)/rolling_window, mode='valid')\n",
    "rolling_rmse = np.sqrt(rolling_rmse)\n",
    "\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.plot(y_test.index[rolling_window-1:], rolling_mae, label=\"Rolling MAE\", color='blue')\n",
    "plt.plot(y_test.index[rolling_window-1:], rolling_rmse, label=\"Rolling RMSE\", color='orange')\n",
    "plt.title(\"Rolling Error Analysis (MAE & RMSE Over Time)\")\n",
    "plt.xlabel(\"Time\")\n",
    "plt.ylabel(\"Error\")\n",
    "plt.legend()\n",
    "plt.grid()\n",
    "plt.show()\n",
    "\n",
    "# Extract trials and corresponding objective values\n",
    "trials = [t.number for t in study.trials if t.state == optuna.trial.TrialState.COMPLETE]\n",
    "values = [t.value for t in study.trials if t.state == optuna.trial.TrialState.COMPLETE]\n",
    "\n",
    "# Best value tracking\n",
    "best_values = [min(values[:i+1]) for i in range(len(values))]\n",
    "\n",
    "# Compute percentage improvement\n",
    "initial_value = values[0]  # First trial\n",
    "final_value = min(values)  # Best trial\n",
    "\n",
    "# **1ï¸âƒ£ Convergence Plot**\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.plot(trials, values, marker='o', linestyle='-', color='b', label=\"MAE per trial\")\n",
    "plt.plot(trials, best_values, marker='o', linestyle='-', color='g', label=\"Best MAE found\")\n",
    "\n",
    "# **Add a percentage bar**\n",
    "plt.xlabel(\"Trial Number\")\n",
    "plt.ylabel(\"Objective Value (MAE)\")\n",
    "plt.title(\"Optuna Optimization History (Convergence Plot)\")\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()\n",
    "\n",
    "fig1 = optuna.visualization.plot_optimization_history(study)\n",
    "fig1.show()\n",
    "\n",
    "fig2 = optuna.visualization.plot_param_importances(study)\n",
    "fig2.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.percentile(y_test, 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "## **1ï¸âƒ£ Plot Actual SOC (Train & Test) & Predictions (Only for Test)**\n",
    "\n",
    "ax[0].plot(y_train.index, y_train, label=\"Actual SOC (Train)\", color='blue', linestyle=\"dashed\", alpha=0.7)  # Show only actual values for train\n",
    "ax[0].plot(y_test.index, y_test, label=\"Actual SOC (Test)\", color='black', linestyle=\"dashed\")\n",
    "ax[0].plot(y_test.index, y_pred, label=\"Predicted SOC (Test)\", color='red')\n",
    "\n",
    "ax[0].set_title(f\"Optimized XGBoost SOC Prediction vs Actual SOC (Train & Test)\")\n",
    "ax[0].set_ylabel(\"SOC Battery Level (%)\")\n",
    "ax[0].legend(loc=\"upper left\", bbox_to_anchor=(1, 1))  # Legend outside\n",
    "ax[0].grid(True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "# **Step 1: Identify Gaps Greater Than 1 Day in Training & Test Data**\n",
    "gap_threshold = pd.Timedelta(days=1)\n",
    "\n",
    "# Find gaps in training data\n",
    "time_gaps_train = y_train.index.to_series().diff() > gap_threshold\n",
    "train_segment_indices = np.where(time_gaps_train)[0]\n",
    "\n",
    "# Find gaps in test data\n",
    "time_gaps_test = y_test.index.to_series().diff() > gap_threshold\n",
    "test_segment_indices = np.where(time_gaps_test)[0]\n",
    "\n",
    "# Copy actual values to introduce NaNs where gaps exist (preserving gaps only for actual SOC)\n",
    "y_train_gapfree = y_train.copy()\n",
    "y_test_gapfree = y_test.copy()\n",
    "\n",
    "y_train_gapfree.iloc[train_segment_indices] = np.nan\n",
    "y_test_gapfree.iloc[test_segment_indices] = np.nan\n",
    "\n",
    "# **Clip predictions to a maximum of 100%**\n",
    "y_pred_clipped = np.clip(y_pred, 0, 100)  # Ensures predictions stay between 0% and 100%\n",
    "predictions_clipped = np.clip(predictions, 0, 100)  # Multi-step forecast predictions clipped\n",
    "\n",
    "# **Create subplots**\n",
    "fig, ax = plt.subplots(2, 1, figsize=(12, 10), sharex=False)\n",
    "\n",
    "# **1ï¸âƒ£ Plot Actual SOC (Train & Test) with Gaps & Predictions (No Gaps, Clipped at 100%)**\n",
    "ax[0].plot(y_train.index, y_train_gapfree, label=\"Actual SOC (Train)\", color='blue', alpha=0.7, linewidth=1)  # Gaps in train\n",
    "ax[0].plot(y_test.index, y_test_gapfree, label=\"Actual SOC (Test)\", color='blue', linestyle=\"dashed\", alpha=0.7, linewidth=1)  # Gaps in test\n",
    "ax[0].plot(y_test.index, y_pred_clipped, label=\"Predicted SOC (Test)\", color='red', linestyle=\"dashed\", alpha=0.7, linewidth=1)  # Predictions remain continuous but clipped\n",
    "\n",
    "ax[0].set_title(\"Optimized XGBoost SOC Prediction vs Actual SOC (Train & Test)\")\n",
    "ax[0].set_ylabel(\"SOC Battery Level (%)\")\n",
    "ax[0].legend(loc=\"upper left\", bbox_to_anchor=(1, 1))  # Legend outside\n",
    "ax[0].grid(True)\n",
    "\n",
    "# **2ï¸âƒ£ Multi-Step Forecasting & Safe SOC Threshold (Clipped at 100%)**\n",
    "for i in range(future_steps):\n",
    "    ax[1].plot(y_test.index[:len(predictions_clipped[:, i])], predictions_clipped[:, i], linestyle=\"dotted\", linewidth=1, alpha=0.7, label=f\"Step {i+1} Forecast\")\n",
    "\n",
    "ax[1].plot(y_test.index[:len(dynamic_thresholds[:, i])], dynamic_thresholds[:, -1], linestyle=\"solid\", color='green', alpha=0.8, label=\"Safe SOC Threshold (Final Step)\")\n",
    "\n",
    "ax[1].set_title(\"Multi-Step SOC Forecast with Dynamic Safe Thresholds\")\n",
    "ax[1].set_xlabel(\"Time\")\n",
    "ax[1].set_ylabel(\"SOC Battery Level (%)\")\n",
    "ax[1].legend(loc=\"upper left\", bbox_to_anchor=(1, 1))  # Legend outside\n",
    "ax[1].grid(True)\n",
    "\n",
    "# Adjust layout\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions_clipped"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "# **Step 1: Retrain Model on Full Dataset**\n",
    "X_full = df_device[features]\n",
    "y_full = df_device['SOC_batt']\n",
    "\n",
    "xgb_model_full = xgb.XGBRegressor(**best_params)\n",
    "xgb_model_full.fit(X_full, y_full)  # Train on full dataset\n",
    "\n",
    "# **Step 2: Predict SOC for All Data**\n",
    "y_pred_full = xgb_model_full.predict(X_full)\n",
    "\n",
    "# **Step 3: Compute Dynamic Safe SOC Threshold for All Data**\n",
    "prior_mean = np.mean(y_full)\n",
    "prior_std = np.std(y_full)\n",
    "\n",
    "failure_prob_cache = stats.norm.cdf(10, loc=prior_mean, scale=prior_std)\n",
    "\n",
    "safe_soc_thresholds = np.zeros(len(y_full))\n",
    "\n",
    "for i in range(len(y_pred_full)):\n",
    "    depletion_factor = np.mean(np.diff(y_pred_full[max(0, i-5):i])) if i >= 5 else 0\n",
    "    prediction_uncertainty = np.std(y_pred_full[max(0, i-5):i]) if i >= 5 else 0\n",
    "\n",
    "    safe_soc = 30 + (10 * failure_prob_cache) + (5 * prediction_uncertainty)\n",
    "    safe_soc += (X_full['Prev_Charging'].iloc[i] * 5)\n",
    "    safe_soc -= (X_full['Prev_Solar_Current'].iloc[i] > 0) * 3\n",
    "    safe_soc = np.clip(safe_soc, 15, 45)\n",
    "    \n",
    "    safe_soc_thresholds[i] = safe_soc\n",
    "\n",
    "# **Step 4: Identify Gaps Greater Than 1 Day**\n",
    "gap_threshold = pd.Timedelta(days=1)\n",
    "time_gaps = df_device.index.to_series().diff() > gap_threshold  # Find where gaps are > 3 days\n",
    "segment_indices = np.where(time_gaps)[0]  # Indices where gaps exist\n",
    "\n",
    "# **Step 5: Plot Results with Unlinked Segments**\n",
    "plt.figure(figsize=(12, 6))\n",
    "\n",
    "# Track first segment to add legend only once\n",
    "first_segment = True\n",
    "\n",
    "# Loop through segments and plot separately\n",
    "start_idx = 0\n",
    "for idx in segment_indices:\n",
    "    if first_segment:\n",
    "        plt.plot(df_device.index[start_idx:idx], y_full[start_idx:idx], label=\"Actual SOC\", color='black', linestyle=\"dashed\")\n",
    "        plt.plot(df_device.index[start_idx:idx], y_pred_full[start_idx:idx], label=\"Predicted SOC\", color='red')\n",
    "        plt.plot(df_device.index[start_idx:idx], safe_soc_thresholds[start_idx:idx], linestyle=\"dashdot\", color='blue', label=\"Safe SOC Threshold\")\n",
    "        first_segment = False  # After first plot, disable legend labels\n",
    "    else:\n",
    "        plt.plot(df_device.index[start_idx:idx], y_full[start_idx:idx], color='black', linestyle=\"dashed\")\n",
    "        plt.plot(df_device.index[start_idx:idx], y_pred_full[start_idx:idx], color='red')\n",
    "        plt.plot(df_device.index[start_idx:idx], safe_soc_thresholds[start_idx:idx], linestyle=\"dashdot\", color='blue')\n",
    "\n",
    "    start_idx = idx  # Move start index to the next segment\n",
    "\n",
    "# Plot the last segment\n",
    "plt.plot(df_device.index[start_idx:], y_full[start_idx:], color='black', linestyle=\"dashed\")\n",
    "plt.plot(df_device.index[start_idx:], y_pred_full[start_idx:], color='red')\n",
    "plt.plot(df_device.index[start_idx:], safe_soc_thresholds[start_idx:], linestyle=\"dashdot\", color='blue')\n",
    "\n",
    "plt.xlabel(\"Time\")\n",
    "plt.ylabel(\"SOC Battery Level (%)\")\n",
    "plt.title(\"Deployed Model: SOC Prediction & Safe Threshold Over Time (Unlinked Gaps > 1 Day)\")\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "# **Step 1: Load and preprocess data for all devices**\n",
    "df_threshold = df.copy()\n",
    "\n",
    "# Sort and prepare dataset\n",
    "df_threshold = df_threshold.sort_values(by=['deviceID', 'Timestamp'])\n",
    "df_threshold.set_index('Timestamp', inplace=True)\n",
    "\n",
    "# **Step 2: Initialize List for Safe SOC Thresholds**\n",
    "all_safe_soc_thresholds = []\n",
    "\n",
    "# **Step 3: Loop over each device**\n",
    "for device_id in df_threshold['deviceID'].unique():\n",
    "    print(f\"\\nðŸ”¹ Processing Device ID: {device_id}\")\n",
    "\n",
    "    # **Extract data for the current device**\n",
    "    df_device = df_threshold[df_threshold['deviceID'] == device_id].copy()\n",
    "\n",
    "    # Convert SOC and related features to numeric\n",
    "    for col in ['SOC_batt', 'current_batt', 'solar_current', 'isCharginS', 'volatge_batt', 'isCharging']:\n",
    "        df_device[col] = pd.to_numeric(df_device[col], errors='coerce')\n",
    "\n",
    "    df_device.dropna(inplace=True)\n",
    "\n",
    "    # **Feature Engineering**\n",
    "    df_device['Hour'] = df_device.index.hour\n",
    "    df_device['Prev_SOC'] = df_device['SOC_batt'].shift(1)\n",
    "    df_device['Prev_Current'] = df_device['current_batt'].shift(1)\n",
    "    df_device['Prev_Solar_Current'] = df_device['solar_current'].shift(1)\n",
    "    df_device['Prev_Solar'] = df_device['isCharginS'].shift(1)\n",
    "    df_device['Prev_Voltage'] = df_device['volatge_batt'].shift(1)\n",
    "    df_device['Prev_Charging'] = df_device['isCharging'].shift(1)\n",
    "\n",
    "    # Compute rolling depletion rate\n",
    "    df_device['Depletion_Rate'] = df_device['SOC_batt'].diff().rolling(window=5).mean()\n",
    "    df_device['Depletion_Rate'].fillna(0, inplace=True)\n",
    "\n",
    "    df_device.dropna(inplace=True)\n",
    "\n",
    "    # Define input features and target\n",
    "    features = ['Hour', 'Prev_SOC', 'Prev_Current', 'Prev_Solar_Current', 'Prev_Solar', 'Prev_Voltage', 'Prev_Charging', 'Depletion_Rate']\n",
    "    X_full = df_device[features]\n",
    "    y_full = df_device['SOC_batt']\n",
    "\n",
    "    # **Step 4: Train XGBoost model for this device**\n",
    "    xgb_model_full = xgb.XGBRegressor(**best_params)\n",
    "    xgb_model_full.fit(X_full, y_full)\n",
    "\n",
    "    # **Step 5: Predict SOC for all timestamps in this device**\n",
    "    y_pred_full = xgb_model_full.predict(X_full)\n",
    "\n",
    "    # **Step 6: Compute Dynamic Safe SOC Threshold**\n",
    "    prior_mean = np.mean(y_full)\n",
    "    prior_std = np.std(y_full)\n",
    "    failure_prob_cache = stats.norm.cdf(10, loc=prior_mean, scale=prior_std)\n",
    "\n",
    "    safe_soc_thresholds = np.zeros(len(y_full))\n",
    "\n",
    "    for i in range(len(y_pred_full)):\n",
    "        depletion_factor = np.mean(np.diff(y_pred_full[max(0, i-5):i])) if i >= 5 else 0\n",
    "        prediction_uncertainty = np.std(y_pred_full[max(0, i-5):i]) if i >= 5 else 0\n",
    "\n",
    "        safe_soc = 50 + (10 * failure_prob_cache) + (5 * prediction_uncertainty)\n",
    "        safe_soc += (X_full['Prev_Charging'].iloc[i] * 5)\n",
    "        safe_soc -= (X_full['Prev_Solar_Current'].iloc[i] > 0) * 3\n",
    "        safe_soc = np.clip(safe_soc, 10, 70)\n",
    "\n",
    "        safe_soc_thresholds[i] = safe_soc\n",
    "\n",
    "    # **Step 7: Store Safe SOC Thresholds for Merging**\n",
    "    device_results = pd.DataFrame({\n",
    "        'deviceID': df_device['deviceID'].values,\n",
    "        'Timestamp': df_device.index,\n",
    "        'Safe_SOC_Threshold': safe_soc_thresholds\n",
    "    })\n",
    "    all_safe_soc_thresholds.append(device_results)\n",
    "\n",
    "    # **Step 8: Identify Gaps Greater Than 1 Day**\n",
    "    gap_threshold = pd.Timedelta(days=1)\n",
    "    time_gaps = df_device.index.to_series().diff() > gap_threshold\n",
    "    segment_indices = np.where(time_gaps)[0]\n",
    "\n",
    "    # **Step 9: Plot Results for Each Device with Unlinked Segments**\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    first_segment = True\n",
    "\n",
    "    start_idx = 0\n",
    "    for idx in segment_indices:\n",
    "        if first_segment:\n",
    "            plt.plot(df_device.index[start_idx:idx], y_full[start_idx:idx], label=\"Actual SOC\", color='black', linestyle=\"dashed\")\n",
    "            plt.plot(df_device.index[start_idx:idx], y_pred_full[start_idx:idx], label=\"Predicted SOC\", color='red')\n",
    "            plt.plot(df_device.index[start_idx:idx], safe_soc_thresholds[start_idx:idx], linestyle=\"dashdot\", color='blue', label=\"Safe SOC Threshold\")\n",
    "            first_segment = False\n",
    "        else:\n",
    "            plt.plot(df_device.index[start_idx:idx], y_full[start_idx:idx], color='black', linestyle=\"dashed\")\n",
    "            plt.plot(df_device.index[start_idx:idx], y_pred_full[start_idx:idx], color='red')\n",
    "            plt.plot(df_device.index[start_idx:idx], safe_soc_thresholds[start_idx:idx], linestyle=\"dashdot\", color='blue')\n",
    "\n",
    "        start_idx = idx  \n",
    "\n",
    "    plt.plot(df_device.index[start_idx:], y_full[start_idx:], color='black', linestyle=\"dashed\")\n",
    "    plt.plot(df_device.index[start_idx:], y_pred_full[start_idx:], color='red')\n",
    "    plt.plot(df_device.index[start_idx:], safe_soc_thresholds[start_idx:], linestyle=\"dashdot\", color='blue')\n",
    "\n",
    "    plt.xlabel(\"Time\")\n",
    "    plt.ylabel(\"SOC Battery Level (%)\")\n",
    "    plt.title(f\"Device {device_id}: SOC Prediction & Safe Threshold Over Time (Unlinked Gaps > 1 Day)\")\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    plt.show()\n",
    "\n",
    "# **Step 10: Merge Computed Safe SOC Thresholds into `df_gdf`**\n",
    "safe_soc_thresholds_df = pd.concat(all_safe_soc_thresholds)\n",
    "df_gdf = df_gdf.merge(safe_soc_thresholds_df, on=['deviceID', 'Timestamp'], how='left')\n",
    "\n",
    "# **Step 11: Replace Static Threshold with Dynamic Safe SOC**\n",
    "df_gdf['Depleted'] = df_gdf['SOC_batt'] < df_gdf['Safe_SOC_Threshold']  # Dynamic threshold check\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **SOC Depletion Modeling with Sensor OFF Strategies**\n",
    "\n",
    "## **Context and Objective**\n",
    "This implementation models the **State of Charge (SOC) depletion** of sensor-equipped vehicles while applying **dynamic sensor OFF strategies** based on pre-defined **time thresholds**. The goal is to assess the impact of temporarily turning **OFF** sensors to conserve energy, tracking **SOC savings**, and evaluating the difference in battery depletion rates under different sensor control policies.\n",
    "\n",
    "The core of this method involves:\n",
    "1. **Tracking the energy savings** from sensor OFF periods.\n",
    "2. **Modeling sensor activation behavior** based on pre-defined **time thresholds**.\n",
    "3. **Computing the modified SOC depletion** with stored energy savings.\n",
    "4. **Comparing the baseline SOC depletion with sensor control policies.**\n",
    "\n",
    "## **1. Data Preparation and Sorting**\n",
    "The dataset is first copied and indexed sequentially to ensure **chronological order**:\n",
    "\n",
    "$$\n",
    "\\text{Sort}(df, \\text{by}=[\\text{Timestamp}])\n",
    "$$\n",
    "\n",
    "This sorting is essential for maintaining **temporal consistency**, ensuring that energy tracking occurs in the correct **sequential order**.\n",
    "\n",
    "## **2. Definition of Sensor OFF Strategy**\n",
    "A sensor OFF threshold $ T_{\\text{threshold}} $ is defined, which determines how long a **grid cell** can remain **inactive** before reactivation is allowed. Given:\n",
    "\n",
    "$$\n",
    "T_{\\text{threshold}} = \\{ 12 \\text{ sec} \\}\n",
    "$$\n",
    "\n",
    "a sensor remains **OFF** if a vehicle has recently occupied the same **grid cell** within the threshold:\n",
    "\n",
    "$$\n",
    "\\text{Sensor\\_ON}_i =\n",
    "\\begin{cases} \n",
    "0, & \\text{if } (t_i - t_{\\text{last sensed}}) < T_{\\text{threshold}} \\\\\n",
    "1, & \\text{otherwise}\n",
    "\\end{cases}\n",
    "$$\n",
    "\n",
    "where:\n",
    "- $ t_i $ is the **current timestamp**.\n",
    "- $ t_{\\text{last sensed}} $ is the timestamp of the **last recorded sensor activation**.\n",
    "- $ T_{\\text{threshold}} $ is the **predefined time limit** for keeping the sensor OFF.\n",
    "\n",
    "## **3. Energy Storage Mechanism**\n",
    "During **sensor OFF periods**, the energy that would have been consumed is tracked using a **stored energy accumulation model**. The change in **SOC depletion rate** due to the sensor OFF mechanism is computed as:\n",
    "\n",
    "$$\n",
    "\\Delta \\text{SOC}_i = \\max(0, \\text{SOC}_{i-1} - \\text{SOC}_i)\n",
    "$$\n",
    "\n",
    "where:\n",
    "- $ \\text{SOC}_i $ is the **current SOC measurement**.\n",
    "- $ \\text{SOC}_{i-1} $ is the **previous SOC measurement**.\n",
    "\n",
    "Stored energy savings accumulate over time:\n",
    "\n",
    "$$\n",
    "\\text{Energy\\_Saved}_i = \\sum_{j=1}^{i} \\Delta \\text{SOC}_j, \\quad \\text{if Sensor\\_ON} = 0\n",
    "$$\n",
    "\n",
    "where:\n",
    "- $ \\text{Energy\\_Saved}_i $ represents the **total accumulated SOC savings**.\n",
    "- The accumulation occurs **only when the sensor is OFF**.\n",
    "\n",
    "## **4. Dynamic SOC Update with Energy Savings**\n",
    "The SOC for a given time step is **recomputed** using the stored energy:\n",
    "\n",
    "$$\n",
    "\\text{SOC\\_batt}_i' = \\text{SOC\\_batt}_i + \\text{Energy\\_Saved}_i\n",
    "$$\n",
    "\n",
    "where:\n",
    "- $ \\text{SOC\\_batt}_i' $ is the **corrected SOC value** incorporating energy savings.\n",
    "- $ \\text{SOC\\_batt}_i $ is the **original SOC value**.\n",
    "- $ \\text{Energy\\_Saved}_i $ represents the **cumulative stored SOC improvements**.\n",
    "\n",
    "To ensure **SOC does not exceed 100%**, the final corrected SOC values are clipped:\n",
    "\n",
    "$$\n",
    "\\text{SOC\\_batt}_i' = \\min(100, \\text{SOC\\_batt}_i + \\text{Energy\\_Saved}_i)\n",
    "$$\n",
    "\n",
    "## **5. Baseline and Threshold Comparisons**\n",
    "To assess the impact of sensor OFF strategies, SOC depletion is **compared across different thresholds**. The average daily SOC depletion per device is computed as:\n",
    "\n",
    "$$\n",
    "\\text{SOC\\_depletion} = \\frac{1}{N} \\sum_{i=1}^{N} \\text{SOC\\_batt}_i'\n",
    "$$\n",
    "\n",
    "where:\n",
    "- $ N $ is the number of **time steps in a day**.\n",
    "- $ \\text{SOC\\_batt}_i' $ represents the **SOC levels incorporating sensor OFF savings**.\n",
    "\n",
    "The **baseline depletion rate** without any sensor OFF strategy is also computed:\n",
    "\n",
    "$$\n",
    "\\text{Baseline\\_SOC} = \\frac{1}{N} \\sum_{i=1}^{N} \\text{SOC\\_batt}_i\n",
    "$$\n",
    "\n",
    "where no energy savings are incorporated.\n",
    "\n",
    "## **6. Expected Outcomes and Justification**\n",
    "This method enables:\n",
    "- **Quantification of energy savings** from sensor OFF strategies.\n",
    "- **Comparison of SOC depletion trends** across different sensor OFF thresholds.\n",
    "- **Validation of sensor optimization policies** to maximize battery lifespan.\n",
    "\n",
    "By implementing **grid-based sensing optimization**, the system **reduces unnecessary sensor activations**, ensuring **longer sensor endurance** while maintaining **effective data collection**.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from graphviz import Digraph\n",
    "\n",
    "# Create a new directed graph\n",
    "flowchart = Digraph(format=\"png\")\n",
    "\n",
    "# Define nodes\n",
    "flowchart.node(\"A\", \"Start: Load and Sort Data\", shape=\"oval\")\n",
    "flowchart.node(\"B\", \"Define Sensor OFF Threshold\", shape=\"parallelogram\")\n",
    "flowchart.node(\"C\", \"Iterate Through Data Entries\", shape=\"parallelogram\")\n",
    "flowchart.node(\"D\", \"Check if Sensor Recently ON?\", shape=\"diamond\")\n",
    "flowchart.node(\"E\", \"Turn Sensor OFF\\n(Store Energy Savings)\", shape=\"box\")\n",
    "flowchart.node(\"F\", \"Turn Sensor ON\\n(Reset Energy Savings)\", shape=\"box\")\n",
    "flowchart.node(\"G\", \"Compute Modified SOC with Savings\", shape=\"box\")\n",
    "flowchart.node(\"H\", \"Clip SOC at 100%\", shape=\"box\")\n",
    "flowchart.node(\"I\", \"Compute SOC Depletion Per Device\", shape=\"box\")\n",
    "flowchart.node(\"J\", \"Compare with Baseline SOC Depletion\", shape=\"parallelogram\")\n",
    "flowchart.node(\"K\", \"Save Results & End\", shape=\"oval\")\n",
    "\n",
    "# Define edges (process flow)\n",
    "flowchart.edge(\"A\", \"B\")\n",
    "flowchart.edge(\"B\", \"C\")\n",
    "flowchart.edge(\"C\", \"D\")\n",
    "flowchart.edge(\"D\", \"E\", label=\"Yes\")\n",
    "flowchart.edge(\"D\", \"F\", label=\"No\")\n",
    "flowchart.edge(\"E\", \"G\")\n",
    "flowchart.edge(\"F\", \"G\")\n",
    "flowchart.edge(\"G\", \"H\")\n",
    "flowchart.edge(\"H\", \"I\")\n",
    "flowchart.edge(\"I\", \"J\")\n",
    "flowchart.edge(\"J\", \"K\")\n",
    "\n",
    "# Render and display flowchart\n",
    "flowchart_path = \"/workspace/data/soc_depletion_flowchart\"\n",
    "flowchart.render(flowchart_path)\n",
    "flowchart_path + \".png\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_temp = df.copy().reset_index(drop=True)  # Ensure indices are sequential\n",
    "df_temp=df_temp.sort_values(by=['Timestamp']).reset_index(drop=True) \n",
    "\n",
    "# Define different time thresholds to compare\n",
    "time_thresholds = {\n",
    "    # \"3 sec\": 3,\n",
    "    \"360 sec\": 360\n",
    "}\n",
    "\n",
    "# Create a dictionary to store SOC and sensor states for each threshold\n",
    "soc_depletion_results = {}\n",
    "\n",
    "# Iterate over different time thresholds\n",
    "for label, TIME_THRESHOLD in time_thresholds.items():\n",
    "\n",
    "    # Track last sensed timestamp, and stored energy during OFF periods\n",
    "    last_sensed_time = {}\n",
    "    stored_energy = {}\n",
    "\n",
    "    # Previous date\n",
    "    prev_date=None\n",
    "    \n",
    "    for i in range(len(df_temp)):\n",
    "        row = df_temp.iloc[i]\n",
    "        grid_key = (row['Lat_Grid'], row['Log_Grid'])  # Unique grid identifier\n",
    "        current_time = row['Timestamp']\n",
    "        current_date = row['Date']\n",
    "        device= row['deviceID']\n",
    "\n",
    "        # Initialise inter-row differences when OFF\n",
    "        d_diff_prev=0 \n",
    "        \n",
    "        # Reset stored energy at the start of a new day\n",
    "        if prev_date is not None and current_date != prev_date:\n",
    "            stored_energy={}  # Reset stored energy for all grid cells\n",
    "            df_temp.loc[i:, f'Energy_Saved_{label}'] = 0  # Reset energy savings for the new day\n",
    "            print(f\"[RESET] Reset stored energy for new day: {current_date}\")\n",
    "\n",
    "        prev_date = current_date  # Update previous date tracker\n",
    "\n",
    "        if df_temp.loc[i, 'SOC_batt']>99:\n",
    "            stored_energy[grid_key]=0\n",
    "            df_temp.loc[i:, f'Energy_Saved_{label}']=0\n",
    "\n",
    "        # If the grid was sensed recently (within the threshold), turn OFF the sensor\n",
    "        if grid_key in last_sensed_time and (current_time - last_sensed_time[grid_key]).total_seconds() < TIME_THRESHOLD:\n",
    "            df_temp.at[i, f'Sensor_ON_{label}'] = False   \n",
    "\n",
    "            # Accumulate stored energy\n",
    "            if i > 0 and pd.notna(df_temp.iloc[i - 1]['SOC_batt']) and pd.notna(row['SOC_batt']):\n",
    "            \n",
    "                # Find the last preceding row for this device\n",
    "                if device == df_temp.iloc[i-1]['deviceID']:\n",
    "                    d_diff = max(0, df_temp.iloc[i - 1]['SOC_batt'] - row['SOC_batt']) #current inter-row difference\n",
    "                    diff = max(d_diff_prev, d_diff)\n",
    "                    stored_energy[grid_key] = df_temp.loc[i-1, f'Energy_Saved_{label}']\n",
    "                    stored_energy[grid_key] += diff\n",
    "                    \n",
    "                    if diff != 0: \n",
    "                        df_temp.loc[i:, f'Energy_Saved_{label}'] = stored_energy[grid_key]\n",
    "                    else:\n",
    "                        df_temp.loc[i:, f'Energy_Saved_{label}'] = df_temp.loc[i-1, f'Energy_Saved_{label}']\n",
    "\n",
    "                    d_diff_prev=diff\n",
    "                    print(f\"[OFF]: Accumulated {diff:.2f}% for device {device}. Total stored: {stored_energy[grid_key]:.2f}%\")\n",
    "\n",
    "                else:\n",
    "                    d_diff = max(0, df_temp.loc[df_temp.deviceID == device, :]['SOC_batt'].iloc[-1] - row['SOC_batt']) #current inter-row difference\n",
    "                    diff = max(d_diff_prev, d_diff)\n",
    "                    stored_energy[grid_key] = df_temp.loc[df_temp.deviceID == device, :][f'Energy_Saved_{label}'].iloc[-1]\n",
    "                    stored_energy[grid_key] += diff\n",
    "                    \n",
    "                    if diff != 0: \n",
    "                        df_temp.loc[i:, f'Energy_Saved_{label}'] = stored_energy[grid_key]\n",
    "                    else:\n",
    "                        df_temp.loc[i:, f'Energy_Saved_{label}'] = df_temp.loc[df_temp.deviceID == device, :][f'Energy_Saved_{label}'].iloc[-1]\n",
    "\n",
    "                    d_diff_prev=diff\n",
    "                    print(f\"CHANGE [OFF]: Accumulated {diff:.2f}% for device {device}. Total stored: {stored_energy[grid_key]:.2f}%\")\n",
    "\n",
    "        else:\n",
    "            # Update last sensed time when the sensor turns ON\n",
    "            last_sensed_time[grid_key] = current_time\n",
    "            df_temp.at[i, f'Sensor_ON_{label}'] = True\n",
    "            d_diff_prev=0\n",
    "\n",
    "            if device == df_temp.iloc[i-1]['deviceID']:\n",
    "\n",
    "                # Ensure stored_energy is initialized per grid cell without overwriting previous values\n",
    "                if grid_key not in stored_energy:\n",
    "                    if i > 0:\n",
    "                        #Carry forward the stored energy from the last known row\n",
    "                        stored_energy[grid_key] = df_temp.loc[i-1, f'Energy_Saved_{label}']\n",
    "                        df_temp.loc[i:, f'Energy_Saved_{label}'] = stored_energy[grid_key]\n",
    "                    elif i == 0:\n",
    "                        stored_energy[grid_key] = 0  # First iteration, no prior energy \n",
    "                        df_temp.loc[i:, f'Energy_Saved_{label}'] = stored_energy[grid_key]  \n",
    "                print(f\"[ON]: device {device}. Total stored: {stored_energy[grid_key]:.2f}%\")\n",
    "\n",
    "            else:\n",
    "                 # Ensure stored_energy is initialized per grid cell without overwriting previous values\n",
    "                if grid_key not in stored_energy:\n",
    "                    if i > 0:\n",
    "                        #Carry forward the stored energy from the last known row\n",
    "                        stored_energy[grid_key] = df_temp.loc[df_temp.deviceID == device, :][f'Energy_Saved_{label}'].iloc[-1]\n",
    "                        df_temp.loc[i:, f'Energy_Saved_{label}'] = stored_energy[grid_key]\n",
    "                    elif i == 0:\n",
    "                        stored_energy[grid_key] = 0  # First iteration, no prior energy \n",
    "                        df_temp.loc[i:, f'Energy_Saved_{label}'] = stored_energy[grid_key]                 \n",
    "                \n",
    "                print(f\"CHANGE [ON]: device {device}. Total stored: {stored_energy[grid_key]:.2f}%\")\n",
    "\n",
    "    \n",
    "    # Compute new SOC_batt with savings\n",
    "    df_temp[f'SOC_batt_{label}'] = df_temp['SOC_batt'] + df_temp[f'Energy_Saved_{label}']\n",
    "    df_temp[f'SOC_batt_{label}'] = df_temp[f'SOC_batt_{label}'].clip(upper=100)\n",
    "\n",
    "    # Compute SOC depletion for this threshold\n",
    "    daily_soc = df_temp.groupby(['Date', 'deviceID'])[f'SOC_batt_{label}'].mean()\n",
    "    soc_depletion_results[label] = daily_soc\n",
    "\n",
    "\n",
    "# Baseline: Compute SOC depletion without constraints\n",
    "soc_depletion_results[\"Baseline\"] = df_temp.groupby(['Date', 'deviceID'])['SOC_batt'].mean()\n",
    "\n",
    "# Convert results to a DataFrame for plotting\n",
    "soc_depletion_df = pd.DataFrame(soc_depletion_results)\n",
    "\n",
    "# Save the updated dataset with sensor states and energy savings for each threshold\n",
    "output_path = \"/workspace/data/updated_SOC_batt_with_energy_savings.xlsx\"\n",
    "df_temp.to_excel(output_path, index=False)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define line styles and transparency levels for each threshold\n",
    "line_styles = {\n",
    "    \"Baseline\": \"--\",\n",
    "    # \"3 sec\": \"--\",\n",
    "    \"360 sec\": \"-\"\n",
    "}\n",
    "\n",
    "# Transparency levels for each threshold\n",
    "alpha_values = {\n",
    "    \"Baseline\": 0.5,  # 70% transparent\n",
    "    # \"3 sec\": 0.7,  # 30% transparent\n",
    "    \"360 sec\": 1   #Fully visible\n",
    "}\n",
    "\n",
    "# Predefined colors for devices\n",
    "predefined_colors = ['#007FFF', '#DC143C', '#FF4500','#39FF14', '#800080']\n",
    "device_ids = set()\n",
    "\n",
    "for soc_series in soc_depletion_results.values():\n",
    "    device_ids.update(soc_series.index.get_level_values('deviceID').unique())\n",
    "\n",
    "# Create a color map using predefined colors\n",
    "color_map = {device_id: predefined_colors[i % len(predefined_colors)] for i, device_id in enumerate(sorted(device_ids))}\n",
    "\n",
    "# Plot SOC depletion for different devices and thresholds\n",
    "plt.figure(figsize=(12, 6))\n",
    "\n",
    "# Iterate over thresholds and plot per device\n",
    "for label, soc_series in soc_depletion_results.items():  # soc_series is a MultiIndexed Series\n",
    "    for device_id in soc_series.index.get_level_values('deviceID').unique():  # Get unique devices\n",
    "        device_data = soc_series[soc_series.index.get_level_values('deviceID') == device_id]\n",
    "        plt.plot(\n",
    "            device_data.index.get_level_values('Date'),  # X-axis: Dates\n",
    "            device_data.values,  # Y-axis: SOC values\n",
    "            linestyle=line_styles[label],\n",
    "            color=color_map[device_id],  # Use predefined color for the device\n",
    "            # marker='o',\n",
    "            # markersize=3,\n",
    "            alpha=alpha_values[label],  # Apply transparency per threshold\n",
    "            label=f\"Device {device_id} - {label}\"\n",
    "        )\n",
    "\n",
    "plt.xlabel('Date')\n",
    "plt.ylabel('Mean SOC (%)')\n",
    "plt.title('SOC Depletion Comparison Across Devices and Time Constraints')\n",
    "\n",
    "# Place the legend outside the plot\n",
    "plt.legend(\n",
    "    bbox_to_anchor=(1.05, 1),  # Place legend to the right of the plot\n",
    "    loc='upper left',          # Align legend to the top-left of the bounding box\n",
    "    borderaxespad=0.           # Reduce spacing between the legend and the plot\n",
    ")\n",
    "plt.grid(True)\n",
    "plt.xticks(rotation=45)\n",
    "\n",
    "# Adjust layout to make room for the legend\n",
    "plt.tight_layout()\n",
    "\n",
    "# Show plot\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "HEATMAPS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make copies for Baseline and Adaptive to avoid overwriting\n",
    "grid_gdf_baseline = grid_gdf.copy()\n",
    "grid_gdf_adaptive = grid_gdf.copy()\n",
    "\n",
    "### ---- BASELINE ----\n",
    "# Step 1: Compute Gamma Scaling on Raw Counts (Without Log Transformation)\n",
    "mean_count_baseline = grid_gdf_baseline['Count'].mean()\n",
    "std_count_baseline = grid_gdf_baseline['Count'].std()\n",
    "CV_baseline = std_count_baseline / mean_count_baseline if mean_count_baseline > 0 else 1\n",
    "baseline_gamma = 1 / (1 + CV_baseline)\n",
    "\n",
    "# Apply gamma scaling directly on raw counts\n",
    "grid_gdf_baseline['Scaled_Count_Baseline'] = (grid_gdf_baseline['Count'] + 1) ** baseline_gamma\n",
    "\n",
    "# Step 2: Normalize the Scaled Counts for Better Visual Comparison\n",
    "scaled_min_baseline = grid_gdf_baseline['Scaled_Count_Baseline'].min()\n",
    "scaled_max_baseline = grid_gdf_baseline['Scaled_Count_Baseline'].max()\n",
    "\n",
    "\n",
    "### ---- ADAPTIVE ----\n",
    "# Define the label used in your code (e.g., \"30 sec\")\n",
    "label = \"360 sec\"\n",
    "\n",
    "# Filter the dataset to include only rows where the sensor is ON\n",
    "adaptive_df = df_temp[df_temp[f'Sensor_ON_{label}'] == True]\n",
    "\n",
    "# Create a GeoDataFrame for the points in df\n",
    "adaptive_df_gdf = gpd.GeoDataFrame(adaptive_df, geometry=gpd.points_from_xy(adaptive_df['Log'], adaptive_df['Lat']), crs=\"EPSG:4326\")\n",
    "\n",
    "# Perform spatial join for Adaptive case\n",
    "joined = gpd.sjoin(adaptive_df_gdf, grid_gdf_adaptive, how=\"left\", predicate=\"within\")\n",
    "grid_gdf_adaptive['Count'] = joined.groupby(joined.index_right).size()\n",
    "grid_gdf_adaptive['Count'].fillna(0, inplace=True)\n",
    "\n",
    "# Step 1: Apply Gamma Scaling on Raw Counts (Without Log Transformation)\n",
    "mean_count_adaptive = grid_gdf_adaptive['Count'].mean()\n",
    "std_count_adaptive = grid_gdf_adaptive['Count'].std()\n",
    "CV_adaptive = std_count_adaptive / mean_count_adaptive if mean_count_adaptive > 0 else 1\n",
    "adaptive_gamma = 1 / (1 + CV_adaptive)\n",
    "\n",
    "# Apply gamma scaling directly on raw counts\n",
    "grid_gdf_adaptive['Scaled_Count_Adaptive'] = (grid_gdf_adaptive['Count'] + 1) ** adaptive_gamma\n",
    "\n",
    "# Step 2: Normalize the Scaled Counts for Better Visual Comparison\n",
    "scaled_min_adaptive = grid_gdf_adaptive['Scaled_Count_Adaptive'].min()\n",
    "scaled_max_adaptive = grid_gdf_adaptive['Scaled_Count_Adaptive'].max()\n",
    "\n",
    "\n",
    "# Global min and max for both datasets\n",
    "global_min = min(scaled_min_baseline, scaled_min_adaptive)\n",
    "global_max = max(scaled_max_baseline, scaled_max_adaptive)\n",
    "\n",
    "print(f\"Global Min: {global_min}, Global Max: {global_max}\")\n",
    "\n",
    "## ViZ\n",
    "grid_gdf_baseline['Normalized_Scaled_Baseline'] = (grid_gdf_baseline['Scaled_Count_Baseline'] - global_min) / (global_max - global_min)\n",
    "# Step 3: Apply Sigmoid Scaling ONLY for Visualization (Baseline)\n",
    "k = 10  # Adjust this value for better contrast enhancement\n",
    "grid_gdf_baseline['Visual_Baseline'] = 1 / (1 + np.exp(-k * (grid_gdf_baseline['Normalized_Scaled_Baseline'] - 0.5)))\n",
    "\n",
    "grid_gdf_adaptive['Normalized_Scaled_Adaptive'] = (grid_gdf_adaptive['Scaled_Count_Adaptive'] - global_min) / (global_max - global_min)\n",
    "# Step 3: Apply Sigmoid Scaling ONLY for Visualization (Adaptive)\n",
    "grid_gdf_adaptive['Visual_Adaptive'] = 1 / (1 + np.exp(-k * (grid_gdf_adaptive['Normalized_Scaled_Adaptive'] - 0.5)))\n",
    "\n",
    "\n",
    "### ---- COMMON VMIN COMPUTATION ----\n",
    "# Compute the common vmax for both Baseline and Adaptive visualizations\n",
    "common_vmin = min(\n",
    "    grid_gdf_baseline['Visual_Baseline'].min(),\n",
    "    grid_gdf_adaptive['Visual_Adaptive'].min()\n",
    ")\n",
    "\n",
    "print(f\"Computed common_vmin: {common_vmin:.4f}\")\n",
    "\n",
    "### ---- COMMON VMAX COMPUTATION ----\n",
    "# Compute the common vmax for both Baseline and Adaptive visualizations\n",
    "common_vmax = max(\n",
    "    grid_gdf_baseline['Visual_Baseline'].quantile(0.99),\n",
    "    grid_gdf_adaptive['Visual_Adaptive'].quantile(0.99)\n",
    ")\n",
    "\n",
    "print(f\"Computed common_vmax: {common_vmax:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make a copy for Baseline to avoid overwriting\n",
    "# grid_gdf_baseline = grid_gdf.copy()\n",
    "\n",
    "# Step 1: Compute Gamma Scaling on Raw Counts (Without Log Transformation)\n",
    "# mean_count_baseline = grid_gdf_baseline['Count'].mean()\n",
    "# std_count_baseline = grid_gdf_baseline['Count'].std()\n",
    "# CV_baseline = std_count_baseline / mean_count_baseline if mean_count_baseline > 0 else 1\n",
    "# baseline_gamma = 1 / (1 + CV_baseline)\n",
    "\n",
    "# Apply gamma scaling directly on raw counts\n",
    "# grid_gdf_baseline['Scaled_Count_Baseline'] = (grid_gdf_baseline['Count'] + 1) ** baseline_gamma\n",
    "\n",
    "# Step 2: Normalize the Scaled Counts for Better Visual Comparison\n",
    "# scaled_min = grid_gdf_baseline['Scaled_Count_Baseline'].min()\n",
    "# scaled_max = grid_gdf_baseline['Scaled_Count_Baseline'].max()\n",
    "# grid_gdf_baseline['Normalized_Scaled_Baseline'] = (grid_gdf_baseline['Scaled_Count_Baseline'] - scaled_min) / (scaled_max - scaled_min)\n",
    "\n",
    "# Step 3: Apply Sigmoid Scaling ONLY for Visualization (Baseline)\n",
    "# k = 10  # Adjust this value for better contrast enhancement\n",
    "# grid_gdf_baseline['Visual_Baseline'] = 1 / (1 + np.exp(-k * (grid_gdf_baseline['Normalized_Scaled_Baseline'] - 0.5)))\n",
    "\n",
    "print(f\"Baseline Optimal Gamma: {baseline_gamma:.4f}\")\n",
    "\n",
    "# Define a colormap\n",
    "colormap = cm.LinearColormap(\n",
    "    colors=['darkblue', 'cyan', 'yellow', 'orangered'],\n",
    "    vmin=common_vmin,\n",
    "    vmax=common_vmax\n",
    ")\n",
    "\n",
    "# Create folium map with a dark basemap\n",
    "m = folium.Map(location=[df['Lat'].mean(), df['Log'].mean()], zoom_start=12, tiles='Cartodb dark_matter')\n",
    "\n",
    "# Add GeoJSON overlay\n",
    "geojson = folium.GeoJson(\n",
    "    grid_gdf_baseline,\n",
    "    style_function=lambda feature: {\n",
    "        'fillColor': colormap(feature['properties']['Visual_Baseline']),\n",
    "        'weight': 0.0,\n",
    "        'fillOpacity': 0.4\n",
    "    }\n",
    ").add_to(m)\n",
    "\n",
    "colormap.caption = \"Log Scaled Count Intensity (Baseline)\"\n",
    "colormap.add_to(m)  # Attach to the map\n",
    "\n",
    "# Show map\n",
    "m"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute baseline statistics\n",
    "baseline_counts = df_temp.groupby(['Lat_Grid', 'Log_Grid']).size()\n",
    "mean_count_baseline = baseline_counts.mean()\n",
    "std_count_baseline = baseline_counts.std()\n",
    "cv_baseline = std_count_baseline / mean_count_baseline\n",
    "\n",
    "print(f\"Baseline Statistics:\")\n",
    "print(f\"Mean Count: {mean_count_baseline:.4f}\")\n",
    "print(f\"Standard Deviation: {std_count_baseline:.4f}\")\n",
    "print(f\"CV (Baseline): {cv_baseline:.4f}\")\n",
    "\n",
    "# Define the label used in your code (e.g., \"30 sec\")\n",
    "label = \"360 sec\"\n",
    "\n",
    "# Filter the dataset to include only rows where the sensor is ON\n",
    "adaptive_df = df_temp[df_temp[f'Sensor_ON_{label}'] == True]\n",
    "\n",
    "# Compute adaptive statistics\n",
    "adaptive_counts = adaptive_df.groupby(['Lat_Grid', 'Log_Grid']).size()\n",
    "mean_count_adaptive = adaptive_counts.mean()\n",
    "std_count_adaptive = adaptive_counts.std()\n",
    "cv_adaptive = std_count_adaptive / mean_count_adaptive\n",
    "\n",
    "print(f\"\\n Adaptive Sensing Statistics ({label}):\")\n",
    "print(f\"Mean Count: {mean_count_adaptive:.4f}\")\n",
    "print(f\"Standard Deviation: {std_count_adaptive:.4f}\")\n",
    "print(f\"CV (Adaptive): {cv_adaptive:.4f}\")\n",
    "\n",
    "# Calculate improvement in uniformity\n",
    "delta_cv = cv_baseline - cv_adaptive\n",
    "percentage_improvement = (delta_cv / cv_baseline) * 100\n",
    "\n",
    "print(f\"\\n Improvement in Spatial Uniformity (Î”CV): {delta_cv:.4f}\")\n",
    "print(f\" Percentage Improvement in Uniformity: {percentage_improvement:.2f}%\")\n",
    "\n",
    "# Plot histogram of counts for baseline and adaptive scenarios\n",
    "plt.figure(figsize=(12, 6))\n",
    "\n",
    "plt.hist(baseline_counts, bins=50, alpha=0.5, label='Baseline (All Measurements)')\n",
    "plt.hist(adaptive_counts, bins=50, alpha=0.5, label=f'Adaptive Sensing ({label})')\n",
    "\n",
    "plt.title('Distribution of Measurements per Grid Cell')\n",
    "plt.xlabel('Number of Measurements per Grid Cell')\n",
    "plt.ylabel('Frequency')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make a copy for Adaptive to avoid overwriting\n",
    "grid_gdf_adaptive = grid_gdf.copy()\n",
    "\n",
    "# Create a GeoDataFrame for the points in df\n",
    "adaptive_df_gdf = gpd.GeoDataFrame(adaptive_df, geometry=gpd.points_from_xy(adaptive_df['Log'], adaptive_df['Lat']), crs=\"EPSG:4326\")\n",
    "\n",
    "# Perform spatial join (vectorized operation)\n",
    "joined = gpd.sjoin(adaptive_df_gdf, grid_gdf_adaptive, how=\"left\", predicate=\"within\")\n",
    "\n",
    "# Count points per grid cell\n",
    "grid_gdf_adaptive['Count'] = joined.groupby(joined.index_right).size()\n",
    "\n",
    "# Fill NaN with 0 for empty grid cells\n",
    "grid_gdf_adaptive['Count'].fillna(0, inplace=True)\n",
    "\n",
    "# Step 1: Apply Gamma Scaling on Raw Counts (Without Log Transformation)\n",
    "mean_count_adaptive = grid_gdf_adaptive['Count'].mean()\n",
    "std_count_adaptive = grid_gdf_adaptive['Count'].std()\n",
    "CV_adaptive = std_count_adaptive / mean_count_adaptive if mean_count_adaptive > 0 else 1\n",
    "adaptive_gamma = 1 / (1 + CV_adaptive)\n",
    "\n",
    "# Apply gamma scaling directly on raw counts\n",
    "grid_gdf_adaptive['Scaled_Count_Adaptive'] = (grid_gdf_adaptive['Count'] + 1) ** adaptive_gamma\n",
    "\n",
    "# Step 2: Normalize the Scaled Counts for Better Visual Comparison\n",
    "scaled_min = grid_gdf_adaptive['Scaled_Count_Adaptive'].min()\n",
    "scaled_max = grid_gdf_adaptive['Scaled_Count_Adaptive'].max()\n",
    "grid_gdf_adaptive['Normalized_Scaled_Adaptive'] = (grid_gdf_adaptive['Scaled_Count_Adaptive'] - scaled_min) / (scaled_max - scaled_min)\n",
    "\n",
    "# Step 3: Apply Sigmoid Scaling ONLY for Visualization (Adaptive)\n",
    "grid_gdf_adaptive['Visual_Adaptive'] = 1 / (1 + np.exp(-k * (grid_gdf_adaptive['Normalized_Scaled_Adaptive'] - 0.5)))\n",
    "\n",
    "print(f\"Adaptive Optimal Gamma: {adaptive_gamma:.4f}\")\n",
    "\n",
    "# Define a colormap\n",
    "colormap = cm.LinearColormap(\n",
    "    colors=['darkblue', 'cyan', 'yellow', 'orangered'],\n",
    "    vmin=grid_gdf_adaptive['Visual_Adaptive'].min(),\n",
    "    vmax=common_vmax\n",
    ")\n",
    "\n",
    "# Create folium map with a dark basemap\n",
    "m = folium.Map(location=[df['Lat'].mean(), df['Log'].mean()], zoom_start=12, tiles='Cartodb dark_matter')\n",
    "\n",
    "# Add GeoJSON overlay for the adaptive map\n",
    "geojson = folium.GeoJson(\n",
    "    grid_gdf_adaptive,\n",
    "    style_function=lambda feature: {\n",
    "        'fillColor': colormap(feature['properties']['Visual_Adaptive']),\n",
    "        'weight': 0.0,\n",
    "        'fillOpacity': 0.4\n",
    "    }\n",
    ").add_to(m)\n",
    "\n",
    "# Add the color legend\n",
    "colormap.caption = \"Log Scaled Count Intensity (Adaptive)\"\n",
    "colormap.add_to(m)\n",
    "\n",
    "# Show map\n",
    "m"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Ensure Both Visual Maps Are Normalized Between 0 and 1\n",
    "visual_baseline_min = grid_gdf_baseline['Visual_Baseline'].min()\n",
    "visual_baseline_max = grid_gdf_baseline['Visual_Baseline'].max()\n",
    "grid_gdf_baseline['Normalized_Visual_Baseline'] = (grid_gdf_baseline['Visual_Baseline'] - visual_baseline_min) / (visual_baseline_max - visual_baseline_min)\n",
    "\n",
    "visual_adaptive_min = grid_gdf_adaptive['Visual_Adaptive'].min()\n",
    "visual_adaptive_max = grid_gdf_adaptive['Visual_Adaptive'].max()\n",
    "grid_gdf_adaptive['Normalized_Visual_Adaptive'] = (grid_gdf_adaptive['Visual_Adaptive'] - visual_adaptive_min) / (visual_adaptive_max - visual_adaptive_min)\n",
    "\n",
    "# Step 2: Calculate Relative Improvement (Enhanced Difference Calculation)\n",
    "grid_gdf_adaptive['Difference'] = grid_gdf_adaptive['Normalized_Visual_Adaptive'] - grid_gdf_baseline['Normalized_Visual_Baseline']\n",
    "grid_gdf_adaptive['Difference_Sign'] = np.sign(grid_gdf_adaptive['Difference'])\n",
    "\n",
    "# Sigmoid Scaling (Adjust k for more/less aggressive enhancement)\n",
    "k = 30  # Increase for stronger contrast, decrease for smoother visualization\n",
    "grid_gdf_adaptive['Enhanced_Difference'] = grid_gdf_adaptive['Difference_Sign'] * (1 / (1 + np.exp(-k * grid_gdf_adaptive['Difference'])) - 0.5)\n",
    "\n",
    "# Step 4: Normalization of Enhanced Differences for Optimal Visualization\n",
    "enhanced_diff_min = grid_gdf_adaptive['Enhanced_Difference'].min()\n",
    "enhanced_diff_max = grid_gdf_adaptive['Enhanced_Difference'].max()\n",
    "\n",
    "# Normalizing to range [0, 1] for Blue intensities\n",
    "grid_gdf_adaptive['Enhanced_Difference_Vis'] = (grid_gdf_adaptive['Enhanced_Difference'] - enhanced_diff_min) / (enhanced_diff_max - enhanced_diff_min)\n",
    "\n",
    "# Step 5: Define a More Sensitive Colormap with Strong Contrast (Optimized Blue Scale)\n",
    "colormap = cm.LinearColormap(\n",
    "    colors=['white','blue'],  # Only Blue to highlight improvements\n",
    "    vmin=0,  # Minimum of the normalized data\n",
    "    vmax=1   # Maximum of the normalized data\n",
    ")\n",
    "\n",
    "# Step 6: Create folium map to visualize differences\n",
    "m = folium.Map(location=[df['Lat'].mean(), df['Log'].mean()], zoom_start=12, tiles='Cartodb dark_matter')\n",
    "\n",
    "# Add GeoJSON overlay for the difference map\n",
    "geojson = folium.GeoJson(\n",
    "    grid_gdf_adaptive,\n",
    "    style_function=lambda feature: {\n",
    "        'fillColor': colormap(feature['properties']['Enhanced_Difference_Vis']),\n",
    "        'weight': 0.0,\n",
    "        'fillOpacity': 0.4\n",
    "    }\n",
    ").add_to(m)\n",
    "\n",
    "# Add the color legend\n",
    "colormap.caption = \"Improvement Map (Uniformity Improvement in Blue)\"\n",
    "colormap.add_to(m)\n",
    "\n",
    "# Show map\n",
    "m\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize raw counts for Baseline\n",
    "colormap_baseline_raw = cm.LinearColormap(\n",
    "    colors=['darkblue', 'cyan', 'yellow', 'orangered'],\n",
    "    vmin=grid_gdf_baseline['Count'].min(),\n",
    "    vmax=grid_gdf_baseline['Count'].quantile(0.99)  # Clipping extreme values for better visualization\n",
    ")\n",
    "\n",
    "m1 = folium.Map(location=[df['Lat'].mean(), df['Log'].mean()], zoom_start=12, tiles='Cartodb dark_matter')\n",
    "geojson_baseline_raw = folium.GeoJson(\n",
    "    grid_gdf_baseline,\n",
    "    style_function=lambda feature: {\n",
    "        'fillColor': colormap_baseline_raw(feature['properties']['Count']),\n",
    "        'weight': 0.0,\n",
    "        'fillOpacity': 0.4\n",
    "    }\n",
    ").add_to(m1)\n",
    "\n",
    "colormap_baseline_raw.caption = \"Raw Count Intensity (Baseline)\"\n",
    "colormap_baseline_raw.add_to(m1)\n",
    "m1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize raw counts for Adaptive\n",
    "colormap_adaptive_raw = cm.LinearColormap(\n",
    "    colors=['darkblue', 'cyan', 'yellow', 'orangered'],\n",
    "    vmin=grid_gdf_adaptive['Count'].min(),\n",
    "    vmax=grid_gdf_adaptive['Count'].quantile(0.99)\n",
    ")\n",
    "\n",
    "m2 = folium.Map(location=[df['Lat'].mean(), df['Log'].mean()], zoom_start=12, tiles='Cartodb dark_matter')\n",
    "geojson_adaptive_raw = folium.GeoJson(\n",
    "    grid_gdf_adaptive,\n",
    "    style_function=lambda feature: {\n",
    "        'fillColor': colormap_adaptive_raw(feature['properties']['Count']),\n",
    "        'weight': 0.0,\n",
    "        'fillOpacity': 0.4\n",
    "    }\n",
    ").add_to(m2)\n",
    "\n",
    "colormap_adaptive_raw.caption = \"Raw Count Intensity (Adaptive)\"\n",
    "colormap_adaptive_raw.add_to(m2)\n",
    "m2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sigmoid Scaling Function\n",
    "def sigmoid(x):\n",
    "    return 1 / (1 + np.exp(-10 * (x - 0.5)))\n",
    "\n",
    "# Apply sigmoid and arctangent scaling for better contrast\n",
    "grid_gdf_baseline['Visual_Baseline_Sigmoid'] = sigmoid(grid_gdf_baseline['Normalized_Scaled_Baseline'])\n",
    "grid_gdf_adaptive['Visual_Adaptive_Sigmoid'] = sigmoid(grid_gdf_adaptive['Normalized_Scaled_Adaptive'])\n",
    "\n",
    "# Visualize Sigmoid-scaled Adaptive Map\n",
    "colormap_sigmoid = cm.LinearColormap(\n",
    "    colors=['darkblue', 'cyan', 'yellow', 'orangered'],\n",
    "    vmin=0,\n",
    "    vmax=1\n",
    ")\n",
    "\n",
    "m_sigmoid = folium.Map(location=[df['Lat'].mean(), df['Log'].mean()], zoom_start=12, tiles='Cartodb dark_matter')\n",
    "geojson_sigmoid = folium.GeoJson(\n",
    "    grid_gdf_adaptive,\n",
    "    style_function=lambda feature: {\n",
    "        'fillColor': colormap_sigmoid(feature['properties']['Visual_Adaptive_Sigmoid']),\n",
    "        'weight': 0.0,\n",
    "        'fillOpacity': 0.4\n",
    "    }\n",
    ").add_to(m_sigmoid)\n",
    "\n",
    "colormap_sigmoid.caption = \"Sigmoid Scaled Intensity (Adaptive)\"\n",
    "colormap_sigmoid.add_to(m_sigmoid)\n",
    "m_sigmoid\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply Sigmoid Scaling to Baseline\n",
    "grid_gdf_baseline['Visual_Baseline_Sigmoid'] = sigmoid(grid_gdf_baseline['Normalized_Scaled_Baseline'])\n",
    "\n",
    "# Define colormap for Baseline using Sigmoid Scaling\n",
    "colormap_baseline_sigmoid = cm.LinearColormap(\n",
    "    colors=['darkblue', 'cyan', 'yellow', 'orangered'],\n",
    "    vmin=0,\n",
    "    vmax=1\n",
    ")\n",
    "\n",
    "# Create a Folium map for the Baseline Scenario\n",
    "m_baseline_sigmoid = folium.Map(location=[df['Lat'].mean(), df['Log'].mean()], zoom_start=12, tiles='Cartodb dark_matter')\n",
    "\n",
    "# Add GeoJSON overlay for the Baseline map\n",
    "geojson_baseline_sigmoid = folium.GeoJson(\n",
    "    grid_gdf_baseline,\n",
    "    style_function=lambda feature: {\n",
    "        'fillColor': colormap_baseline_sigmoid(feature['properties']['Visual_Baseline_Sigmoid']),\n",
    "        'weight': 0.0,\n",
    "        'fillOpacity': 0.4\n",
    "    }\n",
    ").add_to(m_baseline_sigmoid)\n",
    "\n",
    "# Add the color legend\n",
    "colormap_baseline_sigmoid.caption = \"Sigmoid Scaled Intensity (Baseline)\"\n",
    "colormap_baseline_sigmoid.add_to(m_baseline_sigmoid)\n",
    "\n",
    "# Show the map\n",
    "m_baseline_sigmoid\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Markov-Based Spatial Transition Modeling for Vehicle Movement and Sensor Depletion Analysis**\n",
    "\n",
    "This methodology enables **data-driven mobility prediction** and **battery depletion analysis** using **Markovian dynamics**.\n",
    "\n",
    "## **1. Assigning Vehicles to Grid Cells Using Polygon Containment**\n",
    "Given a dataset of GPS points (`Lat`, `Log`), we spatially bin the data into a **120m x 120m** grid using polygon containment:\n",
    "\n",
    "- Convert each GPS point into a **GeoDataFrame** (`df_gdf`).\n",
    "- Each point is assigned to its corresponding grid cell using the spatial containment function:\n",
    "\n",
    "  $$\n",
    "  G(i) = \\arg\\max_j \\mathbb{1}(p_i \\in P_j)\n",
    "  $$\n",
    "\n",
    "  where:\n",
    "  - $ p_i $ is the point geometry of record $ i $,\n",
    "  - $ P_j $ is the polygon geometry of grid cell $ j $,\n",
    "  - $ \\mathbb{1}(p_i \\in P_j) $ is an indicator function that is **1** if $ p_i $ is inside $ P_j $, else **0**.\n",
    "\n",
    "- Any points that **do not match** a grid cell are treated as **outliers** and removed.\n",
    "\n",
    "## **2. Temporal Sorting for Transition Analysis**\n",
    "To analyze transitions between grid cells, the dataset is **sorted** by:\n",
    "\n",
    "$$\n",
    "\\text{Sort}(df, \\text{by}=[\\text{deviceID}, \\text{Timestamp}])\n",
    "$$\n",
    "\n",
    "where:\n",
    "- `deviceID` ensures sorting is done **per vehicle**,\n",
    "- `Timestamp` orders data points **chronologically**.\n",
    "\n",
    "## **3. Identifying Sensor Depletion Events**\n",
    "A **battery depletion threshold** is defined as:\n",
    "\n",
    "$$\n",
    "SOC_{\\text{batt}} < 50\\%\n",
    "$$\n",
    "\n",
    "where **State of Charge (SOC)** is the batteryâ€™s remaining capacity. We create a binary indicator:\n",
    "\n",
    "$$\n",
    "D_i = \n",
    "\\begin{cases} \n",
    "1, & SOC_{\\text{batt}, i} < 50\\% \\\\\n",
    "0, & \\text{otherwise}\n",
    "\\end{cases}\n",
    "$$\n",
    "\n",
    "- **Pre-Depletion Data**: $ D_i = 0 $ (battery level above threshold).\n",
    "- **Post-Depletion Data**: $ D_i = 1 $ (battery level below threshold).\n",
    "\n",
    "## **4. Constructing Grid Cell Transitions**\n",
    "To model **spatial movement**, we define a transition as:\n",
    "\n",
    "$$\n",
    "T_i = (G_i, G_{i+1})\n",
    "$$\n",
    "\n",
    "where:\n",
    "- $ G_i $ is the grid cell at time $ t_i $,\n",
    "- $ G_{i+1} $ is the **next** grid cell at $ t_{i+1} $.\n",
    "\n",
    "We compute the **state transitions** for each vehicle:\n",
    "\n",
    "$$\n",
    "\\text{Next\\_Grid\\_Cell} = G_{i+1} = \\text{shift}(G_i, -1)\n",
    "$$\n",
    "\n",
    "Dropping the last row for each vehicle ensures only **valid transitions** are included.\n",
    "\n",
    "## **5. Constructing the Markov Transition Matrix**\n",
    "A **first-order Markov model** is constructed, where each transition probability is estimated as:\n",
    "\n",
    "$$\n",
    "P(G_{i+1} | G_i) = \\frac{N(G_i \\to G_{i+1})}{\\sum_{G_j} N(G_i \\to G_j)}\n",
    "$$\n",
    "\n",
    "where:\n",
    "- $ N(G_i \\to G_{i+1}) $ is the **count** of observed transitions from $ G_i $ to $ G_{i+1} $,\n",
    "- The denominator sums over **all possible** next states.\n",
    "\n",
    "The **transition matrix** $ P $ is structured as:\n",
    "\n",
    "$$\n",
    "P = \\begin{bmatrix}\n",
    "P(G_1 | G_1) & P(G_2 | G_1) & \\dots & P(G_n | G_1) \\\\\n",
    "P(G_1 | G_2) & P(G_2 | G_2) & \\dots & P(G_n | G_2) \\\\\n",
    "\\vdots & \\vdots & \\ddots & \\vdots \\\\\n",
    "P(G_1 | G_n) & P(G_2 | G_n) & \\dots & P(G_n | G_n)\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "This is a **stochastic matrix**, where each row sums to **1**:\n",
    "\n",
    "$$\n",
    "\\sum_{G_{i+1}} P(G_{i+1} | G_i) = 1, \\quad \\forall G_i\n",
    "$$\n",
    "\n",
    "## **6. Predicting the Most Likely Next Grid Cell**\n",
    "For a given grid cell $ G_i $, the predicted **next location** is:\n",
    "\n",
    "$$\n",
    "G_{\\text{predicted}} = \\arg\\max_{G_{i+1}} P(G_{i+1} | G_i)\n",
    "$$\n",
    "\n",
    "This follows a **greedy decision rule**, selecting the most probable transition based on historical data.\n",
    "\n",
    "## **7. Validation Against Post-Depletion Data**\n",
    "For vehicles that **experienced battery depletion**:\n",
    "\n",
    "- The predicted transition is compared to the **actual** next grid cell.\n",
    "- A **correct prediction** is counted if:\n",
    "\n",
    "  $$\n",
    "  G_{\\text{predicted}} = G_{\\text{actual}}\n",
    "  $$\n",
    "\n",
    "- The **prediction accuracy** is computed as:\n",
    "\n",
    "  $$\n",
    "  \\text{Accuracy} = \\frac{\\sum \\mathbb{1}(G_{\\text{predicted}} = G_{\\text{actual}})}{N_{\\text{post-depletion}}}\n",
    "  $$\n",
    "\n",
    "where $ N_{\\text{post-depletion}} $ is the total number of post-depletion data points.\n",
    "\n",
    "## **8. Output and Insights**\n",
    "- The **transition matrix** is saved for further analysis.\n",
    "- The **post-depletion validation results** are stored.\n",
    "- The **top 10 most likely transitions** are displayed, providing insight into dominant movement patterns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## 1ST ORDER MC\n",
    "\n",
    "# Step 1: Assign Vehicles to Grid Cells Using Polygon Containment\n",
    "#df_gdf = gpd.GeoDataFrame(df, geometry=gpd.points_from_xy(df['Log'], df['Lat']), crs=\"EPSG:4326\")\n",
    "\n",
    "# Function to find which grid cell a point belongs to\n",
    "def find_grid_cell(point, grid_gdf):\n",
    "    match = grid_gdf.contains(point.geometry)\n",
    "    if match.any():\n",
    "        return match.idxmax()  # Return index of matching grid cell\n",
    "    else:\n",
    "        return None  # No match found\n",
    "\n",
    "# Apply function to assign each GPS point to a grid cell\n",
    "df_gdf['Grid_Cell'] = df_gdf.apply(lambda row: find_grid_cell(row, grid_gdf), axis=1)\n",
    "\n",
    "# Drop rows where no grid cell was matched (outliers)\n",
    "df_gdf = df_gdf.dropna(subset=['Grid_Cell'])\n",
    "\n",
    "# Step 2: Sort by deviceID and Timestamp for Transition Analysis\n",
    "df_gdf = df_gdf.sort_values(by=['deviceID', 'Timestamp'])\n",
    "\n",
    "# Step 3: Identify Sensor Depletion (SOC_batt < 30)\n",
    "df_gdf['Depleted'] = df_gdf['SOC_batt'] < df_gdf['Safe_SOC_Threshold']\n",
    "\n",
    "# Step 4: Separate Pre- and Post-Depletion Data\n",
    "df_pre_depletion = df_gdf[~df_gdf['Depleted']].copy()\n",
    "df_post_depletion = df_gdf[df_gdf['Depleted']].copy()\n",
    "\n",
    "# Step 5: Create Transitions (from one grid cell to the next)\n",
    "df_pre_depletion['Next_Grid_Cell'] = df_pre_depletion.groupby('deviceID')['Grid_Cell'].shift(-1)\n",
    "\n",
    "# Drop last row per vehicle (no next transition available)\n",
    "df_transitions = df_pre_depletion.dropna(subset=['Next_Grid_Cell'])\n",
    "\n",
    "# Step 6: Build the Markov Transition Matrix\n",
    "transition_counts = (\n",
    "    df_transitions.groupby(['Grid_Cell', 'Next_Grid_Cell'])\n",
    "    .size()\n",
    "    .unstack(fill_value=0)\n",
    ")\n",
    "transition_probabilities = transition_counts.div(transition_counts.sum(axis=1), axis=0)  # Normalize to probabilities\n",
    "\n",
    "# Step 7: Define a Function to Predict the Next Grid Cell\n",
    "def predict_next_grid(current_grid, transition_matrix):\n",
    "    if current_grid in transition_matrix.index:\n",
    "        return transition_matrix.loc[current_grid].idxmax()  # Most likely transition\n",
    "    else:\n",
    "        return None  # No transition data available\n",
    "\n",
    "# Step 8: Validate Predictions Using Post-Depletion Data\n",
    "df_post_depletion['Predicted_Grid_Cell'] = df_post_depletion['Grid_Cell'].apply(\n",
    "    lambda x: predict_next_grid(x, transition_probabilities)\n",
    ")\n",
    "\n",
    "# Step 9: Measure Prediction Accuracy\n",
    "df_post_depletion['Correct_Prediction'] = df_post_depletion['Predicted_Grid_Cell'] == df_post_depletion['Grid_Cell']\n",
    "accuracy = df_post_depletion['Correct_Prediction'].mean()\n",
    "\n",
    "print(f\"Prediction Accuracy: {accuracy:.2%}\")\n",
    "\n",
    "# # Step 10: Save the Transition Matrix and Post-Depletion Validation\n",
    "# output_path_transition_matrix = \"/Users/mayar/Desktop/MIT/Research_Fellow/ENERGY_SENSING/DATA/Transition_Matrix.xlsx\"\n",
    "# transition_probabilities.to_excel(output_path_transition_matrix)\n",
    "\n",
    "# output_path_post_depletion = \"/Users/mayar/Desktop/MIT/Research_Fellow/ENERGY_SENSING/DATA/Post_Depletion_Validation.xlsx\"\n",
    "# df_post_depletion.to_excel(output_path_post_depletion, index=False)\n",
    "\n",
    "# # Step 11: Analyze and Print the Top 10 Most Likely Transitions\n",
    "# most_likely_transitions = (\n",
    "#     transition_probabilities.stack()\n",
    "#     .reset_index()\n",
    "#     .rename(columns={0: 'Probability', 'level_0': 'From_Grid', 'level_1': 'To_Grid'})\n",
    "#     .sort_values(by='Probability', ascending=False)\n",
    "# )\n",
    "\n",
    "# print(\"Top 10 Most Likely Transitions:\")\n",
    "# print(most_likely_transitions.head(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# **ðŸ”¹ Use Spatial Indexing for Fast Grid Cell Lookup**\n",
    "grid_sindex = grid_gdf.sindex\n",
    "\n",
    "def find_grid_cell(point):\n",
    "    possible_matches = list(grid_sindex.intersection(point.bounds))  # âœ… FIXED: Use `.bounds`\n",
    "    for match in possible_matches:\n",
    "        if grid_gdf.iloc[match].geometry.contains(point):  # âœ… FIXED: Use `point` directly\n",
    "            return grid_gdf.index[match]  # Return grid cell index\n",
    "    return None\n",
    "\n",
    "# **ðŸ”¹ Apply Fast Lookup in Parallel**\n",
    "df_gdf['Grid_Cell'] = df_gdf['geometry'].map(find_grid_cell)  # âœ… FIXED: Ensure correct `.map()` usage\n",
    "\n",
    "# Drop rows where no grid cell was matched\n",
    "df_gdf.dropna(subset=['Grid_Cell'], inplace=True)\n",
    "\n",
    "# Step 2: Sort by deviceID and Timestamp for Transition Analysis\n",
    "df_gdf.sort_values(by=['deviceID', 'Timestamp'], inplace=True)\n",
    "\n",
    "# Step 3: Identify Sensor Depletion \n",
    "depletion_threshold = df_gdf['Safe_SOC_Threshold']\n",
    "df_gdf['Depleted'] = df_gdf['SOC_batt'] < depletion_threshold\n",
    "\n",
    "# Step 4: Separate Pre- and Post-Depletion Data\n",
    "df_pre_depletion = df_gdf[~df_gdf['Depleted']].copy()\n",
    "df_post_depletion = df_gdf[df_gdf['Depleted']].copy()\n",
    "\n",
    "# Step 5: Compute Transitions Efficiently\n",
    "df_pre_depletion['Next_Grid_Cell'] = df_pre_depletion.groupby('deviceID')['Grid_Cell'].shift(-1)\n",
    "df_transitions = df_pre_depletion.dropna(subset=['Next_Grid_Cell'])\n",
    "\n",
    "# **ðŸ”¹ Fast Markov Transition Matrix Computation (Numba-Optimized)**\n",
    "@numba.njit(parallel=True)\n",
    "def compute_transition_matrix(transitions):\n",
    "    unique_states = np.unique(transitions)  # Extract unique states\n",
    "    num_states = len(unique_states)\n",
    "    \n",
    "    # **Numba-Compatible Mapping (Replace Dictionary)**\n",
    "    state_map = {state: i for i, state in enumerate(unique_states)}  # Index mapping\n",
    "    \n",
    "    # **Initialize Transition Matrix**\n",
    "    matrix = np.zeros((num_states, num_states), dtype=np.float32)\n",
    "\n",
    "    # **Compute State Transitions in Parallel**\n",
    "    for i in numba.prange(len(transitions) - 1):\n",
    "        if transitions[i] in state_map and transitions[i + 1] in state_map:\n",
    "            from_idx = state_map[transitions[i]]\n",
    "            to_idx = state_map[transitions[i + 1]]\n",
    "            matrix[from_idx, to_idx] += 1  # Increment count\n",
    "\n",
    "    # **Normalize Matrix (Convert to Probabilities)**\n",
    "    row_sums = matrix.sum(axis=1)\n",
    "    for i in range(num_states):\n",
    "        if row_sums[i] > 0:\n",
    "            matrix[i, :] /= row_sums[i]\n",
    "\n",
    "    return matrix, unique_states  # Return transition matrix & corresponding states\n",
    "\n",
    "# **ðŸ”¹ Convert transitions to NumPy for Fast Processing**\n",
    "transitions_np = df_transitions[['Grid_Cell', 'Next_Grid_Cell']].to_numpy().flatten()\n",
    "\n",
    "# **ðŸš€ Compute Transition Matrix Using Numba**\n",
    "transition_matrix, state_list = compute_transition_matrix(transitions_np)\n",
    "\n",
    "# **Convert Back to Pandas DataFrame**\n",
    "transition_df = pd.DataFrame(transition_matrix, index=state_list, columns=state_list)\n",
    "\n",
    "# **ðŸ”¹ Step 7: Optimized Prediction Function**\n",
    "def predict_next_grid_batch(grid_cells, transition_df):\n",
    "    return [transition_df.loc[cell].idxmax() if cell in transition_df.index else None for cell in grid_cells]\n",
    "\n",
    "# **Parallel Prediction**\n",
    "df_post_depletion['Predicted_Grid_Cell'] = predict_next_grid_batch(df_post_depletion['Grid_Cell'], transition_df)\n",
    "\n",
    "# **Step 8: Measure Prediction Accuracy**\n",
    "df_post_depletion['Correct_Prediction'] = df_post_depletion['Predicted_Grid_Cell'] == df_post_depletion['Grid_Cell']\n",
    "accuracy = df_post_depletion['Correct_Prediction'].mean()\n",
    "\n",
    "print(f\"Optimized Prediction Accuracy: {accuracy:.2%}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# **Step 1: Convert GPS Data to Geospatial Format**\n",
    "df_gdf = gpd.GeoDataFrame(df, geometry=gpd.points_from_xy(df['Log'], df['Lat']), crs=\"EPSG:4326\")\n",
    "\n",
    "# **ðŸ”¹ Use Spatial Indexing for Fast Grid Cell Lookup**\n",
    "grid_sindex = grid_gdf.sindex\n",
    "\n",
    "def find_grid_cell(point):\n",
    "    possible_matches = list(grid_sindex.intersection(point.bounds))\n",
    "    for match in possible_matches:\n",
    "        if grid_gdf.iloc[match].geometry.contains(point):\n",
    "            return grid_gdf.index[match]  # Return grid cell index\n",
    "    return None\n",
    "\n",
    "# **ðŸ”¹ Apply Fast Lookup in Parallel**\n",
    "df_gdf['Grid_Cell'] = df_gdf['geometry'].map(find_grid_cell)\n",
    "df_gdf.dropna(subset=['Grid_Cell'], inplace=True)\n",
    "\n",
    "# **Step 2: Merge Computed Safe SOC Thresholds**\n",
    "df_gdf = df_gdf.merge(safe_soc_thresholds_df, on=['deviceID', 'Timestamp'], how='left')\n",
    "\n",
    "# **Step 3: Sort by deviceID and Timestamp for Transition Analysis**\n",
    "df_gdf.sort_values(by=['deviceID', 'Timestamp'], inplace=True)\n",
    "\n",
    "# **Step 4: Identify Sensor Depletion Using Dynamic Safe SOC**\n",
    "df_gdf['Depleted'] = df_gdf['SOC_batt'] < df_gdf['Safe_SOC_Threshold']  # âœ… Replaced static threshold\n",
    "\n",
    "# **Step 5: Separate Pre- and Post-Depletion Data**\n",
    "df_pre_depletion = df_gdf[~df_gdf['Depleted']].copy()\n",
    "df_post_depletion = df_gdf[df_gdf['Depleted']].copy()\n",
    "\n",
    "# **Step 6: Compute Next Unique Grid Cell for the Entire Dataset**\n",
    "def find_next_different(series):\n",
    "    \"\"\"Finds the next different grid cell for each row within a deviceID group.\"\"\"\n",
    "    next_values = series.to_numpy()\n",
    "    result = np.full_like(next_values, fill_value=np.nan, dtype=np.float64)\n",
    "\n",
    "    for i in range(len(next_values) - 1):  \n",
    "        for j in range(i + 1, len(next_values)):  # Look ahead to find the next different value\n",
    "            if next_values[j] != next_values[i]:  \n",
    "                result[i] = next_values[j]  # Assign the first different value found\n",
    "                break  \n",
    "\n",
    "    return pd.Series(result, index=series.index)  # Ensure index remains unchanged\n",
    "\n",
    "# âœ… Apply Next Unique Grid Cell to the Entire Dataset (NOT JUST df_pre_depletion)\n",
    "df_gdf['Next_Grid_Cell'] = df_gdf.groupby('deviceID')['Grid_Cell'].transform(find_next_different)\n",
    "df_transitions = df_gdf.dropna(subset=['Next_Grid_Cell'])  # Train on full dataset\n",
    "\n",
    "# **ðŸ”¹ Fast Markov Transition Matrix Computation (Numba-Optimized)**\n",
    "@numba.njit(parallel=True)\n",
    "def compute_transition_matrix(transitions):\n",
    "    unique_states = np.unique(transitions)  \n",
    "    num_states = len(unique_states)\n",
    "\n",
    "    # **Numba-Compatible Mapping**\n",
    "    state_map = {state: i for i, state in enumerate(unique_states)}\n",
    "\n",
    "    # **Initialize Transition Matrix**\n",
    "    matrix = np.zeros((num_states, num_states), dtype=np.float32)\n",
    "\n",
    "    # **Compute State Transitions in Parallel**\n",
    "    for i in numba.prange(len(transitions) - 1):\n",
    "        if transitions[i] in state_map and transitions[i + 1] in state_map:\n",
    "            from_idx = state_map[transitions[i]]\n",
    "            to_idx = state_map[transitions[i + 1]]\n",
    "            matrix[from_idx, to_idx] += 1  # Increment count\n",
    "\n",
    "    # **Normalize Matrix (Convert to Probabilities)**\n",
    "    row_sums = matrix.sum(axis=1)\n",
    "    for i in range(num_states):\n",
    "        if row_sums[i] > 0:\n",
    "            matrix[i, :] /= row_sums[i]\n",
    "\n",
    "    return matrix, unique_states  \n",
    "\n",
    "# **ðŸ”¹ Convert transitions to NumPy for Fast Processing**\n",
    "transitions_np = df_transitions[['Grid_Cell', 'Next_Grid_Cell']].to_numpy().flatten()\n",
    "\n",
    "# **ðŸš€ Compute Transition Matrix Using Numba**\n",
    "transition_matrix, state_list = compute_transition_matrix(transitions_np)\n",
    "\n",
    "# **Convert Back to Pandas DataFrame**\n",
    "transition_df = pd.DataFrame(transition_matrix, index=state_list, columns=state_list)\n",
    "\n",
    "# **ðŸ”¹ Step 7: Optimized Prediction Function**\n",
    "def predict_next_grid_batch(grid_cells, transition_df):\n",
    "    return [transition_df.loc[cell].idxmax() if cell in transition_df.index else None for cell in grid_cells]\n",
    "\n",
    "# **Use `Next_Grid_Cell` from Full Dataset for Predictions**\n",
    "df_post_depletion['Next_Grid_Cell'] = df_post_depletion.groupby('deviceID')['Grid_Cell'].transform(find_next_different)\n",
    "\n",
    "# **Parallel Prediction**\n",
    "df_post_depletion['Predicted_Grid_Cell'] = predict_next_grid_batch(df_post_depletion['Next_Grid_Cell'], transition_df)\n",
    "\n",
    "# âœ… Reorder Columns to Place Next_Grid_Cell After Depleted\n",
    "column_order = df_post_depletion.columns.tolist()\n",
    "column_order.remove('Next_Grid_Cell')  # Remove if exists\n",
    "depleted_index = column_order.index('Depleted')\n",
    "column_order.insert(depleted_index + 1, 'Next_Grid_Cell')  # Insert after Depleted\n",
    "df_post_depletion = df_post_depletion[column_order]  # Apply new column order\n",
    "\n",
    "# **Step 8: Measure Prediction Accuracy**\n",
    "df_post_depletion['Correct_Prediction'] = (\n",
    "    df_post_depletion['Predicted_Grid_Cell'].fillna(\"\").astype(str)\n",
    "    == df_post_depletion['Grid_Cell'].fillna(\"\").astype(str)\n",
    ")\n",
    "accuracy = df_post_depletion['Correct_Prediction'].mean()\n",
    "\n",
    "print(f\"Optimized Prediction Accuracy: {accuracy:.2%}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save DataFrame to Excel file\n",
    "df_post_depletion.to_excel(\"/workspace/data/df_post_depletion_1.xlsx\", index=False)\n",
    "df_pre_depletion.to_excel(\"/workspace/data/df_pre_depletion_1.xlsx\", index=False)\n",
    "\n",
    "# Return file path for download\n",
    "file_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # 2ND ORDER MC\n",
    "# # Step 1: Assign Vehicles to Grid Cells Using Polygon Containment\n",
    "# df_gdf = gpd.GeoDataFrame(df, geometry=gpd.points_from_xy(df['Log'], df['Lat']), crs=\"EPSG:4326\")\n",
    "\n",
    "# # Function to find which grid cell a point belongs to\n",
    "# def find_grid_cell(point, grid_gdf):\n",
    "#     match = grid_gdf.contains(point.geometry)\n",
    "#     if match.any():\n",
    "#         return match.idxmax()  # Return index of matching grid cell\n",
    "#     else:\n",
    "#         return None  # No match found\n",
    "\n",
    "# # Apply function to assign each GPS point to a grid cell\n",
    "# df_gdf['Grid_Cell'] = df_gdf.apply(lambda row: find_grid_cell(row, grid_gdf), axis=1)\n",
    "\n",
    "# # Drop rows where no grid cell was matched (outliers)\n",
    "# df_gdf = df_gdf.dropna(subset=['Grid_Cell'])\n",
    "\n",
    "# # Step 2: Sort by deviceID and Timestamp for Transition Analysis\n",
    "# df_gdf = df_gdf.sort_values(by=['deviceID', 'Timestamp'])\n",
    "\n",
    "# # Step 3: Identify Sensor Depletion (SOC_batt < 50)\n",
    "# depletion_threshold = 30\n",
    "# df_gdf['Depleted'] = df_gdf['SOC_batt'] < depletion_threshold\n",
    "\n",
    "# # Step 4: Separate Pre- and Post-Depletion Data\n",
    "# df_pre_depletion = df_gdf[~df_gdf['Depleted']].copy()\n",
    "# df_post_depletion = df_gdf[df_gdf['Depleted']].copy()\n",
    "\n",
    "# # Step 5: Create Second-Order Transitions (considering two previous grid cells)\n",
    "# df_pre_depletion['Prev_Grid_Cell'] = df_pre_depletion.groupby('deviceID')['Grid_Cell'].shift(1)\n",
    "# df_pre_depletion['Next_Grid_Cell'] = df_pre_depletion.groupby('deviceID')['Grid_Cell'].shift(-1)\n",
    "\n",
    "# # Drop rows where not enough history is available\n",
    "# df_transitions_2nd = df_pre_depletion.dropna(subset=['Prev_Grid_Cell', 'Next_Grid_Cell'])\n",
    "\n",
    "# # Step 6: Build the Second-Order Transition Matrix\n",
    "# transition_counts_2nd = (\n",
    "#     df_transitions_2nd.groupby(['Prev_Grid_Cell', 'Grid_Cell', 'Next_Grid_Cell'])\n",
    "#     .size()\n",
    "#     .unstack(fill_value=0)\n",
    "# )\n",
    "# transition_probabilities_2nd = transition_counts_2nd.div(transition_counts_2nd.sum(axis=1), axis=0)  # Normalize to probabilities\n",
    "\n",
    "# # Step 7: Define Prediction Function for Second-Order Markov Chain\n",
    "# def predict_next_grid_2nd(prev_grid, current_grid, transition_matrix):\n",
    "#     key = (prev_grid, current_grid)\n",
    "#     if key in transition_matrix.index:\n",
    "#         return transition_matrix.loc[key].idxmax()  # Most likely transition\n",
    "#     else:\n",
    "#         return None  # No transition data available\n",
    "\n",
    "# # Step 8: Apply Second-Order Prediction to Post-Depletion Data\n",
    "# df_post_depletion['Prev_Grid_Cell'] = df_post_depletion.groupby('deviceID')['Grid_Cell'].shift(1)\n",
    "# df_post_depletion['Predicted_Grid_Cell_2nd'] = df_post_depletion.apply(\n",
    "#     lambda row: predict_next_grid_2nd(row['Prev_Grid_Cell'], row['Grid_Cell'], transition_probabilities_2nd), axis=1\n",
    "# )\n",
    "\n",
    "# # Step 9: Measure Prediction Accuracy for Second-Order Markov Chain\n",
    "# df_post_depletion['Correct_Prediction_2nd'] = df_post_depletion['Predicted_Grid_Cell_2nd'] == df_post_depletion['Grid_Cell']\n",
    "# accuracy_2nd_order = df_post_depletion['Correct_Prediction_2nd'].mean()\n",
    "\n",
    "# print(f\"Second-Order Markov Chain Prediction Accuracy: {accuracy_2nd_order:.2%}\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# **Step 1: Convert GPS Data to Geospatial Format**\n",
    "df_gdf = gpd.GeoDataFrame(df, geometry=gpd.points_from_xy(df['Log'], df['Lat']), crs=\"EPSG:4326\")\n",
    "\n",
    "# **Step 2: Assign Vehicles to Grid Cells Using Spatial Index**\n",
    "grid_sindex = grid_gdf.sindex  \n",
    "\n",
    "def find_grid_cell(point):\n",
    "    \"\"\"Find the grid cell containing the given point.\"\"\"\n",
    "    possible_matches = list(grid_sindex.intersection(point.bounds))\n",
    "    for match in possible_matches:\n",
    "        if grid_gdf.iloc[match].geometry.contains(point):\n",
    "            return grid_gdf.index[match]  \n",
    "    return None  \n",
    "\n",
    "df_gdf['Grid_Cell'] = df_gdf['geometry'].map(find_grid_cell)\n",
    "df_gdf.dropna(subset=['Grid_Cell'], inplace=True)\n",
    "\n",
    "# **Step 3: Merge Safe SOC Thresholds**\n",
    "df_gdf = df_gdf.merge(safe_soc_thresholds_df, on=['deviceID', 'Timestamp'], how='left')\n",
    "\n",
    "# **Step 4: Sort and Identify Sensor Depletion**\n",
    "df_gdf.sort_values(by=['deviceID', 'Timestamp'], inplace=True)\n",
    "df_gdf['Depleted'] = df_gdf['SOC_batt'] < df_gdf['Safe_SOC_Threshold']\n",
    "\n",
    "# **Step 5: Separate Pre- and Post-Depletion Data**\n",
    "df_pre_depletion = df_gdf[~df_gdf['Depleted']].copy()\n",
    "df_post_depletion = df_gdf[df_gdf['Depleted']].copy()\n",
    "\n",
    "# **Step 6: Compute Transition Matrices**\n",
    "max_transitions_per_device = df_pre_depletion.groupby('deviceID')['Grid_Cell'].count().min()\n",
    "MAX_ORDER = max(1, min(max_transitions_per_device - 1, 3))  \n",
    "print(f\"ðŸ”¹ Automatically Set MAX_ORDER = {MAX_ORDER}\")\n",
    "\n",
    "def find_next_different(series):\n",
    "    \"\"\"Finds the next different grid cell for each row within a deviceID group.\"\"\"\n",
    "    next_values = series.to_numpy()  # Convert to NumPy array for efficiency\n",
    "    result = np.full_like(next_values, fill_value=np.nan, dtype=np.float64)  # Initialize result\n",
    "\n",
    "    # Loop through each value in the series\n",
    "    for i in range(len(next_values) - 1):  \n",
    "        for j in range(i + 1, len(next_values)):  # Look ahead to find the next different value\n",
    "            if next_values[j] != next_values[i]:  \n",
    "                result[i] = next_values[j]  # Assign the first different value found\n",
    "                break  \n",
    "\n",
    "    return pd.Series(result, index=series.index)  # Ensure index remains unchanged\n",
    "\n",
    "# âœ… Apply it safely without breaking the DataFrame index\n",
    "df_pre_depletion['Next_Grid_Cell'] = df_pre_depletion.groupby('deviceID')['Grid_Cell'].transform(find_next_different)\n",
    "\n",
    "def find_previous_different(series):\n",
    "    \"\"\"Finds the last different grid cell for each row within a deviceID group.\"\"\"\n",
    "    prev_values = series.to_numpy()  # Convert to NumPy array for efficiency\n",
    "    result = np.full_like(prev_values, fill_value=np.nan, dtype=np.float64)  # Initialize result\n",
    "\n",
    "    for i in range(1, len(prev_values)):  \n",
    "        for j in range(i - 1, -1, -1):  # Look backward to find the last different value\n",
    "            if prev_values[j] != prev_values[i]:  \n",
    "                result[i] = prev_values[j]  # Assign the first different value found\n",
    "                break  \n",
    "\n",
    "    return pd.Series(result, index=series.index)\n",
    "\n",
    "# âœ… Apply for the first order\n",
    "df_pre_depletion['Prev_Grid_Cell_1'] = df_pre_depletion.groupby('deviceID')['Grid_Cell'].transform(find_previous_different)\n",
    "\n",
    "# âœ… Build higher orders iteratively, stopping when history runs out\n",
    "for order in range(2, MAX_ORDER + 1):\n",
    "    prev_col = f'Prev_Grid_Cell_{order - 1}'  # Previous order column\n",
    "    new_col = f'Prev_Grid_Cell_{order}'\n",
    "\n",
    "    # Only compute next order for rows where the previous order is NOT NaN\n",
    "    df_pre_depletion[new_col] = df_pre_depletion.groupby('deviceID')[prev_col].transform(\n",
    "        lambda x: find_previous_different(x) if x.notna().any() else np.nan\n",
    "    )\n",
    "\n",
    "# **Step 7: Compute Transition Matrices for Each Order with Laplace Smoothing**\n",
    "alpha = 0.01  \n",
    "transition_probabilities = {}\n",
    "\n",
    "# âœ… Ensure First-Order Matches Standalone Model\n",
    "transition_probabilities[1] = transition_df.copy()  # Directly use standalone first-order model\n",
    "\n",
    "# âœ… Compute Higher-Order Matrices\n",
    "for order in range(2, MAX_ORDER + 1):\n",
    "    prev_cols = [f'Prev_Grid_Cell_{i}' for i in range(order, 0, -1)]\n",
    "    required_cols = prev_cols + ['Grid_Cell', 'Next_Grid_Cell']\n",
    "    df_transitions = df_pre_depletion.dropna(subset=required_cols)\n",
    "\n",
    "    if not df_transitions.empty:\n",
    "        transition_counts = df_transitions.groupby(required_cols).size().unstack(fill_value=0)\n",
    "        row_sums = transition_counts.sum(axis=1)\n",
    "        transition_probabilities[order] = (transition_counts).div(row_sums, axis=0).fillna(0)\n",
    "\n",
    "# **Step 8: Define Hybrid N-th Order Prediction Function**\n",
    "def predict_next_grid_n_order(prev_grids, current_grid, transition_probabilities):\n",
    "    \"\"\"Predict the next grid cell using the highest available Markov order, gradually falling back.\"\"\"\n",
    "    print(f\"ðŸ” Predicting for {current_grid} with history: {prev_grids}\")\n",
    "\n",
    "    # Try the highest available order first\n",
    "    for order in range(len(prev_grids), 0, -1):  \n",
    "        key = (current_grid,) if order == 1 else tuple(prev_grids[-order:]) + (current_grid,)\n",
    "\n",
    "        if order in transition_probabilities and key in transition_probabilities[order].index:\n",
    "            print(f\"âœ… Used Order {order} for {current_grid} (Key: {key})\")\n",
    "            return transition_probabilities[order].loc[key].idxmax()\n",
    "\n",
    "    # Fall back to first-order correctly\n",
    "    if 1 in transition_probabilities and current_grid in transition_probabilities[1].index:\n",
    "        print(f\"âš ï¸ Falling back to Order 1 for {current_grid} (Key: ({current_grid},))\")\n",
    "        return transition_probabilities[1].loc[current_grid].idxmax()\n",
    "\n",
    "    # If everything fails, use most frequent transition\n",
    "    if 1 in transition_probabilities and not transition_probabilities[1].empty:\n",
    "        most_common_transition = transition_probabilities[1].sum(axis=1).idxmax()\n",
    "        print(f\"ðŸ”¹ Choosing most frequent transition for {current_grid}: {most_common_transition}\")\n",
    "        return most_common_transition\n",
    "\n",
    "    print(f\"ðŸš¨ No prediction found for {current_grid}. Returning Unknown.\")\n",
    "    return \"Unknown\"\n",
    "\n",
    "# **Step 9: Apply Hybrid N-th Order Prediction to Post-Depletion Data**\n",
    "prev_grid_columns = [f'Prev_Grid_Cell_{order}' for order in range(1, MAX_ORDER + 1)]\n",
    "# **Step 9: Apply Hybrid N-th Order Prediction to Post-Depletion Data**\n",
    "prev_grid_columns = [f'Prev_Grid_Cell_{order}' for order in range(1, MAX_ORDER + 1)]\n",
    "\n",
    "# âœ… Apply first order correctly\n",
    "df_post_depletion['Prev_Grid_Cell_1'] = df_post_depletion.groupby('deviceID')['Grid_Cell'].transform(find_previous_different)\n",
    "\n",
    "# âœ… Apply higher orders while ensuring correct stopping\n",
    "for order in range(2, MAX_ORDER + 1):\n",
    "    prev_col = f'Prev_Grid_Cell_{order - 1}'  # Previous order column\n",
    "    new_col = f'Prev_Grid_Cell_{order}'\n",
    "\n",
    "    # Only compute for rows where the previous order is NOT NaN\n",
    "    df_post_depletion[new_col] = df_post_depletion.groupby('deviceID')[prev_col].transform(\n",
    "        lambda x: find_previous_different(x) if x.notna().any() else np.nan\n",
    "    )\n",
    "\n",
    "def safe_predict(row):\n",
    "    prev_grids = [row[col] for col in prev_grid_columns if pd.notna(row[col])]\n",
    "\n",
    "    if not prev_grids:  \n",
    "        return np.nan\n",
    "\n",
    "    predicted = predict_next_grid_n_order(prev_grids, row['Grid_Cell'], transition_probabilities)\n",
    "    return predicted if isinstance(predicted, (int, str, float)) else \"Unknown\"\n",
    "\n",
    "# âœ… Debugging Function\n",
    "def debug_prediction(prev_grids, current_grid, transition_probabilities):\n",
    "    for order in range(len(prev_grids), 0, -1):\n",
    "        key = (current_grid,) if order == 1 else tuple(prev_grids[-order:]) + (current_grid,)\n",
    "\n",
    "        if order in transition_probabilities and key in transition_probabilities[order].index:\n",
    "            print(f\"âœ… Order {order} used for {current_grid} (Key: {key})\")\n",
    "            return transition_probabilities[order].loc[key].idxmax()\n",
    "\n",
    "    print(f\"âš ï¸ No prediction found for {current_grid} with {prev_grids}. Falling back...\")\n",
    "    return None\n",
    "\n",
    "# **Apply Prediction**\n",
    "df_post_depletion['Predicted_Grid_Cell_Hybrid'] = df_post_depletion.apply(\n",
    "    lambda row: safe_predict(row), axis=1\n",
    ")\n",
    "df_post_depletion['Predicted_Grid_Cell_Hybrid'].fillna(\"Unknown\", inplace=True)\n",
    "\n",
    "# **Step 10: Measure Prediction Accuracy**\n",
    "df_post_depletion['Correct_Prediction_Hybrid'] = df_post_depletion['Predicted_Grid_Cell_Hybrid'].eq(df_post_depletion['Grid_Cell'])\n",
    "accuracy_hybrid = df_post_depletion['Correct_Prediction_Hybrid'].mean()\n",
    "\n",
    "print(f\"Hybrid N-th Order Markov Chain Prediction Accuracy: {accuracy_hybrid:.2%}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save DataFrame to Excel file\n",
    "df_post_depletion.to_excel(\"/workspace/data/df_post_depletion.xlsx\", index=False)\n",
    "df_pre_depletion.to_excel(\"/workspace/data/df_pre_depletion.xlsx\", index=False)\n",
    "\n",
    "# Return file path for download\n",
    "file_path\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check how many transitions exist at each order\n",
    "for order in transition_probabilities:\n",
    "    print(f\"ðŸ“Š Order {order}: {len(transition_probabilities[order])} transitions recorded\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Hybrid N-th Order Markov Chain Prediction Accuracy: {accuracy_hybrid:.2%}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract entropy values\n",
    "orders = list(transition_probabilities.keys())\n",
    "entropy_values = []\n",
    "\n",
    "for order in orders:\n",
    "    probs = transition_probabilities[order]\n",
    "    entropy = -np.nansum(probs * np.log2(probs))\n",
    "    entropy_values.append(entropy)\n",
    "    print(f\"ðŸ”¹ Entropy (Order {order}): {entropy:.4f}\")\n",
    "\n",
    "# Convert orders and entropy values to numpy arrays\n",
    "orders_array = np.array(orders)\n",
    "entropy_array = np.array(entropy_values)\n",
    "\n",
    "# Normalize the values for better numerical stability\n",
    "orders_norm = (orders_array - orders_array.min()) / (orders_array.max() - orders_array.min())\n",
    "entropy_norm = (entropy_array - entropy_array.min()) / (entropy_array.max() - entropy_array.min())\n",
    "\n",
    "# Define the line from the first to the last point\n",
    "line_vec = np.array([orders_norm[-1] - orders_norm[0], entropy_norm[-1] - entropy_norm[0]])\n",
    "line_vec_norm = line_vec / np.linalg.norm(line_vec)\n",
    "\n",
    "# Compute distances of each point from the line\n",
    "distances = []\n",
    "for i in range(len(orders_norm)):\n",
    "    point_vec = np.array([orders_norm[i] - orders_norm[0], entropy_norm[i] - entropy_norm[0]])\n",
    "    proj = np.dot(point_vec, line_vec_norm) * line_vec_norm\n",
    "    dist_vec = point_vec - proj\n",
    "    distances.append(np.linalg.norm(dist_vec))\n",
    "\n",
    "# Find the elbow point as the max distance from the line\n",
    "elbow_index = np.argmax(distances)\n",
    "elbow_order = orders[elbow_index]\n",
    "\n",
    "# Plot entropy vs. Markov order with elbow point\n",
    "plt.figure(figsize=(8, 5))\n",
    "plt.plot(orders, entropy_values, marker='o', linestyle='-', color='b', label=\"Entropy\")\n",
    "plt.axvline(x=elbow_order, color='r', linestyle='--', label=f'Elbow at N={elbow_order}')\n",
    "plt.xlabel(\"Markov Chain Order (N)\")\n",
    "plt.ylabel(\"Entropy\")\n",
    "plt.title(\"Entropy vs. Markov Order with Corrected Elbow Point\")\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()\n",
    "\n",
    "# Return the detected elbow order\n",
    "print(f\"ðŸ”¹ Optimal order is {elbow_order}\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import scipy.stats as stats\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import mutual_info_score\n",
    "\n",
    "# **Step 1: Compute Mutual Information (MI)**\n",
    "def compute_mutual_information(df, prev_grid_col, next_grid_col='Next_Grid_Cell'):\n",
    "    valid_df = df.dropna(subset=[prev_grid_col, next_grid_col])\n",
    "    return mutual_info_score(valid_df[prev_grid_col], valid_df[next_grid_col])\n",
    "\n",
    "# **Step 2: Compute Conditional Entropy (Fixed)**\n",
    "def compute_conditional_entropy(df, prev_grid_col, next_grid_col='Next_Grid_Cell'):\n",
    "    contingency_table = pd.crosstab(df[prev_grid_col], df[next_grid_col])\n",
    "    probs = contingency_table.div(contingency_table.sum(axis=1), axis=0)\n",
    "\n",
    "    # ðŸ”¹ Fix: Replace zero values with NaN to avoid log(0) issues\n",
    "    probs = probs.replace(0, np.nan)\n",
    "    \n",
    "    entropy = -np.nansum(probs * np.log2(probs))  # Ignore NaN values in computation\n",
    "    return entropy\n",
    "\n",
    "# **Step 3: Chi-Square Test for Independence**\n",
    "def chi_square_test(df, prev_grid_col, next_grid_col='Next_Grid_Cell'):\n",
    "    contingency_table = pd.crosstab(df[prev_grid_col], df[next_grid_col])\n",
    "    chi2_stat, p_value, _, _ = stats.chi2_contingency(contingency_table)\n",
    "    return chi2_stat, p_value\n",
    "\n",
    "# Assuming df_pre_depletion is already defined in the environment\n",
    "orders = list(range(1, 31))\n",
    "\n",
    "# Initialize dictionaries to store values\n",
    "mi_scores = {}\n",
    "conditional_entropy_scores = {}\n",
    "chi_square_results = {}\n",
    "\n",
    "for order in orders:\n",
    "    prev_col = f'Prev_Grid_Cell_{order}'\n",
    "\n",
    "    mi_scores[order] = compute_mutual_information(df_pre_depletion, prev_col)\n",
    "    conditional_entropy_scores[order] = compute_conditional_entropy(df_pre_depletion, prev_col)\n",
    "    chi2_stat, p_value = chi_square_test(df_pre_depletion, prev_col)\n",
    "    chi_square_results[order] = (chi2_stat, p_value)\n",
    "\n",
    "# Convert results into lists for plotting\n",
    "mi_values = list(mi_scores.values())\n",
    "conditional_entropy_values = list(conditional_entropy_scores.values())\n",
    "chi_square_values = [chi_square_results[o][0] for o in orders]\n",
    "\n",
    "# **Step 5: Plot the Results with Dual Y-Axis**\n",
    "fig, ax1 = plt.subplots(figsize=(10, 5))\n",
    "\n",
    "# Plot Mutual Information on secondary Y-axis\n",
    "ax2 = ax1.twinx()\n",
    "ax2.plot(orders, mi_values, marker='o', linestyle='-', color='b', label=\"Mutual Information\")\n",
    "ax2.set_ylabel(\"Mutual Information\", color='b')\n",
    "ax2.tick_params(axis='y', labelcolor='b')\n",
    "\n",
    "# Plot Conditional Entropy and Chi-Square Statistic on primary Y-axis\n",
    "ax1.plot(orders, chi_square_values, marker='^', linestyle='-', color='r', label=\"Chi-Square Statistic\")\n",
    "\n",
    "ax1.set_xlabel(\"Markov Chain Order (N)\")\n",
    "ax1.set_ylabel(\"Score (Entropy & Chi-Square)\", color='r')\n",
    "ax1.tick_params(axis='y', labelcolor='r')\n",
    "\n",
    "# Tertiary Y-Axis (Conditional Entropy)\n",
    "ax3 = ax1.twinx()\n",
    "ax3.spines['right'].set_position(('outward', 60))  # Move third axis outward\n",
    "ax3.plot(orders, conditional_entropy_values, marker='s', linestyle='-', color='g', label=\"Conditional Entropy\")\n",
    "ax3.set_ylabel(\"Conditional Entropy\", color='g')\n",
    "ax3.tick_params(axis='y', labelcolor='g')\n",
    "\n",
    "# Title and Legend\n",
    "fig.suptitle(\"MI, Conditional Entropy, and Chi-Square vs. Markov Order\")\n",
    "fig.legend(loc=\"upper right\", bbox_to_anchor=(1,1), bbox_transform=ax1.transAxes)\n",
    "\n",
    "plt.grid(True)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ## HYBRID 1ST & 2ND ORDER MC\n",
    "\n",
    "# # **Step 1: Convert GPS Data to Geospatial Format**\n",
    "# df_gdf = gpd.GeoDataFrame(df, geometry=gpd.points_from_xy(df['Log'], df['Lat']), crs=\"EPSG:4326\")\n",
    "\n",
    "# # **Step 2: Assign Vehicles to Grid Cells Using Spatial Index**\n",
    "# grid_sindex = grid_gdf.sindex  # Create spatial index for fast lookups\n",
    "\n",
    "# def find_grid_cell(point):\n",
    "#     \"\"\"Find the grid cell containing the given point.\"\"\"\n",
    "#     possible_matches = list(grid_sindex.intersection(point.bounds))\n",
    "#     for match in possible_matches:\n",
    "#         if grid_gdf.iloc[match].geometry.contains(point):\n",
    "#             return grid_gdf.index[match]  # Return grid cell index\n",
    "#     return None  # No match found\n",
    "\n",
    "# # **ðŸš€ Optimized Grid Cell Assignment**\n",
    "# df_gdf['Grid_Cell'] = df_gdf['geometry'].map(find_grid_cell)  # Vectorized `.map()` approach\n",
    "\n",
    "# # Drop unmatched points\n",
    "# df_gdf.dropna(subset=['Grid_Cell'], inplace=True)\n",
    "\n",
    "# # **Step 3: Merge Computed Safe SOC Thresholds**\n",
    "# df_gdf = df_gdf.merge(safe_soc_thresholds_df, on=['deviceID', 'Timestamp'], how='left')\n",
    "\n",
    "# # **Step 4: Sort by deviceID and Timestamp**\n",
    "# df_gdf.sort_values(by=['deviceID', 'Timestamp'], inplace=True)\n",
    "\n",
    "\n",
    "# # **Step 5: Identify Sensor Depletion Using Dynamic Safe SOC**\n",
    "# df_gdf['Depleted'] = df_gdf['SOC_batt'] < df_gdf['Safe_SOC_Threshold']  # âœ… Replaced static threshold\n",
    "\n",
    "# # **Step 6: Separate Pre- and Post-Depletion Data**\n",
    "# df_pre_depletion = df_gdf[~df_gdf['Depleted']].copy()\n",
    "# df_post_depletion = df_gdf[df_gdf['Depleted']].copy()\n",
    "\n",
    "# # **Step 7: Create First-Order & Second-Order Transitions**\n",
    "# df_pre_depletion['Next_Grid_Cell'] = df_pre_depletion.groupby('deviceID')['Grid_Cell'].shift(-1)\n",
    "# df_pre_depletion['Prev_Grid_Cell'] = df_pre_depletion.groupby('deviceID')['Grid_Cell'].shift(1)\n",
    "\n",
    "# df_transitions_1st = df_pre_depletion.dropna(subset=['Next_Grid_Cell'])\n",
    "# df_transitions_2nd = df_pre_depletion.dropna(subset=['Prev_Grid_Cell', 'Next_Grid_Cell'])\n",
    "\n",
    "# # **Step 8: Compute Transition Matrices with Laplace Smoothing**\n",
    "# alpha = 0.01  # Smoothing Factor\n",
    "\n",
    "# # **1st Order Markov Chain**\n",
    "# transition_counts_1st = (\n",
    "#     df_transitions_1st.groupby(['Grid_Cell', 'Next_Grid_Cell'])\n",
    "#     .size()\n",
    "#     .unstack(fill_value=0)\n",
    "# )\n",
    "# transition_probabilities_1st = (transition_counts_1st + alpha).div(transition_counts_1st.sum(axis=1) + alpha * transition_counts_1st.shape[1], axis=0)\n",
    "\n",
    "# # **2nd Order Markov Chain**\n",
    "# transition_counts_2nd = (\n",
    "#     df_transitions_2nd.groupby(['Prev_Grid_Cell', 'Grid_Cell', 'Next_Grid_Cell'])\n",
    "#     .size()\n",
    "#     .unstack(fill_value=0)\n",
    "# )\n",
    "\n",
    "# # Normalize with Laplace Smoothing\n",
    "# row_sums = transition_counts_2nd.sum(axis=1)\n",
    "# transition_probabilities_2nd = (transition_counts_2nd + alpha).div(row_sums + alpha * transition_counts_2nd.shape[1], axis=0)\n",
    "\n",
    "# # **Step 9: Define Hybrid Prediction Function**\n",
    "# def predict_next_grid_hybrid(prev_grid, current_grid, transition_matrix_1st, transition_matrix_2nd):\n",
    "#     key = (prev_grid, current_grid)\n",
    "    \n",
    "#     # Use Second-Order if available\n",
    "#     if key in transition_matrix_2nd.index:\n",
    "#         return transition_matrix_2nd.loc[key].idxmax()  # Most likely transition\n",
    "    \n",
    "#     # If Second-Order not available, use First-Order\n",
    "#     elif current_grid in transition_matrix_1st.index:\n",
    "#         return transition_matrix_1st.loc[current_grid].idxmax()\n",
    "    \n",
    "#     # ðŸš¨ If neither is available, return a random choice from observed states\n",
    "#     elif len(transition_matrix_1st) > 0:\n",
    "#         return np.random.choice(transition_matrix_1st.index)\n",
    "    \n",
    "#     return None  # If neither available\n",
    "\n",
    "# # **Step 10: Apply Hybrid Prediction to Post-Depletion Data**\n",
    "# df_post_depletion['Prev_Grid_Cell'] = df_post_depletion.groupby('deviceID')['Grid_Cell'].shift(1)\n",
    "# df_post_depletion['Predicted_Grid_Cell_Hybrid'] = [\n",
    "#     predict_next_grid_hybrid(prev, curr, transition_probabilities_1st, transition_probabilities_2nd)\n",
    "#     for prev, curr in zip(df_post_depletion['Prev_Grid_Cell'], df_post_depletion['Grid_Cell'])\n",
    "# ]\n",
    "\n",
    "# # **Step 11: Measure Prediction Accuracy for Hybrid Model**\n",
    "# df_post_depletion['Correct_Prediction_Hybrid'] = df_post_depletion['Predicted_Grid_Cell_Hybrid'] == df_post_depletion['Grid_Cell']\n",
    "# accuracy_hybrid = df_post_depletion['Correct_Prediction_Hybrid'].mean()\n",
    "\n",
    "# print(f\"Hybrid Markov Chain Prediction Accuracy: {accuracy_hybrid:.2%}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import scipy.stats as stats\n",
    "# from sklearn.metrics import mutual_info_score\n",
    "\n",
    "# # **Step 1: Compute Mutual Information (MI)**\n",
    "# def compute_mutual_information(df):\n",
    "#     \"\"\" Computes Mutual Information between Previous and Next Grid Cells \"\"\"\n",
    "#     valid_df = df.dropna(subset=['Prev_Grid_Cell', 'Next_Grid_Cell'])\n",
    "#     return mutual_info_score(valid_df['Prev_Grid_Cell'], valid_df['Next_Grid_Cell'])\n",
    "\n",
    "# mi_score = compute_mutual_information(df_transitions_2nd)\n",
    "# print(f\"ðŸ”¹ Mutual Information (MI) Score: {mi_score:.4f}\")\n",
    "\n",
    "# # **Step 2: Compute Conditional Entropy (Fixed)**\n",
    "# def compute_conditional_entropy(df):\n",
    "#     \"\"\" Computes Conditional Entropy H(Y|X) for Next Grid given Previous Grid \"\"\"\n",
    "#     contingency_table = pd.crosstab(df['Prev_Grid_Cell'], df['Next_Grid_Cell'])\n",
    "#     probs = contingency_table.div(contingency_table.sum(axis=1), axis=0)\n",
    "    \n",
    "#     # **ðŸ”¹ Fix: Replace zero values with a small number to avoid log(0) issues**\n",
    "#     probs = probs.replace(0, np.nan)  # Convert zeros to NaN to avoid log(0)\n",
    "    \n",
    "#     entropy = -np.nansum(probs * np.log2(probs))  # Ignore NaN values in computation\n",
    "#     return entropy\n",
    "\n",
    "# conditional_entropy = compute_conditional_entropy(df_transitions_2nd)\n",
    "# print(f\"ðŸ”¹ Conditional Entropy (H|Prev_Grid_Cell): {conditional_entropy:.4f}\")\n",
    "\n",
    "# # **Step 3: Chi-Square Test for Independence**\n",
    "# def chi_square_test(df):\n",
    "#     \"\"\" Performs Chi-Square Test for Independence between Previous and Next Grid Cells \"\"\"\n",
    "#     contingency_table = pd.crosstab(df['Prev_Grid_Cell'], df['Next_Grid_Cell'])\n",
    "#     chi2_stat, p_value, _, _ = stats.chi2_contingency(contingency_table)\n",
    "#     return chi2_stat, p_value\n",
    "\n",
    "# chi2_stat, p_value = chi_square_test(df_transitions_2nd)\n",
    "# print(f\"ðŸ”¹ Chi-Square Test: Ï‡Â² = {chi2_stat:.4f}, p-value = {p_value:.4f}\")\n",
    "\n",
    "# # **Interpret Results**\n",
    "# if p_value < 0.05:\n",
    "#     print(\"âœ… Significant correlation detected between Prev_Grid_Cell and Next_Grid_Cell (p < 0.05)\")\n",
    "# else:\n",
    "#     print(\"âŒ No significant correlation detected (p >= 0.05)\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # TIME-BASED MC\n",
    "\n",
    "# # Step 1: Assign Vehicles to Grid Cells Using Polygon Containment\n",
    "# df_gdf = gpd.GeoDataFrame(df, geometry=gpd.points_from_xy(df['Log'], df['Lat']), crs=\"EPSG:4326\")\n",
    "\n",
    "# # Function to find which grid cell a point belongs to\n",
    "# def find_grid_cell(point, grid_gdf):\n",
    "#     match = grid_gdf.contains(point.geometry)\n",
    "#     if match.any():\n",
    "#         return match.idxmax()  # Return index of matching grid cell\n",
    "#     else:\n",
    "#         return None  # No match found\n",
    "\n",
    "# # Apply function to assign each GPS point to a grid cell\n",
    "# df_gdf['Grid_Cell'] = df_gdf.apply(lambda row: find_grid_cell(row, grid_gdf), axis=1)\n",
    "\n",
    "# # Drop rows where no grid cell was matched (outliers)\n",
    "# df_gdf = df_gdf.dropna(subset=['Grid_Cell'])\n",
    "\n",
    "# # Step 2: Sort by deviceID and Timestamp for Transition Analysis\n",
    "# df_gdf = df_gdf.sort_values(by=['deviceID', 'Timestamp'])\n",
    "\n",
    "# # Step 3: Identify Sensor Depletion (SOC_batt < 30)\n",
    "# depletion_threshold = 30\n",
    "# df_gdf['Depleted'] = df_gdf['SOC_batt'] < depletion_threshold\n",
    "\n",
    "# # Step 4: Extract Time-of-Day Bins\n",
    "# df_gdf['Hour'] = df_gdf['Timestamp'].dt.hour\n",
    "# df_gdf['Time_Bin'] = pd.cut(\n",
    "#     df_gdf['Hour'],\n",
    "#     bins=[0, 6, 12, 18, 24],  # Defining the four time periods\n",
    "#     labels=['Night', 'Morning', 'Afternoon', 'Evening'],\n",
    "#     right=False,\n",
    "#     include_lowest=True\n",
    "# )\n",
    "\n",
    "# # Step 5: Separate Pre- and Post-Depletion Data\n",
    "# df_pre_depletion = df_gdf[~df_gdf['Depleted']].copy()\n",
    "# df_post_depletion = df_gdf[df_gdf['Depleted']].copy()\n",
    "\n",
    "# # Step 6: Create First-Order Transitions Within Each Time Bin\n",
    "# df_pre_depletion['Next_Grid_Cell'] = df_pre_depletion.groupby(['deviceID', 'Time_Bin'])['Grid_Cell'].shift(-1)\n",
    "\n",
    "# # Drop last row per vehicle per time bin (no next transition available)\n",
    "# df_transitions_time = df_pre_depletion.dropna(subset=['Next_Grid_Cell'])\n",
    "\n",
    "# # Step 7: Build Time-Based Transition Matrices\n",
    "# transition_matrices_time = {}\n",
    "\n",
    "# for time_bin in df_transitions_time['Time_Bin'].unique():\n",
    "#     df_time_bin = df_transitions_time[df_transitions_time['Time_Bin'] == time_bin]\n",
    "    \n",
    "#     transition_counts_time = (\n",
    "#         df_time_bin.groupby(['Grid_Cell', 'Next_Grid_Cell'])\n",
    "#         .size()\n",
    "#         .unstack(fill_value=0)\n",
    "#     )\n",
    "    \n",
    "#     transition_matrices_time[time_bin] = transition_counts_time.div(transition_counts_time.sum(axis=1), axis=0)\n",
    "\n",
    "# # Step 8: Define Time-Based Prediction Function\n",
    "# def predict_next_grid_time_based(current_grid, time_bin, transition_matrices):\n",
    "#     if time_bin in transition_matrices and current_grid in transition_matrices[time_bin].index:\n",
    "#         return transition_matrices[time_bin].loc[current_grid].idxmax()  # Most likely transition\n",
    "#     else:\n",
    "#         return None  # No transition data available\n",
    "\n",
    "# # Step 9: Apply Time-Based Prediction to Post-Depletion Data\n",
    "# df_post_depletion['Predicted_Grid_Cell_Time_Based'] = df_post_depletion.apply(\n",
    "#     lambda row: predict_next_grid_time_based(row['Grid_Cell'], row['Time_Bin'], transition_matrices_time), axis=1\n",
    "# )\n",
    "\n",
    "# # Step 10: Measure Prediction Accuracy for Time-Based Markov Chain\n",
    "# df_post_depletion['Correct_Prediction_Time_Based'] = df_post_depletion['Predicted_Grid_Cell_Time_Based'] == df_post_depletion['Grid_Cell']\n",
    "# accuracy_time_based = df_post_depletion['Correct_Prediction_Time_Based'].mean()\n",
    "\n",
    "# print(f\"Time-Based Markov Chain Prediction Accuracy: {accuracy_time_based:.2%}\")\n",
    "\n",
    "# # # Step 11: Save the Time-Based Transition Matrices and Post-Depletion Validation\n",
    "# # for time_bin, matrix in transition_matrices_time.items():\n",
    "# #     output_path = f\"/Users/mayar/Desktop/MIT/Research_Fellow/ENERGY_SENSING/DATA/Transition_Matrix_{time_bin}.xlsx\"\n",
    "# #     matrix.to_excel(output_path)\n",
    "\n",
    "# # output_path_post_depletion_time = \"/Users/mayar/Desktop/MIT/Research_Fellow/ENERGY_SENSING/DATA/Post_Depletion_Validation_Time_Based.xlsx\"\n",
    "# # df_post_depletion.to_excel(output_path_post_depletion_time, index=False)\n",
    "\n",
    "# # # Step 12: Analyze and Print the Top 10 Most Likely Transitions for Time-Based Markov Chain\n",
    "# # most_likely_transitions_time_based = {}\n",
    "\n",
    "# # for time_bin, matrix in transition_matrices_time.items():\n",
    "# #     most_likely_transitions_time_based[time_bin] = (\n",
    "# #         matrix.stack()\n",
    "# #         .reset_index()\n",
    "# #         .rename(columns={0: 'Probability', 'level_0': 'Current_Grid', 'level_1': 'To_Grid'})\n",
    "# #         .sort_values(by='Probability', ascending=False)\n",
    "# #     )\n",
    "    \n",
    "# #     print(f\"Top 10 Most Likely Transitions ({time_bin}):\")\n",
    "# #     print(most_likely_transitions_time_based[time_bin].head(10))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import pandas as pd\n",
    "# import numpy as np\n",
    "# import geopandas as gpd\n",
    "# import random\n",
    "# from hyperopt import fmin, tpe, hp, STATUS_OK, Trials\n",
    "\n",
    "# # Step 1: Assign Vehicles to Grid Cells Using Polygon Containment\n",
    "# df_gdf = gpd.GeoDataFrame(df, geometry=gpd.points_from_xy(df['Log'], df['Lat']), crs=\"EPSG:4326\")\n",
    "\n",
    "# # Function to find which grid cell a point belongs to\n",
    "# def find_grid_cell(point, grid_gdf):\n",
    "#     match = grid_gdf.contains(point.geometry)\n",
    "#     if match.any():\n",
    "#         return match.idxmax()\n",
    "#     else:\n",
    "#         return None  # No match found\n",
    "\n",
    "# # Apply function to assign each GPS point to a grid cell\n",
    "# df_gdf['Grid_Cell'] = df_gdf.apply(lambda row: find_grid_cell(row, grid_gdf), axis=1)\n",
    "\n",
    "# # Drop rows where no grid cell was matched (outliers)\n",
    "# df_gdf = df_gdf.dropna(subset=['Grid_Cell'])\n",
    "\n",
    "# # Step 2: Sort by deviceID and Timestamp for Transition Analysis\n",
    "# df_gdf = df_gdf.sort_values(by=['deviceID', 'Timestamp'])\n",
    "\n",
    "# # Step 3: Identify Sensor Depletion (SOC_batt < 30)\n",
    "# depletion_threshold = 30\n",
    "# df_gdf['Depleted'] = df_gdf['SOC_batt'] < depletion_threshold\n",
    "\n",
    "# # Step 4: Separate Pre- and Post-Depletion Data\n",
    "# df_pre_depletion = df_gdf[~df_gdf['Depleted']].copy()\n",
    "# df_post_depletion = df_gdf[df_gdf['Depleted']].copy()\n",
    "\n",
    "# # Step 5: Extract Transitions (State, Action, Next State)\n",
    "# df_pre_depletion['Next_Grid_Cell'] = df_pre_depletion.groupby('deviceID')['Grid_Cell'].shift(-1)\n",
    "# df_transitions = df_pre_depletion.dropna(subset=['Next_Grid_Cell'])\n",
    "\n",
    "# # Step 6: Define RL Training Function for Hyperparameter Optimization\n",
    "# def train_rl(params):\n",
    "#     alpha = params['alpha']\n",
    "#     gamma = params['gamma']\n",
    "#     epsilon = params['epsilon']\n",
    "#     epsilon_decay = params['epsilon_decay']\n",
    "#     training_iterations = int(params['training_iterations'])\n",
    "\n",
    "#     # Initialize Q-Table\n",
    "#     q_table = {}\n",
    "\n",
    "#     # RL Training with Dynamic Exploration and Replay\n",
    "#     for _ in range(training_iterations):\n",
    "#         for _, row in df_transitions.iterrows():\n",
    "#             state = row['Grid_Cell']\n",
    "#             action = row['Next_Grid_Cell']\n",
    "\n",
    "#             if state not in q_table:\n",
    "#                 q_table[state] = {}\n",
    "\n",
    "#             if action not in q_table[state]:\n",
    "#                 q_table[state][action] = 0\n",
    "\n",
    "#             # Log-based reward normalization\n",
    "#             transition_count = df_transitions[(df_transitions['Grid_Cell'] == state) & (df_transitions['Next_Grid_Cell'] == action)].shape[0]\n",
    "#             reward = np.log(transition_count + 1)\n",
    "\n",
    "#             # Q-learning update\n",
    "#             max_future_q = max(q_table[state].values()) if q_table[state] else 0\n",
    "#             q_table[state][action] = (1 - alpha) * q_table[state][action] + alpha * (reward + gamma * max_future_q)\n",
    "\n",
    "#     # RL Prediction Function with Decaying Exploration\n",
    "#     def predict_next_grid_rl(state, q_table, epsilon):\n",
    "#         if state in q_table:\n",
    "#             if random.uniform(0, 1) < epsilon:\n",
    "#                 return random.choice(list(q_table[state].keys()))  # Explore\n",
    "#             else:\n",
    "#                 return max(q_table[state], key=q_table[state].get)  # Exploit\n",
    "#         return None\n",
    "\n",
    "#     # Apply RL-Based Prediction to Post-Depletion Data\n",
    "#     df_post_depletion['Predicted_Grid_Cell_RL'] = df_post_depletion['Grid_Cell'].apply(lambda x: predict_next_grid_rl(x, q_table, epsilon))\n",
    "\n",
    "#     # Reduce Epsilon Over Time\n",
    "#     epsilon = max(0.05, epsilon * epsilon_decay)\n",
    "\n",
    "#     # Calculate Accuracy\n",
    "#     df_post_depletion['Correct_Prediction_RL'] = df_post_depletion['Predicted_Grid_Cell_RL'] == df_post_depletion['Grid_Cell']\n",
    "#     accuracy = df_post_depletion['Correct_Prediction_RL'].mean()\n",
    "\n",
    "#     print(f\"Trial Accuracy: {accuracy:.2%} | Params: {params}\")\n",
    "\n",
    "#     # Stop if accuracy reaches 85%\n",
    "#     if accuracy >= 0.85:\n",
    "#         print(\"Target accuracy reached! Stopping optimization.\")\n",
    "#         return {'loss': -accuracy, 'status': STATUS_OK, 'params': params}\n",
    "\n",
    "#     return {'loss': -accuracy, 'status': STATUS_OK}\n",
    "\n",
    "# # Step 7: Define Hyperparameter Search Space\n",
    "# param_space = {\n",
    "#     'alpha': hp.uniform('alpha', 0.1, 0.5),  # Learning rate (0.1 - 0.5)\n",
    "#     'gamma': hp.uniform('gamma', 0.5, 0.9),  # Discount factor (0.5 - 0.9)\n",
    "#     'epsilon': hp.uniform('epsilon', 0.1, 0.3),  # Exploration rate (10%-30%)\n",
    "#     'epsilon_decay': hp.uniform('epsilon_decay', 0.98, 0.999),  # Slower decay (98%-99.9%)\n",
    "#     'training_iterations': hp.quniform('training_iterations', 2, 10, 1)  # Replay transitions (2-10 times)\n",
    "# }\n",
    "\n",
    "# # Step 8: Run Hyperparameter Optimization\n",
    "# trials = Trials()\n",
    "# best_params = fmin(fn=train_rl, space=param_space, algo=tpe.suggest, max_evals=50, trials=trials)\n",
    "\n",
    "\n",
    "# print(f\"Best RL Parameters: {best_params}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import pandas as pd\n",
    "# import numpy as np\n",
    "# import cudf\n",
    "# import cupy as cp\n",
    "# from cuml.preprocessing import LabelEncoder\n",
    "# from numba import cuda\n",
    "# import random\n",
    "# from hyperopt import fmin, tpe, hp, STATUS_OK, Trials\n",
    "\n",
    "# # ==========================\n",
    "# # ðŸ”¹ Load and Convert Data to cuDF\n",
    "# # ==========================\n",
    "# dfcopied = df.copy()\n",
    "# for col in dfcopied.columns:\n",
    "#     if dfcopied[col].dtype == 'object' and col not in ['deviceID', 'Date']:  \n",
    "#         dfcopied[col] = pd.to_numeric(dfcopied[col], errors='coerce')\n",
    "\n",
    "# dfcopied['deviceID'] = dfcopied['deviceID'].astype(str)  \n",
    "# dfcopied['Date'] = dfcopied['Date'].astype(str)  \n",
    "\n",
    "# df_gdf = cudf.DataFrame(dfcopied)\n",
    "\n",
    "# df_gdf['deviceID'] = df_gdf['deviceID'].astype('str')\n",
    "# df_gdf['Date'] = df_gdf['Date'].astype('str')\n",
    "\n",
    "# # ==========================\n",
    "# # ðŸ”¹ Assign Grid Cells Using GPU\n",
    "# # ==========================\n",
    "# df_gdf['Grid_Cell'] = df_gdf['Log'].astype(str) + \"_\" + df_gdf['Lat'].astype(str)\n",
    "\n",
    "# le_grid = LabelEncoder()\n",
    "# df_gdf['Grid_Cell_ID'] = le_grid.fit_transform(df_gdf['Grid_Cell'])\n",
    "\n",
    "# df_gdf = df_gdf.dropna(subset=['Grid_Cell'])\n",
    "\n",
    "# df_gdf = df_gdf.sort_values(by=['deviceID', 'Timestamp'])\n",
    "\n",
    "# # ==========================\n",
    "# # ðŸ”¹ Identify Sensor Depletion\n",
    "# # ==========================\n",
    "# depletion_threshold = 20\n",
    "# df_gdf['Depleted'] = df_gdf['SOC_batt'] < depletion_threshold\n",
    "\n",
    "# df_pre_depletion = df_gdf[~df_gdf['Depleted']].copy()\n",
    "# df_post_depletion = df_gdf[df_gdf['Depleted']].copy()\n",
    "\n",
    "# df_pre_depletion['Next_Grid_Cell'] = df_pre_depletion.groupby('deviceID')['Grid_Cell'].shift(-1)\n",
    "# df_transitions = df_pre_depletion.dropna(subset=['Next_Grid_Cell'])\n",
    "\n",
    "# all_grid_cells = cudf.concat([df_transitions['Grid_Cell'], df_transitions['Next_Grid_Cell']]).drop_duplicates()\n",
    "\n",
    "# le = LabelEncoder()\n",
    "# le.fit(all_grid_cells.to_pandas())  \n",
    "\n",
    "# df_transitions['State_ID'] = le.transform(df_transitions['Grid_Cell'].to_pandas())\n",
    "# df_transitions['Next_State_ID'] = le.transform(df_transitions['Next_Grid_Cell'].to_pandas())\n",
    "\n",
    "# state_actions = cp.array(df_transitions[['State_ID', 'Next_State_ID']].to_numpy(), dtype=cp.int32)\n",
    "\n",
    "# # ==========================\n",
    "# # ðŸ”¹ Initialize Sparse Q-Table and Rewards\n",
    "# # ==========================\n",
    "# num_states = int(df_transitions['State_ID'].max() + 1)\n",
    "\n",
    "# # Convert Q-table to a sparse format\n",
    "# q_table = cp.zeros((num_states, num_states), dtype=cp.float32)\n",
    "\n",
    "# # Use a dense array for rewards (avoid modifying sparse structure)\n",
    "# rewards = cp.zeros((num_states, num_states), dtype=cp.float32)\n",
    "\n",
    "# for state, action in state_actions:\n",
    "#     rewards[state, action] += 1\n",
    "\n",
    "# rewards = cp.log1p(rewards)  \n",
    "\n",
    "# # ==========================\n",
    "# # ðŸ”¹ GPU-Accelerated Q-Learning (Fixed Epsilon Decay)\n",
    "# # ==========================\n",
    "# @cuda.jit\n",
    "# def train_q_table(q_table, state_actions, rewards, alpha, gamma, max_q_values, epsilon):\n",
    "#     idx = cuda.grid(1)\n",
    "#     if idx >= state_actions.shape[0]:\n",
    "#         return\n",
    "\n",
    "#     state = state_actions[idx, 0]\n",
    "#     action = state_actions[idx, 1]\n",
    "\n",
    "#     reward = rewards[state, action] if state < rewards.shape[0] and action < rewards.shape[1] else 0\n",
    "#     max_future_q = max_q_values[action] if action < max_q_values.shape[0] else 0\n",
    "\n",
    "#     # Apply Q-learning update\n",
    "#     q_table[state, action] = (1 - alpha) * q_table[state, action] + alpha * (reward + gamma * max_future_q)\n",
    "\n",
    "# def train_rl(params):\n",
    "#     alpha = params['alpha']\n",
    "#     gamma = params['gamma']\n",
    "#     epsilon = params['epsilon']\n",
    "#     epsilon_decay = params['epsilon_decay']  \n",
    "#     training_iterations = int(params['training_iterations'])\n",
    "\n",
    "#     num_states = int(df_transitions['State_ID'].max() + 1)\n",
    "    \n",
    "#     # Convert Q-table to CuPy dense matrix for fast updates\n",
    "#     q_table = cp.zeros((num_states, num_states), dtype=cp.float32)\n",
    "\n",
    "#     threadsperblock = 256\n",
    "#     max_blocks = 65535  \n",
    "#     blockspergrid = min((state_actions.shape[0] + (threadsperblock - 1)) // threadsperblock, max_blocks)\n",
    "\n",
    "#     # Precompute max Q-values every N iterations instead of every iteration\n",
    "#     update_freq = 5  # Update every 5 iterations for efficiency\n",
    "\n",
    "#     for i in range(training_iterations):\n",
    "#         if i % update_freq == 0:\n",
    "#             max_q_values = cp.max(q_table, axis=1)\n",
    "\n",
    "\n",
    "#         train_q_table[blockspergrid, threadsperblock](q_table, state_actions, rewards, alpha, gamma, max_q_values, epsilon)\n",
    "\n",
    "#     # ==========================\n",
    "#     # ðŸ”¹ RL Prediction (Using Decayed Epsilon)\n",
    "#     # ==========================\n",
    "#     state_to_id = cudf.Series(le.classes_).to_pandas().reset_index().set_index(0)['index'].to_dict()\n",
    "#     id_to_state = {v: k for k, v in state_to_id.items()}\n",
    "\n",
    "#     def predict_next_grid(state):\n",
    "#         if state in state_to_id:\n",
    "#             state_id = state_to_id[state]\n",
    "#             if random.uniform(0, 1) < epsilon:\n",
    "#                 return id_to_state[random.randint(0, num_states - 1)]  \n",
    "#             return id_to_state[int(cp.argmax(q_table[state_id]))]  \n",
    "#         return None\n",
    "\n",
    "#     df_post_depletion['Predicted_Grid_Cell_RL'] = df_post_depletion['Grid_Cell'].to_pandas().map(predict_next_grid)\n",
    "#     df_post_depletion['Correct_Prediction_RL'] = df_post_depletion['Predicted_Grid_Cell_RL'] == df_post_depletion['Grid_Cell']\n",
    "#     accuracy = df_post_depletion['Correct_Prediction_RL'].mean()\n",
    "#     # Apply epsilon decay \n",
    "#     epsilon = max(0.05, epsilon * epsilon_decay)\n",
    "\n",
    "#     print(f\"Trial Accuracy: {accuracy:.2%} | Params: {params}\")\n",
    "\n",
    "#     if accuracy >= 0.85:\n",
    "#         print(\"Target accuracy reached! Stopping optimization.\")\n",
    "#         return {'loss': -accuracy, 'status': STATUS_OK, 'params': params}\n",
    "\n",
    "#     return {'loss': -accuracy, 'status': STATUS_OK}\n",
    "\n",
    "# # ==========================\n",
    "# # ðŸ”¹ Hyperparameter Optimization\n",
    "# # ==========================\n",
    "# param_space = {\n",
    "#     'alpha': hp.uniform('alpha', 0.1, 0.5),\n",
    "#     'gamma': hp.uniform('gamma', 0.5, 0.9),\n",
    "#     'epsilon': hp.uniform('epsilon', 0.1, 0.3),\n",
    "#     'epsilon_decay': hp.uniform('epsilon_decay', 0.98, 0.999),\n",
    "#     'training_iterations': hp.quniform('training_iterations', 2, 10, 1)\n",
    "# }\n",
    "\n",
    "# trials = Trials()\n",
    "# best_params = fmin(fn=train_rl, space=param_space, algo=tpe.suggest, max_evals=50, trials=trials)\n",
    "\n",
    "# print(f\"Best RL Parameters: {best_params}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import pandas as pd\n",
    "# import numpy as np\n",
    "# import cudf\n",
    "# import cupy as cp\n",
    "# import cuspatial\n",
    "# import random\n",
    "# from hyperopt import fmin, tpe, hp, STATUS_OK, Trials\n",
    "# import shapely.geometry\n",
    "# import geopandas as gpd\n",
    "\n",
    "# # ==========================\n",
    "# # ðŸ”¹ Load and Convert Data to cuDF\n",
    "# # ==========================\n",
    "# dfcopied = df.copy()\n",
    "\n",
    "# # Convert numeric columns\n",
    "# for col in dfcopied.columns:\n",
    "#     if dfcopied[col].dtype == 'object' and col not in ['deviceID', 'Date']:  \n",
    "#         dfcopied[col] = pd.to_numeric(dfcopied[col], errors='coerce')\n",
    "\n",
    "# dfcopied['deviceID'] = dfcopied['deviceID'].astype(str)  \n",
    "# dfcopied['Date'] = dfcopied['Date'].astype(str)  \n",
    "\n",
    "# # Convert to cuDF for GPU processing\n",
    "# df_gdf = cudf.DataFrame(dfcopied)\n",
    "\n",
    "# # Ensure deviceID and Date remain strings\n",
    "# df_gdf['deviceID'] = df_gdf['deviceID'].astype('str')\n",
    "# df_gdf['Date'] = df_gdf['Date'].astype('str')\n",
    "\n",
    "# # ==========================\n",
    "# # ðŸ”¹ Define Grid Binning Using cuSpatial\n",
    "# # ==========================\n",
    "# grid_size = 120  # Grid cell size in meters\n",
    "\n",
    "# # Define spatial bounding box\n",
    "# min_x, max_x = df_gdf['Log'].min(), df_gdf['Log'].max()\n",
    "# min_y, max_y = df_gdf['Lat'].min(), df_gdf['Lat'].max()\n",
    "\n",
    "# EARTH_RADIUS = 6371000  # Earth radius in meters\n",
    "\n",
    "# # Convert lat/lon degrees to meters using Haversine formula approximation\n",
    "# df_gdf['Grid_X'] = ((df_gdf['Log'] - min_x) * (np.pi/180) * EARTH_RADIUS * np.cos(np.radians(df_gdf['Lat']))).astype(int) // grid_size\n",
    "# df_gdf['Grid_Y'] = ((df_gdf['Lat'] - min_y) * (np.pi/180) * EARTH_RADIUS).astype(int) // grid_size\n",
    "\n",
    "# df_gdf['Grid_Cell'] = df_gdf['Grid_X'].astype(str) + \"_\" + df_gdf['Grid_Y'].astype(str)\n",
    "\n",
    "\n",
    "# # Sort by deviceID and Timestamp for Transition Analysis\n",
    "# df_gdf = df_gdf.sort_values(by=['deviceID', 'Timestamp'])\n",
    "\n",
    "# # ==========================\n",
    "# # ðŸ”¹ Identify Sensor Depletion\n",
    "# # ==========================\n",
    "# depletion_threshold = 20\n",
    "# df_gdf['Depleted'] = df_gdf['SOC_batt'] < depletion_threshold\n",
    "\n",
    "# df_pre_depletion = df_gdf[~df_gdf['Depleted']].copy()\n",
    "# df_post_depletion = df_gdf[df_gdf['Depleted']].copy()\n",
    "\n",
    "# df_pre_depletion['Next_Grid_Cell'] = df_pre_depletion.groupby('deviceID')['Grid_Cell'].shift(-1)\n",
    "# df_transitions = cudf.concat([df_pre_depletion, df_post_depletion]).dropna(subset=['Next_Grid_Cell'])\n",
    "\n",
    "# # ðŸ›  Debugging Step: Check if df_transitions has data\n",
    "# print(f\"Total transitions available for training: {len(df_transitions)}\")\n",
    "# if df_transitions.empty:\n",
    "#     print(\"âš ï¸ ERROR: No transitions available for training. Check if 'Next_Grid_Cell' is correctly assigned.\")\n",
    "#     exit()\n",
    "\n",
    "# # ==========================\n",
    "# # ðŸ”¹ Initialize Q-Table (Python Dictionary)\n",
    "# # ==========================\n",
    "# q_table = {}\n",
    "\n",
    "# # ==========================\n",
    "# # ðŸ”¹ GPU-Accelerated Q-Learning (Batch Updates)\n",
    "# # ==========================\n",
    "# def train_rl(params):\n",
    "#     alpha = params['alpha']\n",
    "#     gamma = params['gamma']\n",
    "#     epsilon = params['epsilon']\n",
    "#     epsilon_decay = params['epsilon_decay']\n",
    "#     training_iterations = int(params['training_iterations'])\n",
    "\n",
    "#     # Ensure `q_table` is empty for each run\n",
    "#     q_table.clear()\n",
    "\n",
    "#     # RL Training with Vectorized Operations\n",
    "#     for _ in range(training_iterations):\n",
    "#         batch_size = 1000  # Use mini-batch updates\n",
    "\n",
    "#         for batch_start in range(0, len(df_transitions), batch_size):\n",
    "#             batch = df_transitions.iloc[batch_start:batch_start + batch_size].to_pandas()  # Convert batch to Pandas\n",
    "\n",
    "#             states = batch['Grid_Cell'].values\n",
    "#             actions = batch['Next_Grid_Cell'].values\n",
    "\n",
    "#             # Compute transition counts efficiently\n",
    "#             unique_pairs, counts = np.unique(list(zip(states, actions)), axis=0, return_counts=True)\n",
    "\n",
    "#             for (state, action), count in zip(unique_pairs, counts):\n",
    "#                 if state not in q_table:\n",
    "#                     q_table[state] = {}\n",
    "\n",
    "#                 # Ensure each state has at least one valid action with a default Q-value\n",
    "#                 if action not in q_table[state]:\n",
    "#                     q_table[state][action] = np.random.uniform(0.01, 0.1)  # Small random Q-value\n",
    "\n",
    "#                 # Compute reward and update Q-value\n",
    "#                 reward = np.log(count + 1) + 0.01\n",
    "#                 max_future_q = max(q_table[state].values(), default=0)\n",
    "#                 q_table[state][action] = (1 - alpha) * q_table[state][action] + alpha * (reward + gamma * max_future_q)\n",
    "\n",
    "\n",
    "#         # Decay `epsilon` after every iteration\n",
    "#         epsilon = max(0.05, epsilon * epsilon_decay)\n",
    "\n",
    "#     # ðŸ›  Debugging Step: Check if Q-table was updated\n",
    "#     print(f\"Total states in Q-table: {len(q_table)}\")\n",
    "#     if len(q_table) == 0:\n",
    "#         print(\"âš ï¸ ERROR: Q-table is empty after training. Check if state-action pairs are updating.\")\n",
    "#         exit()\n",
    "\n",
    "#     # ==========================\n",
    "#     # ðŸ”¹ RL Prediction (Using Decayed Epsilon)\n",
    "#     # ==========================\n",
    "#     def predict_next_grid(state):\n",
    "#         if state in q_table and q_table[state]:  # Ensure Q-values exist\n",
    "#             if random.uniform(0, 1) < epsilon:\n",
    "#                 return random.choice(list(q_table[state].keys()))  # Explore\n",
    "#             return max(q_table[state], key=q_table[state].get)  # Exploit\n",
    "#         # If state is missing, return a random known state\n",
    "#         return random.choice(list(q_table.keys())) if q_table else None\n",
    "\n",
    "#     df_post_depletion['Predicted_Grid_Cell_RL'] = df_post_depletion['Grid_Cell'].to_pandas().map(predict_next_grid)\n",
    "#     df_post_depletion['Correct_Prediction_RL'] = df_post_depletion['Predicted_Grid_Cell_RL'] == df_post_depletion['Grid_Cell']\n",
    "#     accuracy = df_post_depletion['Correct_Prediction_RL'].mean()\n",
    "\n",
    "#     # ðŸ›  Debugging Step: Check if predictions are being made\n",
    "#     print(f\"Total predictions made: {df_post_depletion['Predicted_Grid_Cell_RL'].notna().sum()}\")\n",
    "#     if df_post_depletion['Predicted_Grid_Cell_RL'].isna().all():\n",
    "#         print(\"âš ï¸ ERROR: No valid predictions were made. Check 'predict_next_grid()'.\")\n",
    "#         exit()\n",
    "\n",
    "#     print(f\"Trial Accuracy: {accuracy:.2%} | Params: {params}\")\n",
    "\n",
    "#     if accuracy >= 0.85:\n",
    "#         print(\"Target accuracy reached! Stopping optimization.\")\n",
    "#         return {'loss': -accuracy, 'status': STATUS_OK, 'params': params}\n",
    "\n",
    "#     return {'loss': -accuracy, 'status': STATUS_OK}\n",
    "\n",
    "# # ==========================\n",
    "# # ðŸ”¹ Hyperparameter Optimization\n",
    "# # ==========================\n",
    "# param_space = {\n",
    "#     'alpha': hp.uniform('alpha', 0.1, 0.5),\n",
    "#     'gamma': hp.uniform('gamma', 0.5, 0.9),\n",
    "#     'epsilon': hp.uniform('epsilon', 0.1, 0.3),\n",
    "#     'epsilon_decay': hp.uniform('epsilon_decay', 0.98, 0.999),\n",
    "#     'training_iterations': hp.quniform('training_iterations', 2, 10, 1)\n",
    "# }\n",
    "\n",
    "# trials = Trials()\n",
    "# best_params = fmin(fn=train_rl, space=param_space, algo=tpe.suggest, max_evals=50, trials=trials)\n",
    "\n",
    "# print(f\"Best RL Parameters: {best_params}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **GPU-Accelerated Q-Learning for Grid-Based Mobility Prediction**\n",
    "\n",
    "This implementation leverages **GPU-accelerated computing** for **spatial binning, sensor depletion analysis, and reinforcement learning-based mobility prediction**. The core objective is to **train an agent to predict the next grid cell occupied by a vehicle after battery depletion**, based on historical transitions.\n",
    "\n",
    "## **1. Data Loading and Conversion to cuDF for GPU Acceleration**\n",
    "To facilitate **large-scale spatiotemporal data processing**, the dataset is converted into a **cuDF DataFrame**, enabling computations on NVIDIA GPUs via **RAPIDS cuDF**. \n",
    "\n",
    "Given an original DataFrame **$df$** containing numerical and categorical attributes, each numerical column is converted to **floating-point representation** while categorical variables ($deviceID$ and $Date$) remain as strings. The transformation ensures efficient memory alignment for GPU operations.\n",
    "\n",
    "## **2. Grid Binning Using cuSpatial**\n",
    "A **spatial discretization strategy** is applied to **map GPS coordinates into a structured 120m Ã— 120m grid**. The Earth's curvature necessitates an **adaptive longitude resolution**, computed dynamically based on latitude.\n",
    "\n",
    "### **Latitude and Longitude Transformation**\n",
    "For a given latitude $ \\text{Lat}_i $ and longitude $ \\text{Log}_i $, the coordinates are mapped into grid indices as follows:\n",
    "\n",
    "$$\n",
    "\\text{Grid}_X = \\left\\lfloor \\frac{(\\text{Log}_i - \\text{Log}_{\\min}) \\cdot \\pi / 180 \\cdot R_E \\cdot \\cos(\\text{Lat}_i)}{\\text{grid size}} \\right\\rfloor\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\text{Grid}_Y = \\left\\lfloor \\frac{(\\text{Lat}_i - \\text{Lat}_{\\min}) \\cdot \\pi / 180 \\cdot R_E}{\\text{grid size}} \\right\\rfloor\n",
    "$$\n",
    "\n",
    "where:\n",
    "- $ R_E = 6371000 $ m is Earth's radius,\n",
    "- The **longitude transformation** incorporates the **cosine of latitude** to account for Earth's curvature,\n",
    "- The floor function ensures that values are discretized into **integer grid indices**.\n",
    "\n",
    "Each spatial point is **hashed into a grid cell identifier**:\n",
    "\n",
    "$$\n",
    "\\text{Grid\\_Cell}_i = (\\text{Grid}_X, \\text{Grid}_Y)\n",
    "$$\n",
    "\n",
    "## **3. Sensor Depletion Detection**\n",
    "A **binary depletion flag** is assigned to each vehicle record:\n",
    "\n",
    "$$\n",
    "D_i =\n",
    "\\begin{cases} \n",
    "1, & \\text{if } SOC_{\\text{batt}, i} < 20\\% \\\\\n",
    "0, & \\text{otherwise}\n",
    "\\end{cases}\n",
    "$$\n",
    "\n",
    "where **$ SOC_{\\text{batt}} $** is the state of charge. The dataset is split into:\n",
    "- **Pre-depletion** data **$P$**, containing normal mobility patterns.\n",
    "- **Post-depletion** data **$Q$**, capturing movement after battery exhaustion.\n",
    "\n",
    "The next grid cell for each pre-depletion record is assigned using a **time-ordered shift operation**:\n",
    "\n",
    "$$\n",
    "G_{i+1} = \\text{shift}(G_i, -1)\n",
    "$$\n",
    "\n",
    "ensuring that transitions are correctly captured.\n",
    "\n",
    "## **4. Reinforcement Learning (Q-Learning) for Mobility Prediction**\n",
    "\n",
    "The goal of this reinforcement learning (RL) framework is to train an agent that learns **optimal movement patterns** based on past trajectories and predicts the most probable **next grid cell** a vehicle will occupy after battery depletion. The agent is trained using **Q-learning**, a model-free reinforcement learning algorithm that iteratively updates **Q-values** representing the expected reward for selecting an action (i.e., moving to a new grid cell) from a given state.\n",
    "\n",
    "### **State and Action Representation**\n",
    "- The **state space** consists of all possible **grid cells** $ s \\in S $, where each grid cell is a **120m Ã— 120m spatial unit** indexed as $(X, Y)$.  \n",
    "- The **action space** consists of transitions to **neighboring grid cells**, corresponding to potential movements between states.  \n",
    "\n",
    "Each transition is extracted from **pre-depletion data**, where each record consists of:\n",
    "1. **Current grid cell** $ G_i $\n",
    "2. **Next observed grid cell** $ G_{i+1} $\n",
    "3. **State of charge (SOC)** and depletion flag\n",
    "\n",
    "For each **state-action pair** $(s, a)$, we maintain a **Q-value** $ Q(s, a) $, which represents the estimated cumulative reward expected when selecting action $ a $ from state $ s $.\n",
    "\n",
    "### **Q-Value Update Rule**\n",
    "The agent updates its **Q-values** iteratively using the Bellman equation:\n",
    "\n",
    "$$\n",
    "Q(s, a) = (1 - \\alpha) Q(s, a) + \\alpha \\left( r + \\gamma \\max_{a'} Q(s', a') \\right)\n",
    "$$\n",
    "\n",
    "where:\n",
    "- **$ \\alpha $ (learning rate)** determines how much the newly acquired information overrides the existing Q-value.\n",
    "- **$ \\gamma $ (discount factor)** determines the importance of future rewards.\n",
    "- **$ r $ (reward function)** assigns a numerical value to each transition, encouraging movement patterns that match realistic trajectories.\n",
    "- **$ \\max_{a'} Q(s', a') $** represents the highest Q-value of possible actions in the next state $ s' $, guiding the agent toward high-reward decisions.\n",
    "\n",
    "### **Reward Function**\n",
    "To ensure realistic movement, the **reward function** incorporates both **empirical transition frequency** and **spatial coherence**:\n",
    "\n",
    "$$\n",
    "r = \\log (N(s, a) + 1) + 0.01 + 0.5 \\cdot e^{-\\frac{||s - a||}{\\text{grid size}}}\n",
    "$$\n",
    "\n",
    "where:\n",
    "- $ N(s, a) $ is the number of observed transitions from $ s $ to $ a $ in the dataset.\n",
    "- The **logarithmic term** prevents highly frequent transitions from dominating the learning process.\n",
    "- The **exponential decay term** penalizes large jumps, encouraging spatially coherent movement.\n",
    "\n",
    "### **Exploration vs. Exploitation Policy**\n",
    "The agent follows an **$ \\epsilon $-greedy policy**, balancing exploration (random movement selection) and exploitation (selecting the best known action):\n",
    "\n",
    "$$\n",
    "a =\n",
    "\\begin{cases} \n",
    "\\text{random action}, & \\text{with probability } \\epsilon \\\\\n",
    "\\arg\\max_{a} Q(s, a), & \\text{otherwise}\n",
    "\\end{cases}\n",
    "$$\n",
    "\n",
    "where **$ \\epsilon $** is the exploration probability, which **decays over time** to prioritize exploitation:\n",
    "\n",
    "$$\n",
    "\\epsilon = \\max(0.05, \\epsilon \\cdot \\text{decay factor})\n",
    "$$\n",
    "\n",
    "### **Batch Training Strategy**\n",
    "Instead of updating Q-values one transition at a time, **mini-batch updates** are applied using **vectorized operations**:\n",
    "- A batch size of **1000 transitions** is selected per iteration.\n",
    "- Transitions are processed in parallel using **GPU acceleration**.\n",
    "- Each batch computes transition counts and updates the **Q-table** accordingly.\n",
    "\n",
    "## **5. Hyperparameter Optimization Using Bayesian Search**\n",
    "To optimize **Q-learning performance**, we conduct **Bayesian optimization** over the following hyperparameters:\n",
    "- **$ \\alpha $ (learning rate) in $ [0.1, 0.5] $**: Controls how aggressively Q-values are updated.\n",
    "- **$ \\gamma $ (discount factor) in $ [0.5, 0.9] $**: Adjusts how much future rewards impact current decisions.\n",
    "- **$ \\epsilon $ (exploration probability) in $ [0.1, 0.3] $**: Governs the randomness of action selection.\n",
    "- **$ \\epsilon_{\\text{decay}} $ in $ [0.98, 0.999] $**: Ensures a smooth transition from exploration to exploitation.\n",
    "- **Number of training iterations in $ [2,10] $**: Determines how long the agent learns from past data.\n",
    "\n",
    "The optimization process **minimizes the loss function**:\n",
    "\n",
    "$$\n",
    "\\mathcal{L} = -\\text{Accuracy}\n",
    "$$\n",
    "\n",
    "where accuracy is computed as:\n",
    "\n",
    "$$\n",
    "\\text{Accuracy} = \\frac{\\sum \\mathbb{1} (G_{\\text{predicted}} = G_{\\text{actual}})}{N_{\\text{post-depletion}}}\n",
    "$$\n",
    "\n",
    "Bayesian search uses **tree-structured Parzen estimators (TPE)** to efficiently explore the hyperparameter space.\n",
    "\n",
    "## **6. RL-Based Mobility Prediction**\n",
    "Once training is complete, the **Q-table is used for inference** to predict the most likely **next grid cell** after depletion:\n",
    "\n",
    "$$\n",
    "G_{\\text{predicted}} = \\arg\\max_{a} Q(G_{\\text{current}}, a)\n",
    "$$\n",
    "\n",
    "A final evaluation step compares the predicted post-depletion locations with the actual recorded locations. The **model terminates training** if:\n",
    "\n",
    "$$\n",
    "\\text{Accuracy} \\geq 85\\%\n",
    "$$\n",
    "\n",
    "ensuring that the agent achieves a **sufficiently high prediction accuracy**.\n",
    "\n",
    "## **7. Conclusion**\n",
    "This approach leverages:\n",
    "- **Q-learning with batch training**, enabling efficient large-scale learning.\n",
    "- **GPU acceleration via RAPIDS cuDF**, significantly reducing training time.\n",
    "- **Bayesian hyperparameter tuning**, optimizing Q-learning efficiency.\n",
    "- **Adaptive exploration-exploitation balance**, refining the model over iterations.\n",
    "\n",
    "The final model provides **high-accuracy mobility predictions**, supporting **real-time sensor deployment planning and energy-aware urban mobility analysis**.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import pandas as pd\n",
    "# import numpy as np\n",
    "# import cudf\n",
    "# import cupy as cp\n",
    "# import cuspatial\n",
    "# import random\n",
    "# from hyperopt import fmin, tpe, hp, STATUS_OK, Trials\n",
    "# import shapely.geometry\n",
    "# import geopandas as gpd\n",
    "\n",
    "# # ==========================\n",
    "# # ðŸ”¹ Load and Convert Data to cuDF\n",
    "# # ==========================\n",
    "# dfcopied = df.copy()\n",
    "\n",
    "# # Convert numeric columns\n",
    "# for col in dfcopied.columns:\n",
    "#     if dfcopied[col].dtype == 'object' and col not in ['deviceID', 'Date']:  \n",
    "#         dfcopied[col] = pd.to_numeric(dfcopied[col], errors='coerce')\n",
    "\n",
    "# dfcopied['deviceID'] = dfcopied['deviceID'].astype(str)  \n",
    "# dfcopied['Date'] = dfcopied['Date'].astype(str)  \n",
    "\n",
    "# # Convert to cuDF for GPU processing\n",
    "# df_gdf = cudf.DataFrame(dfcopied)\n",
    "\n",
    "# # Ensure deviceID and Date remain strings\n",
    "# df_gdf['deviceID'] = df_gdf['deviceID'].astype('str')\n",
    "# df_gdf['Date'] = df_gdf['Date'].astype('str')\n",
    "\n",
    "# # ==========================\n",
    "# # ðŸ”¹ Define Grid Binning Using cuSpatial\n",
    "# # ==========================\n",
    "# # Define spatial bounding box\n",
    "# min_x, max_x = df_gdf['Log'].min(), df_gdf['Log'].max()\n",
    "# min_y, max_y = df_gdf['Lat'].min(), df_gdf['Lat'].max()\n",
    "\n",
    "# EARTH_RADIUS = 6371000  # Earth radius in meters\n",
    "\n",
    "# # Convert lat/lon degrees to meters using Haversine formula approximation\n",
    "# df_gdf['Grid_X'] = ((df_gdf['Log'] - min_x) * (np.pi/180) * EARTH_RADIUS * np.cos(np.radians(df_gdf['Lat']))).astype(int) // grid_size\n",
    "# df_gdf['Grid_Y'] = ((df_gdf['Lat'] - min_y) * (np.pi/180) * EARTH_RADIUS).astype(int) // grid_size\n",
    "\n",
    "# df_gdf['Grid_Cell'] = df_gdf['Grid_X'].astype(str) + \"_\" + df_gdf['Grid_Y'].astype(str)\n",
    "\n",
    "# # Sort by deviceID and Timestamp for Transition Analysis\n",
    "# df_gdf = df_gdf.sort_values(by=['deviceID', 'Timestamp'])\n",
    "\n",
    "# # ==========================\n",
    "# # ðŸ”¹ Identify Sensor Depletion\n",
    "# # ==========================\n",
    "# depletion_threshold = 20\n",
    "# df_gdf['Depleted'] = df_gdf['SOC_batt'] < depletion_threshold\n",
    "\n",
    "# df_pre_depletion = df_gdf[~df_gdf['Depleted']].copy()\n",
    "# df_post_depletion = df_gdf[df_gdf['Depleted']].copy()\n",
    "\n",
    "# df_pre_depletion['Next_Grid_Cell'] = df_pre_depletion.groupby('deviceID')['Grid_Cell'].shift(-1)\n",
    "# df_transitions = cudf.concat([df_pre_depletion, df_post_depletion]).dropna(subset=['Next_Grid_Cell'])\n",
    "\n",
    "# # ðŸ›  Debugging Step: Check if df_transitions has data\n",
    "# print(f\"Total transitions available for training: {len(df_transitions)}\")\n",
    "\n",
    "# # ==========================\n",
    "# # ðŸ”¹ 4ï¸âƒ£ Initialize Q-Table & Tracking Lists\n",
    "# # ==========================\n",
    "# q_table = {}\n",
    "# q_table_convergence = []\n",
    "# bellman_errors = []\n",
    "# policy_consistency = []\n",
    "# reward_per_episode = []\n",
    "# training_accuracies = []\n",
    "# validation_accuracies = []\n",
    "# hyperparam_values = []\n",
    "# accuracy_values = []\n",
    "\n",
    "# # ==========================\n",
    "# # ðŸ”¹ 5ï¸âƒ£ Helper Functions\n",
    "# # ==========================\n",
    "# def parse_grid_cell(grid_cell_str):\n",
    "#     try:\n",
    "#         x, y = map(int, grid_cell_str.split('_'))\n",
    "#         return np.array([x, y])\n",
    "#     except ValueError:\n",
    "#         return np.array([0, 0])\n",
    "\n",
    "# def track_q_table_convergence(prev_q_table, new_q_table):\n",
    "#     \"\"\"Compute change in Q-table values between iterations.\"\"\"\n",
    "#     delta_q = sum(abs(new_q_table.get(s, {}).get(a, 0) - prev_q_table.get(s, {}).get(a, 0))\n",
    "#                   for s in new_q_table for a in new_q_table[s])\n",
    "#     q_table_convergence.append(delta_q)\n",
    "\n",
    "# def compute_policy_consistency(q_table, df_transitions):\n",
    "#     \"\"\"Check how often the same best action is chosen for a state.\"\"\"\n",
    "#     correct_choices = 0\n",
    "#     total_choices = 0\n",
    "\n",
    "#     for _, row in df_transitions.to_pandas().iterrows():\n",
    "#         state, next_state = row['Grid_Cell'], row['Next_Grid_Cell']\n",
    "#         if state in q_table and next_state in q_table[state]:\n",
    "#             best_action = max(q_table[state], key=q_table[state].get)\n",
    "#             if best_action == next_state:\n",
    "#                 correct_choices += 1\n",
    "#             total_choices += 1\n",
    "\n",
    "#     policy_consistency.append(correct_choices / total_choices if total_choices > 0 else 0)\n",
    "\n",
    "# def compute_expected_reward(rewards_per_episode, gamma):\n",
    "#     \"\"\"Compute expected cumulative reward using discounted sum formula.\"\"\"\n",
    "#     total_reward = sum((gamma**t) * r for t, r in enumerate(rewards_per_episode))\n",
    "#     reward_per_episode.append(total_reward)\n",
    "\n",
    "# # ==========================\n",
    "# # ðŸ”¹ Initialize Tracking Variables\n",
    "# # ==========================\n",
    "# hyperparam_values = []\n",
    "# accuracy_values = []\n",
    "# gamma_values = []\n",
    "\n",
    "# # ==========================\n",
    "# # ðŸ”¹ GPU-Accelerated Q-Learning (Batch Updates)\n",
    "# # ==========================\n",
    "# def train_rl(params):\n",
    "\n",
    "#     global hyperparam_values, accuracy_values, gamma_values # Store hyperparameters and accuracy\n",
    "\n",
    "#     alpha = params['alpha']\n",
    "#     gamma = params['gamma']\n",
    "#     epsilon = params['epsilon']\n",
    "#     epsilon_decay = params['epsilon_decay']\n",
    "#     training_iterations = int(params['training_iterations'])\n",
    "\n",
    "#     gamma_values.append(gamma)  # Track gamma value\n",
    "#     hyperparam_values.append(params)  # Store hyperparameters\n",
    "\n",
    "#     # Ensure `q_table` is empty for each run\n",
    "#     q_table.clear()\n",
    "    \n",
    "#     # Ensure the previous Q-table is tracked properly\n",
    "#     prev_q_table = {}\n",
    "\n",
    "#     # RL Training with Vectorized Operations\n",
    "#     for _ in range(training_iterations):\n",
    "#         batch_size = 1000  # Use mini-batch updates\n",
    "\n",
    "#         for batch_start in range(0, len(df_transitions), batch_size):\n",
    "#             batch = df_transitions.iloc[batch_start:batch_start + batch_size].to_pandas()  # Convert batch to Pandas\n",
    "\n",
    "#             states = batch['Grid_Cell'].values\n",
    "#             actions = batch['Next_Grid_Cell'].values\n",
    "\n",
    "#             # Compute transition counts efficiently\n",
    "#             unique_pairs, counts = np.unique(list(zip(states, actions)), axis=0, return_counts=True)\n",
    "\n",
    "#             for (state, action), count in zip(unique_pairs, counts):\n",
    "#                 if state not in q_table:\n",
    "#                     q_table[state] = {}\n",
    "\n",
    "#                 # Ensure each state has at least one valid action with a default Q-value\n",
    "#                 if action not in q_table[state]:\n",
    "#                     q_table[state][action] = np.random.uniform(0.01, 0.1)  # Small random Q-value\n",
    "\n",
    "#                 # Convert Grid Cells to numerical coordinates\n",
    "#                 state_coords = parse_grid_cell(state)\n",
    "#                 action_coords = parse_grid_cell(action)\n",
    "\n",
    "#                 # Compute Euclidean distance penalty\n",
    "#                 distance_penalty = np.exp(-np.linalg.norm(state_coords - action_coords) / grid_size)\n",
    "\n",
    "#                 # Compute reward\n",
    "#                 reward = np.log(count + 1) + 0.01 + 0.5 * distance_penalty\n",
    "\n",
    "#                 # Q-learning update\n",
    "#                 max_future_q = max(q_table[state].values(), default=0)\n",
    "#                 q_table[state][action] = (1 - alpha) * q_table[state][action] + alpha * (reward + gamma * max_future_q)\n",
    "                \n",
    "#         # âœ… Track Q-Table Convergence **AFTER each training iteration**\n",
    "#         track_q_table_convergence(prev_q_table, q_table)\n",
    "\n",
    "#         # âœ… Compute Policy Consistency\n",
    "#         compute_policy_consistency(q_table, df_transitions)\n",
    "\n",
    "#         # âœ… Compute Expected Reward Growth\n",
    "#         compute_expected_reward([q_table[state][action] for state in q_table for action in q_table[state]], gamma)\n",
    "\n",
    "#         epsilon = max(0.05, epsilon * epsilon_decay)\n",
    "\n",
    "#     # ðŸ›  Debugging Step: Check if Q-table was updated\n",
    "#     print(f\"Total states in Q-table: {len(q_table)}\")\n",
    "\n",
    "#     # ==========================\n",
    "#     # ðŸ”¹ RL Prediction (Using Decayed Epsilon)\n",
    "#     # ==========================\n",
    "#     def predict_next_grid(state):\n",
    "#         if state in q_table and q_table[state]:  # Ensure Q-values exist\n",
    "#             if random.uniform(0, 1) < epsilon:\n",
    "#                 return random.choice(list(q_table[state].keys()))  # Explore\n",
    "#             return max(q_table[state], key=q_table[state].get)  # Exploit\n",
    "#         # If state is missing, return a random known state\n",
    "#         return random.choice(list(q_table.keys())) if q_table else None\n",
    "\n",
    "#     df_post_depletion['Predicted_Grid_Cell_RL'] = df_post_depletion['Grid_Cell'].to_pandas().map(predict_next_grid)\n",
    "#     df_post_depletion['Correct_Prediction_RL'] = df_post_depletion['Predicted_Grid_Cell_RL'] == df_post_depletion['Grid_Cell']\n",
    "#     accuracy = df_post_depletion['Correct_Prediction_RL'].mean()\n",
    "#     accuracy_values.append(accuracy) # store for later validation\n",
    "\n",
    "#     # ðŸ›  Debugging Step: Check if predictions are being made\n",
    "#     print(f\"Total predictions made: {df_post_depletion['Predicted_Grid_Cell_RL'].notna().sum()}\")\n",
    "#     print(f\"Trial Accuracy: {accuracy:.2%} | Params: {params}\")\n",
    "\n",
    "#     return {'loss': -accuracy, 'status': STATUS_OK}\n",
    "\n",
    "# # ==========================\n",
    "# # ðŸ”¹ Hyperparameter Optimization\n",
    "# # ==========================\n",
    "# param_space = {\n",
    "#     'alpha': hp.uniform('alpha', 0.1, 0.5),\n",
    "#     'gamma': hp.uniform('gamma', 0.5, 0.9),\n",
    "#     'epsilon': hp.uniform('epsilon', 0.1, 0.3),\n",
    "#     'epsilon_decay': hp.uniform('epsilon_decay', 0.98, 0.999),\n",
    "#     'training_iterations': hp.quniform('training_iterations', 2, 15, 1)\n",
    "# }\n",
    "\n",
    "# trials = Trials()\n",
    "# best_params = fmin(fn=train_rl, space=param_space, algo=tpe.suggest, max_evals=30, trials=trials)\n",
    "\n",
    "# print(f\"Best RL Parameters: {best_params}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The below is the final RL framework"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import cudf\n",
    "import cupy as cp\n",
    "import cuspatial\n",
    "import random\n",
    "from hyperopt import fmin, tpe, hp, STATUS_OK, Trials\n",
    "import shapely.geometry\n",
    "import geopandas as gpd\n",
    "\n",
    "# ==========================\n",
    "# ðŸ”¹ Load and Convert Data to cuDF\n",
    "# ==========================\n",
    "dfcopied = df.copy()\n",
    "\n",
    "# Convert numeric columns\n",
    "for col in dfcopied.columns:\n",
    "    if dfcopied[col].dtype == 'object' and col not in ['deviceID', 'Date']:  \n",
    "        dfcopied[col] = pd.to_numeric(dfcopied[col], errors='coerce')\n",
    "\n",
    "dfcopied['deviceID'] = dfcopied['deviceID'].astype(str)  \n",
    "dfcopied['Date'] = dfcopied['Date'].astype(str)  \n",
    "\n",
    "# Convert to cuDF for GPU processing\n",
    "df_gdf = cudf.DataFrame(dfcopied)\n",
    "\n",
    "# Ensure deviceID and Date remain strings\n",
    "df_gdf['deviceID'] = df_gdf['deviceID'].astype('str')\n",
    "df_gdf['Date'] = df_gdf['Date'].astype('str')\n",
    "\n",
    "# ==========================\n",
    "# ðŸ”¹ Define Grid Binning Using cuSpatial\n",
    "# ==========================\n",
    "# Define spatial bounding box\n",
    "min_x, max_x = df_gdf['Log'].min(), df_gdf['Log'].max()\n",
    "min_y, max_y = df_gdf['Lat'].min(), df_gdf['Lat'].max()\n",
    "\n",
    "EARTH_RADIUS = 6371000  # Earth radius in meters\n",
    "\n",
    "# Convert lat/lon degrees to meters using Haversine formula approximation\n",
    "df_gdf['Grid_X'] = ((df_gdf['Log'] - min_x) * (np.pi/180) * EARTH_RADIUS * np.cos(np.radians(df_gdf['Lat']))).astype(int) // grid_size\n",
    "df_gdf['Grid_Y'] = ((df_gdf['Lat'] - min_y) * (np.pi/180) * EARTH_RADIUS).astype(int) // grid_size\n",
    "\n",
    "df_gdf['Grid_Cell'] = df_gdf['Grid_X'].astype(str) + \"_\" + df_gdf['Grid_Y'].astype(str)\n",
    "\n",
    "# Sort by deviceID and Timestamp for Transition Analysis\n",
    "df_gdf = df_gdf.sort_values(by=['deviceID', 'Timestamp'])\n",
    "\n",
    "# ==========================\n",
    "# ðŸ”¹ Identify Sensor Depletion\n",
    "# ==========================\n",
    "depletion_threshold = 20\n",
    "df_gdf['Depleted'] = df_gdf['SOC_batt'] < depletion_threshold\n",
    "\n",
    "df_pre_depletion = df_gdf[~df_gdf['Depleted']].copy()\n",
    "df_post_depletion = df_gdf[df_gdf['Depleted']].copy()\n",
    "\n",
    "df_pre_depletion['Next_Grid_Cell'] = df_pre_depletion.groupby('deviceID')['Grid_Cell'].shift(-1)\n",
    "df_transitions = cudf.concat([df_pre_depletion, df_post_depletion]).dropna(subset=['Next_Grid_Cell'])\n",
    "\n",
    "# ðŸ›  Debugging Step: Check if df_transitions has data\n",
    "print(f\"Total transitions available for training: {len(df_transitions)}\")\n",
    "\n",
    "# ==========================\n",
    "# ðŸ”¹ 4ï¸âƒ£ Initialize Q-Table & Tracking Lists\n",
    "# ==========================\n",
    "q_table = {}\n",
    "q_table_convergence = []\n",
    "bellman_errors = []\n",
    "policy_consistency = []\n",
    "reward_per_episode = []\n",
    "training_accuracies = []\n",
    "validation_accuracies = []\n",
    "hyperparam_values = []\n",
    "accuracy_values = []\n",
    "\n",
    "# ==========================\n",
    "# ðŸ”¹ 5ï¸âƒ£ Helper Functions\n",
    "# ==========================\n",
    "def parse_grid_cell(grid_cell_str):\n",
    "    try:\n",
    "        x, y = map(int, grid_cell_str.split('_'))\n",
    "        return np.array([x, y])\n",
    "    except ValueError:\n",
    "        return np.array([0, 0])\n",
    "\n",
    "def track_q_table_convergence(prev_q_table, new_q_table):\n",
    "    \"\"\"Compute change in Q-table values between iterations.\"\"\"\n",
    "    delta_q = sum(abs(new_q_table.get(s, {}).get(a, 0) - prev_q_table.get(s, {}).get(a, 0))\n",
    "                  for s in new_q_table for a in new_q_table[s])\n",
    "    q_table_convergence.append(delta_q)\n",
    "\n",
    "def compute_policy_consistency(q_table, df_transitions):\n",
    "    \"\"\"Check how often the same best action is chosen for a state.\"\"\"\n",
    "    correct_choices = 0\n",
    "    total_choices = 0\n",
    "\n",
    "    for _, row in df_transitions.to_pandas().iterrows():\n",
    "        state, next_state = row['Grid_Cell'], row['Next_Grid_Cell']\n",
    "        if state in q_table and next_state in q_table[state]:\n",
    "            best_action = max(q_table[state], key=q_table[state].get)\n",
    "            if best_action == next_state:\n",
    "                correct_choices += 1\n",
    "            total_choices += 1\n",
    "\n",
    "    policy_consistency.append(correct_choices / total_choices if total_choices > 0 else 0)\n",
    "\n",
    "def compute_expected_reward(rewards_per_episode, gamma):\n",
    "    \"\"\"Compute expected cumulative reward using discounted sum formula.\"\"\"\n",
    "    total_reward = sum((gamma**t) * r for t, r in enumerate(rewards_per_episode))\n",
    "    reward_per_episode.append(total_reward)\n",
    "\n",
    "# ==========================\n",
    "# ðŸ”¹ Initialize Tracking Variables\n",
    "# ==========================\n",
    "hyperparam_values = []\n",
    "accuracy_values = []\n",
    "gamma_values = []\n",
    "\n",
    "# Store best Q-table across trials\n",
    "best_q_table = {}\n",
    "best_trial = {'accuracy': -np.inf, 'policy': [], 'rewards': [], 'q_table': {}}  # Store best trial\n",
    "\n",
    "# ==========================\n",
    "# ðŸ”¹ GPU-Accelerated Q-Learning (Final Stability-Optimized Version)\n",
    "# ==========================\n",
    "def train_rl(params):\n",
    "    global hyperparam_values, accuracy_values, gamma_values, best_q_table, best_trial   # Store hyperparameters and best Q-table\n",
    "\n",
    "    # Extract parameters\n",
    "    alpha_init = params['alpha']\n",
    "    gamma = params['gamma']\n",
    "    epsilon_init = 0.1  # Start with reduced randomness\n",
    "    epsilon_decay = params['epsilon_decay']\n",
    "    training_iterations = 40  # Increased training iterations\n",
    "\n",
    "    # Track hyperparameters\n",
    "    gamma_values.append(gamma)\n",
    "    hyperparam_values.append(params)\n",
    "\n",
    "    # Initialize Q-table (reuse best Q-table if available)\n",
    "    q_table = best_q_table.copy() if best_q_table else {}\n",
    "\n",
    "    # Initialize alpha and epsilon decay parameters\n",
    "    alpha = alpha_init\n",
    "    epsilon = epsilon_init\n",
    "    alpha_decay = 0.002  # Slower decay to allow stable learning\n",
    "    epsilon_min = 0.05\n",
    "    epsilon_lambda = 0.005  # Even smoother transition from exploration to exploitation\n",
    "\n",
    "    # Rolling average for policy consistency\n",
    "    policy_consistency_window = []\n",
    "    q_table_sizes = []  # Track Q-table growth\n",
    "    # Initialize prev_q_table before training\n",
    "    prev_q_table = {}  \n",
    "\n",
    "    # RL Training with Vectorized Operations\n",
    "    batch_size = 2000  # Large batch size stabilizes learning\n",
    "\n",
    "    for t in range(training_iterations):\n",
    "        for batch_start in range(0, len(df_transitions), batch_size):\n",
    "            batch = df_transitions.iloc[batch_start:batch_start + batch_size].to_pandas()  # Convert batch to Pandas\n",
    "\n",
    "            states = batch['Grid_Cell'].values\n",
    "            actions = batch['Next_Grid_Cell'].values\n",
    "\n",
    "            # Compute transition counts efficiently\n",
    "            unique_pairs, counts = np.unique(list(zip(states, actions)), axis=0, return_counts=True)\n",
    "\n",
    "            for (state, action), count in zip(unique_pairs, counts):\n",
    "                if state not in q_table:\n",
    "                    q_table[state] = {}\n",
    "\n",
    "                # Ensure each state has at least one valid action with a default Q-value\n",
    "                if action not in q_table[state]:\n",
    "                    q_table[state][action] = np.random.uniform(0.01, 0.1)  # Small random Q-value\n",
    "\n",
    "                # Convert Grid Cells to numerical coordinates\n",
    "                state_coords = parse_grid_cell(state)\n",
    "                action_coords = parse_grid_cell(action)\n",
    "\n",
    "                # Compute Euclidean distance penalty (More balanced)\n",
    "                distance_penalty = np.exp(-np.linalg.norm(state_coords - action_coords) / grid_size) * 0.2\n",
    "\n",
    "                # 1ï¸âƒ£ **Final Adjusted Reward Function (More Stability)**\n",
    "                reward = np.log(count + 20) / 5 + 0.03 + distance_penalty - 0.008  # Smoother scaling\n",
    "\n",
    "                # âœ… **Regularized Q-learning update with stronger stability**\n",
    "                max_future_q = max(q_table[state].values(), default=0)\n",
    "                q_update = reward + gamma * max_future_q - q_table[state][action]\n",
    "                q_table[state][action] += alpha * q_update - 0.0015 * abs(q_table[state][action])  # Stronger Q-value regularization\n",
    "\n",
    "        # âœ… Preserve learned policies between training iterations\n",
    "        if t > 0:\n",
    "            prev_q_table = q_table.copy()\n",
    "\n",
    "        # âœ… Track Q-Table Convergence **AFTER each training iteration**\n",
    "        track_q_table_convergence(prev_q_table, q_table)\n",
    "        q_table_sizes.append(len(q_table))  # Log Q-table growth\n",
    "\n",
    "        # âœ… Compute Expected Reward Growth\n",
    "        compute_expected_reward([q_table[state][action] for state in q_table for action in q_table[state]], gamma)\n",
    "\n",
    "        # âœ… Adaptive Epsilon Decay (Slower and more stable)\n",
    "        epsilon = epsilon_min + (epsilon_init - epsilon_min) * np.exp(-epsilon_lambda * t)\n",
    "\n",
    "        # âœ… Adaptive Alpha Decay for Controlled Learning Rate\n",
    "        alpha = max(0.005, alpha_init / (1 + alpha_decay * t))  # Lower bound prevents sharp drops\n",
    "\n",
    "        # # âœ… **Final Stability Enhancements**\n",
    "        # # ðŸ”¹ Force Exploitation in Last 5% of Training Iterations\n",
    "        # if t > training_iterations * 0.95:\n",
    "        #     epsilon = 0.01  # Minimal exploration, forcing exploitation\n",
    "        \n",
    "        # # ðŸ”¹ Soft Lock on Policy Updates in the Last 3% of Iterations\n",
    "        # if t > training_iterations * 0.97:\n",
    "        #     alpha = 0.003  # Lock learning with very small updates\n",
    "\n",
    "        # # ðŸ”¹ **Final Q-Value Freezing Near Convergence**\n",
    "        # if t > training_iterations * 0.98:\n",
    "        #     for state in q_table:\n",
    "        #         for action in q_table[state]:\n",
    "        #             q_table[state][action] *= 0.999  # More gradual freezing\n",
    "\n",
    "    # âœ… Store best Q-table after training\n",
    "    best_q_table = q_table.copy()\n",
    "\n",
    "    # ðŸ›  Debugging Step: Check if Q-table was updated\n",
    "    print(f\"Total states in Q-table: {len(q_table)}\")\n",
    "\n",
    "    # ==========================\n",
    "    # ðŸ”¹ RL Prediction (Using Decayed Epsilon)\n",
    "    # ==========================\n",
    "    def predict_next_grid(state):\n",
    "        if state in q_table and q_table[state]:  # Ensure Q-values exist\n",
    "            if random.uniform(0, 1) < epsilon:\n",
    "                return random.choice(list(q_table[state].keys()))  # Explore\n",
    "            return max(q_table[state], key=q_table[state].get)  # Exploit\n",
    "        return random.choice(list(q_table.keys())) if q_table else None\n",
    "\n",
    "    df_post_depletion['Predicted_Grid_Cell_RL'] = df_post_depletion['Grid_Cell'].to_pandas().map(predict_next_grid)\n",
    "    df_post_depletion['Correct_Prediction_RL'] = df_post_depletion['Predicted_Grid_Cell_RL'] == df_post_depletion['Grid_Cell']\n",
    "    accuracy = df_post_depletion['Correct_Prediction_RL'].mean()\n",
    "    accuracy_values.append(accuracy)  # Store for later validation\n",
    "\n",
    "    if accuracy > best_trial['accuracy']:\n",
    "        best_trial['accuracy'] = accuracy\n",
    "        best_trial['policy'] = policy_consistency.copy()\n",
    "        best_trial['rewards'] = reward_per_episode.copy()\n",
    "        best_trial['q_table'] = q_table.copy()\n",
    "        best_q_table = q_table.copy()  # Store best Q-table permanently\n",
    "\n",
    "    # ðŸ›  Debugging Step: Check if predictions are being made\n",
    "    print(f\"Total predictions made: {df_post_depletion['Predicted_Grid_Cell_RL'].notna().sum()}\")\n",
    "    print(f\"Trial Accuracy: {accuracy:.2%} | Params: {params}\")\n",
    "\n",
    "    return {'loss': -accuracy, 'status': STATUS_OK}\n",
    "\n",
    "\n",
    "# ==========================\n",
    "# ðŸ”¹ Hyperparameter Optimization (Final Fine-Tuning)\n",
    "# ==========================\n",
    "param_space = {\n",
    "    'alpha': hp.uniform('alpha', 0.18, 0.23),  # Narrowed for smoother learning\n",
    "    'gamma': hp.uniform('gamma', 0.68, 0.75),  # Focused on long-term reward stability\n",
    "    'epsilon_decay': hp.uniform('epsilon_decay', 0.987, 0.993),  # Optimized for better convergence\n",
    "}\n",
    "\n",
    "trials = Trials()\n",
    "best_params = fmin(fn=train_rl, space=param_space, algo=tpe.suggest, max_evals=4, trials=trials)  # Reduced max_evals\n",
    "\n",
    "\n",
    "print(f\"Best RL Parameters: {best_params}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Set visualization style\n",
    "sns.set(style=\"whitegrid\")\n",
    "\n",
    "\n",
    "\n",
    "# âœ… 2ï¸âƒ£ Policy Consistency Over Time\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.plot(policy_consistency, label=\"Policy Consistency\", color='green', linewidth=2)\n",
    "plt.xlabel(\"Training Iteration\")\n",
    "plt.ylabel(\"Consistency (%)\")\n",
    "plt.title(\"Policy Consistency Over Training\")\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "# âœ… 4ï¸âƒ£ Expected Reward Growth (Checks if Learning is Improving)\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.plot(reward_per_episode, label=\"Expected Reward\", color='purple', marker=\"o\", linewidth=2)\n",
    "plt.xlabel(\"Training Iteration\")\n",
    "plt.ylabel(\"Total Discounted Reward\")\n",
    "plt.title(\"Cumulative Expected Reward Over Training\")\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "# âœ… 5ï¸âƒ£ Accuracy Evolution Across Trials\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.plot(accuracy_values, label=\"Accuracy\", marker=\"o\", color='orange', linewidth=2)\n",
    "plt.xlabel(\"Trial Number\")\n",
    "plt.ylabel(\"Prediction Accuracy (%)\")\n",
    "plt.title(\"RL Model Accuracy Over Trials\")\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "# âœ… 6ï¸âƒ£ Hyperparameter Sensitivity Analysis\n",
    "fig, axes = plt.subplots(1, 3, figsize=(18, 5))\n",
    "\n",
    "# ðŸŽ¯ Alpha vs Accuracy\n",
    "axes[0].scatter([p[\"alpha\"] for p in hyperparam_values], accuracy_values, color='blue', alpha=0.6)\n",
    "axes[0].set_xlabel(\"Alpha (Learning Rate)\")\n",
    "axes[0].set_ylabel(\"Prediction Accuracy (%)\")\n",
    "axes[0].set_title(\"Effect of Alpha on Accuracy\")\n",
    "\n",
    "# ðŸŽ¯ Gamma vs Accuracy\n",
    "axes[1].scatter(gamma_values, accuracy_values, color='red', alpha=0.6)\n",
    "axes[1].set_xlabel(\"Gamma (Discount Factor)\")\n",
    "axes[1].set_ylabel(\"Prediction Accuracy (%)\")\n",
    "axes[1].set_title(\"Effect of Gamma on Accuracy\")\n",
    "\n",
    "# ðŸŽ¯ Epsilon vs Accuracy\n",
    "# axes[2].scatter([p[\"epsilon\"] for p in hyperparam_values], accuracy_values, color='green', alpha=0.6)\n",
    "# axes[2].set_xlabel(\"Epsilon (Exploration Rate)\")\n",
    "# axes[2].set_ylabel(\"Prediction Accuracy (%)\")\n",
    "# axes[2].set_title(\"Effect of Epsilon on Accuracy\")\n",
    "\n",
    "# plt.tight_layout()\n",
    "# plt.show()\n",
    "\n",
    "# Apply rolling average smoothing for reward per episode\n",
    "# reward_per_episode_smoothed = pd.Series(reward_per_episode).rolling(window=10, min_periods=1).mean()\n",
    "\n",
    "# # âœ… 4ï¸âƒ£ Expected Reward Growth (Smoothed Version)\n",
    "# plt.figure(figsize=(10, 5))\n",
    "# plt.plot(reward_per_episode_smoothed, label=\"Smoothed Expected Reward\", color='purple', marker=\"o\", linewidth=2)\n",
    "# plt.xlabel(\"Training Iteration\")\n",
    "# plt.ylabel(\"Total Discounted Reward\")\n",
    "# plt.title(\"Smoothed Cumulative Expected Reward Over Training\")\n",
    "# plt.legend()\n",
    "# plt.show()\n",
    "\n",
    "\n",
    "# ==========================\n",
    "# ðŸ”¹ Plot Best Trial Results\n",
    "# ==========================\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.plot(best_trial['policy'], label=\"Best Policy Consistency\", color='green', linewidth=2)\n",
    "plt.xlabel(\"Training Iteration\")\n",
    "plt.ylabel(\"Consistency (%)\")\n",
    "plt.title(\"Policy Consistency Over Training (Best Trial)\")\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "\n",
    "plt.figure(figsize=(10, 5))\n",
    "colors =plt.cm.viridis(np.linspace(0, 1, len(rewards_per_eval)))  # Unique colors\n",
    "\n",
    "for i, (eval_id, rewards) in enumerate(rewards_per_eval.items()):\n",
    "    plt.plot(rewards, label=f\"Eval {eval_id + 1}\", color=colors[i], linewidth=2)\n",
    "\n",
    "plt.xlabel(\"Training Iteration\")\n",
    "plt.ylabel(\"Total Discounted Reward\")\n",
    "plt.title(\"Cumulative Expected Reward Over Training (All Evaluations)\")\n",
    "plt.legend()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import pandas as pd\n",
    "# import numpy as np\n",
    "\n",
    "# # ==========================\n",
    "# # ðŸ”¹ Load and Prepare Data\n",
    "# # ==========================\n",
    "# df_updated = df.copy().sort_values(by=['deviceID', 'Timestamp']).reset_index(drop=True)\n",
    "\n",
    "# # Compute previous SOC per device to measure depletion rate\n",
    "# df_updated['Prev_SOC'] = df_updated.groupby('deviceID')['SOC_batt'].shift(1)  \n",
    "# df_updated['SOC_Diff'] = df_updated['Prev_SOC'] - df_updated['SOC_batt']  # Compute SOC depletion per row\n",
    "\n",
    "# # Compute average depletion rate per grid\n",
    "# soc_depletion_rate = df_updated.groupby(['Lat_Grid', 'Log_Grid'])['SOC_Diff'].mean().fillna(0)\n",
    "\n",
    "# # Reset negative depletion values (e.g., due to charging)\n",
    "# soc_depletion_rate = soc_depletion_rate.clip(lower=0)\n",
    "\n",
    "# print(\"âœ… SOC depletion rate per grid computed!\")\n",
    "\n",
    "# # ==========================\n",
    "# # ðŸ”¹ Compute Dynamic Threshold for Switching Off\n",
    "# # ==========================\n",
    "# base_threshold = 30  # Default threshold if no adjustments needed\n",
    "\n",
    "# def compute_dynamic_threshold(row):\n",
    "#     depletion = soc_depletion_rate.get((row['Lat_Grid'], row['Log_Grid']), 0)\n",
    "\n",
    "#     # More aggressive threshold adjustment based on depletion\n",
    "#     adjusted_threshold = 20 + np.clip(depletion * 2.5, 5, 35)  # Wider spread from 20% to 55%\n",
    "#     return min(adjusted_threshold, 55)  # Allow up to 55% instead of 45%\n",
    "\n",
    "# df_updated['SOC_Threshold'] = df_updated.apply(compute_dynamic_threshold, axis=1)\n",
    "\n",
    "# print(\"âœ… Dynamic SOC threshold per grid computed!\")\n",
    "\n",
    "# # ==========================\n",
    "# # ðŸ”¹ Implement Adaptive Sensor Switching\n",
    "# # ==========================\n",
    "# df_updated['Sensor_ON'] = df_updated['SOC_batt'] > df_updated['SOC_Threshold']\n",
    "\n",
    "# # Calculate energy saved when sensor is OFF\n",
    "# df_updated['Energy_Saved'] = np.where(df_updated['Sensor_ON'] == False, df_updated['SOC_Threshold'] - df_updated['SOC_batt'], 0)\n",
    "\n",
    "# print(\"âœ… Sensor switching based on dynamic SOC thresholds applied!\")\n",
    "\n",
    "# # ==========================\n",
    "# # ðŸ”¹ Evaluate the Impact on Coverage\n",
    "# # ==========================\n",
    "# static_coverage = df_updated[df_updated['SOC_batt'] > 30]['Lat_Grid'].nunique()\n",
    "# dynamic_coverage = df_updated[df_updated['Sensor_ON'] == True]['Lat_Grid'].nunique()\n",
    "\n",
    "# print(f\"ðŸŒ Coverage with Static 30% Threshold: {static_coverage} grids\")\n",
    "# print(f\"ðŸš€ Coverage with Dynamic Threshold: {dynamic_coverage} grids\")\n",
    "\n",
    "# # Compute percentage increase in coverage\n",
    "# coverage_increase = ((dynamic_coverage - static_coverage) / static_coverage) * 100\n",
    "# print(f\"ðŸ“ˆ Increase in Coverage Due to Adaptive Switching: {coverage_increase:.2f}%\")\n",
    "\n",
    "# # ==========================\n",
    "# # ðŸ”¹ Save the Updated Data\n",
    "# # ==========================\n",
    "# output_path = \"updated_SOC_batt_with_dynamic_threshold.xlsx\"\n",
    "# df_updated.to_excel(output_path, index=False)\n",
    "# print(f\"âœ… Updated dataset saved: {output_path}\")\n",
    "\n",
    "# # ==========================\n",
    "# # ðŸ”¹ Debugging Step: Inspect the Thresholds\n",
    "# # ==========================\n",
    "# print(\"\\nðŸ” Checking SOC Threshold Distribution:\")\n",
    "# print(df_updated[['Lat_Grid', 'Log_Grid', 'SOC_Threshold']].drop_duplicates().sort_values(by='SOC_Threshold'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import numpy as np\n",
    "# import pandas as pd\n",
    "# import random\n",
    "# import cudf\n",
    "# from collections import defaultdict\n",
    "\n",
    "# # âœ… RL Parameters\n",
    "# alpha = 0.1\n",
    "# gamma = 0.9\n",
    "# epsilon = 0.2\n",
    "# episodes = 500\n",
    "# actions = [-5, 0, 5]\n",
    "\n",
    "# # âœ… Convert cuDF to Pandas before datetime processing\n",
    "# df_pd = df_gdf.to_pandas()\n",
    "# df_pd['Timestamp'] = pd.to_datetime(df_pd['Timestamp'])\n",
    "\n",
    "# # âœ… Create Time Bins (every 15 minutes)\n",
    "# df_pd['Time_Bin'] = (df_pd['Timestamp'].dt.hour * 4) + (df_pd['Timestamp'].dt.minute // 15)\n",
    "\n",
    "# # âœ… Compute Battery Consumption Rate\n",
    "# df_pd = df_pd.sort_values(by=['deviceID', 'Timestamp'])\n",
    "# df_pd['Battery_Consumption_Rate'] = (\n",
    "#     df_pd.groupby('deviceID')['SOC_batt'].diff() /\n",
    "#     df_pd.groupby('deviceID')['Timestamp'].diff().dt.total_seconds() * 60\n",
    "# )\n",
    "# df_pd['Battery_Consumption_Rate'] = df_pd['Battery_Consumption_Rate'].fillna(0).abs()\n",
    "\n",
    "# # âœ… RL Training with Minimal Debugging\n",
    "# for episode in range(episodes):\n",
    "#     df_temp = df_pd.copy().sort_values(by=['deviceID', 'Timestamp'])\n",
    "#     reward_per_episode = []\n",
    "\n",
    "#     if episode % 50 == 0:  # Print summary every 50 episodes\n",
    "#         print(f\"\\nðŸ”„ Episode {episode + 1}/{episodes} - Training in Progress...\")\n",
    "\n",
    "#     for i, row in df_temp.iterrows():\n",
    "#         state = (row['SOC_batt'], row['Battery_Consumption_Rate'], row['Grid_Cell'], row['Time_Bin'])\n",
    "\n",
    "#         # âœ… Exploration vs Exploitation\n",
    "#         action = (\n",
    "#             random.choice(actions)\n",
    "#             if random.uniform(0, 1) < epsilon\n",
    "#             else max(q_table[state], key=q_table[state].get, default=0)\n",
    "#         )\n",
    "\n",
    "#         # âœ… Adjust Sensing Interval\n",
    "#         new_interval = max(3, min(30, row.get('Sensing_Interval', 10) + action))\n",
    "#         df_temp.at[i, 'Sensing_Interval'] = new_interval\n",
    "\n",
    "#         # âœ… Reward: Encourage energy savings & balanced sensing\n",
    "#         new_soc = row['SOC_batt'] - (row['Battery_Consumption_Rate'] * (new_interval / 60))\n",
    "#         reward = (new_soc - row['SOC_batt']) + (0.1 * new_interval)\n",
    "#         reward_per_episode.append(reward)\n",
    "\n",
    "#         # âœ… Update Q-table using Bellman equation\n",
    "#         next_state = (new_soc, row['Battery_Consumption_Rate'], row['Grid_Cell'], row['Time_Bin'])\n",
    "#         max_future_q = max(q_table[next_state].values(), default=0)\n",
    "#         q_table[state][action] = (1 - alpha) * q_table[state][action] + alpha * (reward + gamma * max_future_q)\n",
    "\n",
    "#     # âœ… Print episode summary every 50 iterations\n",
    "#     if episode % 50 == 0:\n",
    "#         print(f\"ðŸ“ˆ Avg Reward (Last 50): {np.mean(reward_per_episode[-50:]):.4f}\")\n",
    "\n",
    "#     # âœ… Decay Epsilon (Slower)\n",
    "#     epsilon = max(0.1, epsilon * 0.995)\n",
    "\n",
    "# print(\"\\nâœ… RL Training Completed!\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Mathematical Formulation of Post-Depletion Trajectory Analysis**\n",
    "\n",
    "This analysis enables a detailed comparison of **actual vs predicted movement** following battery depletion, revealing:\n",
    "- How vehicles move after depletion.\n",
    "- The effectiveness of the **Markov-based prediction model**.\n",
    "- Patterns in vehicle trajectory shifts post-depletion.\n",
    "\n",
    "## **1. Extracting Unique Days of Battery Depletion**\n",
    "We define a unique day $ d $ as a calendar date where at least one vehicle experienced battery depletion:\n",
    "\n",
    "$$\n",
    "D = \\{ d \\mid \\exists i, SOC_{\\text{batt}, i} < 50\\%, \\text{on day } d \\}\n",
    "$$\n",
    "\n",
    "where $ D $ is the set of all days in which a depletion event occurred.\n",
    "\n",
    "## **2. Filtering Data for Each Depletion Day**\n",
    "For each $ d \\in D $, we extract:\n",
    "\n",
    "- **Post-depletion records**: \n",
    "  $$\n",
    "  X_d = \\{ x_i \\mid x_i \\in X, \\text{Timestamp}(x_i) = d, SOC_{\\text{batt}, i} < 50\\% \\}\n",
    "  $$\n",
    "\n",
    "- **Pre-depletion records**:\n",
    "  $$\n",
    "  Y_d = \\{ y_i \\mid y_i \\in Y, \\text{Timestamp}(y_i) = d, SOC_{\\text{batt}, i} \\geq 50\\% \\}\n",
    "  $$\n",
    "\n",
    "where:\n",
    "- $ X_d $ represents all vehicle records **after depletion**.\n",
    "- $ Y_d $ represents all vehicle records **before depletion**.\n",
    "\n",
    "## **3. Mapping Vehicles to Grid Cells**\n",
    "For each vehicle's recorded GPS point $ p_i $ on day $ d $:\n",
    "\n",
    "$$\n",
    "G_i = \\arg\\max_j \\mathbb{1}(p_i \\in P_j)\n",
    "$$\n",
    "\n",
    "where:\n",
    "- $ G_i $ is the assigned grid cell.\n",
    "- $ P_j $ represents the grid cells.\n",
    "- $ \\mathbb{1}(p_i \\in P_j) $ is an indicator function that is **1** if $ p_i $ is inside $ P_j $.\n",
    "\n",
    "Thus, we define:\n",
    "\n",
    "$$\n",
    "G_d^{\\text{actual}} = \\{ G_i \\mid x_i \\in X_d \\}\n",
    "$$\n",
    "\n",
    "$$\n",
    "G_d^{\\text{pre}} = \\{ G_i \\mid y_i \\in Y_d \\}\n",
    "$$\n",
    "\n",
    "## **4. Extracting Predicted Grid Cells**\n",
    "The predicted post-depletion grid cells for each vehicle are computed from the **Markov Transition Model**:\n",
    "\n",
    "$$\n",
    "G_{\\text{predicted}, i} = \\arg\\max_{G_j} P(G_j | G_i)\n",
    "$$\n",
    "\n",
    "for each vehicle location $ G_i $ before depletion.\n",
    "\n",
    "The predicted set is:\n",
    "\n",
    "$$\n",
    "G_d^{\\text{predicted}} = \\{ G_{\\text{predicted}, i} \\mid x_i \\in X_d \\}\n",
    "$$\n",
    "\n",
    "## **5. Visualizing the Spatial Trajectories**\n",
    "We generate a geospatial plot for each day $ d $:\n",
    "\n",
    "- **Pre-Depletion Trajectory** $ G_d^{\\text{pre}} $ (Black)\n",
    "- **Actual Post-Depletion Trajectory** $ G_d^{\\text{actual}} $ (Red)\n",
    "- **Predicted Trajectory** $ G_d^{\\text{predicted}} $ (Blue)\n",
    "\n",
    "Each grid cell is represented as a polygon, where:\n",
    "\n",
    "$$\n",
    "P_j = \\{ (x_k, y_k) \\mid k = 1,2,3,4 \\}\n",
    "$$\n",
    "\n",
    "and plotted based on its category:\n",
    "\n",
    "$$\n",
    "\\text{Color}(P_j) =\n",
    "\\begin{cases} \n",
    "\\text{black}, & P_j \\in G_d^{\\text{pre}} \\\\\n",
    "\\text{red}, & P_j \\in G_d^{\\text{actual}} \\\\\n",
    "\\text{blue}, & P_j \\in G_d^{\\text{predicted}}\n",
    "\\end{cases}\n",
    "$$\n",
    "\n",
    "## **6. Overlaying the OpenStreetMap Basemap**\n",
    "The plotted grid cells are projected onto a real-world **OpenStreetMap** (OSM) basemap with coordinate reference system:\n",
    "\n",
    "$$\n",
    "\\text{CRS} = \\text{EPSG:4326}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract unique days where depletion occurred\n",
    "depleted_days = df_post_depletion['Timestamp'].dt.date.unique()\n",
    "\n",
    "# Loop through each depleted day and generate a plot\n",
    "for day in depleted_days:\n",
    "    # Filter data for the current day\n",
    "    df_day = df_post_depletion[df_post_depletion['Timestamp'].dt.date == day]\n",
    "    df_pre_depletion_day = df_pre_depletion[df_pre_depletion['Timestamp'].dt.date == day]\n",
    "\n",
    "    # Convert actual, predicted, and pre-depletion data into GeoDataFrames\n",
    "    gdf_actual = grid_gdf.loc[grid_gdf.index.isin(df_day['Grid_Cell'])].copy()\n",
    "    gdf_actual['Color'] = 'red'\n",
    "\n",
    "    predicted_grid_cells = df_day['Predicted_Grid_Cell_Hybrid'].dropna().unique()\n",
    "    gdf_predicted = grid_gdf.loc[grid_gdf.index.isin(predicted_grid_cells)].copy()\n",
    "    gdf_predicted['Color'] = 'blue'\n",
    "\n",
    "    pre_depletion_grid_cells = df_pre_depletion_day['Grid_Cell'].unique()\n",
    "    gdf_pre_depletion = grid_gdf.loc[grid_gdf.index.isin(pre_depletion_grid_cells)].copy()\n",
    "    gdf_pre_depletion['Color'] = 'black'\n",
    "\n",
    "    # Skip plotting if all GeoDataFrames are empty for the day\n",
    "    if gdf_actual.empty and gdf_predicted.empty and gdf_pre_depletion.empty:\n",
    "        print(f\"Skipping {day}: No valid data for plotting.\")\n",
    "        continue\n",
    "\n",
    "    # Create the plot for the current day\n",
    "    fig, ax = plt.subplots(figsize=(12, 12))\n",
    "\n",
    "    # Plot Grid Cells\n",
    "    grid_gdf.plot(ax=ax, color='lightgrey', alpha=0.2)\n",
    "\n",
    "    # Plot Pre-Depletion Trajectory (Black) if not empty\n",
    "    if not gdf_pre_depletion.empty:\n",
    "        gdf_pre_depletion.plot(ax=ax, color='black', alpha=0.5, label=\"Pre-Depletion Trajectory\")\n",
    "\n",
    "    # Plot Actual Trajectory (Red) if not empty\n",
    "    if not gdf_actual.empty:\n",
    "        gdf_actual.plot(ax=ax, color='red', alpha=0.5, label=\"Actual Trajectory\")\n",
    "\n",
    "    # Plot Predicted Trajectory (Blue) if not empty\n",
    "    if not gdf_predicted.empty:\n",
    "        gdf_predicted.plot(ax=ax, color='blue', alpha=0.5, label=\"Predicted Trajectory\")\n",
    "\n",
    "    # Add Basemap\n",
    "    try:\n",
    "        ctx.add_basemap(ax, crs=grid_gdf.crs.to_string(), source=ctx.providers.CartoDB.Positron)\n",
    "    except Exception as e:\n",
    "        print(f\"Basemap Error on {day}: {e}\")\n",
    "\n",
    "    # Formatting\n",
    "    ax.set_title(f\"Actual vs Predicted Trajectory (Post-Depletion) - {day}\", fontsize=14)\n",
    "    ax.legend()\n",
    "    ax.set_axis_off()\n",
    "\n",
    "    # Show plot\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_temp = df.copy().reset_index(drop=True)  # Ensure indices are sequential\n",
    "df_temp=df_temp.sort_values(by=['Timestamp']).reset_index(drop=True) \n",
    "\n",
    "# Define different time thresholds to compare\n",
    "time_thresholds = {\n",
    "    # \"3 sec\": 3,\n",
    "    \"12 sec\": 12\n",
    "}\n",
    "\n",
    "# Create a dictionary to store SOC and sensor states for each threshold\n",
    "soc_depletion_results = {}\n",
    "\n",
    "# Iterate over different time thresholds\n",
    "for label, TIME_THRESHOLD in time_thresholds.items():\n",
    "\n",
    "    # Track last sensed timestamp, and stored energy during OFF periods\n",
    "    last_sensed_time = {}\n",
    "    stored_energy = {}\n",
    "\n",
    "    # Previous date\n",
    "    prev_date=None\n",
    "    \n",
    "    for i in range(len(df_temp)):\n",
    "        row = df_temp.iloc[i]\n",
    "        grid_key = (row['Lat_Grid'], row['Log_Grid'])  # Unique grid identifier\n",
    "        current_time = row['Timestamp']\n",
    "        current_date = row['Date']\n",
    "        device= row['deviceID']\n",
    "\n",
    "        # Initialise inter-row differences when OFF\n",
    "        d_diff_prev=0 \n",
    "        \n",
    "        # Reset stored energy at the start of a new day\n",
    "        if prev_date is not None and current_date != prev_date:\n",
    "            stored_energy={}  # Reset stored energy for all grid cells\n",
    "            df_temp.loc[i:, f'Energy_Saved_{label}'] = 0  # Reset energy savings for the new day\n",
    "            print(f\"[RESET] Reset stored energy for new day: {current_date}\")\n",
    "\n",
    "        prev_date = current_date  # Update previous date tracker\n",
    "\n",
    "        if df_temp.loc[i, 'SOC_batt']>99:\n",
    "            stored_energy[grid_key]=0\n",
    "            df_temp.loc[i:, f'Energy_Saved_{label}']=0\n",
    "\n",
    "        # If the grid was sensed recently (within the threshold), turn OFF the sensor\n",
    "        if grid_key in last_sensed_time and (current_time - last_sensed_time[grid_key]).total_seconds() < TIME_THRESHOLD:\n",
    "            df_temp.at[i, f'Sensor_ON_{label}'] = False   \n",
    "\n",
    "            # Accumulate stored energy\n",
    "            if i > 0 and pd.notna(df_temp.iloc[i - 1]['SOC_batt']) and pd.notna(row['SOC_batt']):\n",
    "            \n",
    "                # Find the last preceding row for this device\n",
    "                if device == df_temp.iloc[i-1]['deviceID']:\n",
    "                    d_diff = max(0, df_temp.iloc[i - 1]['SOC_batt'] - row['SOC_batt']) #current inter-row difference\n",
    "                    diff = max(d_diff_prev, d_diff)\n",
    "                    stored_energy[grid_key] = df_temp.loc[i-1, f'Energy_Saved_{label}']\n",
    "                    stored_energy[grid_key] += diff\n",
    "                    \n",
    "                    if diff != 0: \n",
    "                        df_temp.loc[i:, f'Energy_Saved_{label}'] = stored_energy[grid_key]\n",
    "                    else:\n",
    "                        df_temp.loc[i:, f'Energy_Saved_{label}'] = df_temp.loc[i-1, f'Energy_Saved_{label}']\n",
    "\n",
    "                    d_diff_prev=diff\n",
    "                    print(f\"[OFF]: Accumulated {diff:.2f}% for device {device}. Total stored: {stored_energy[grid_key]:.2f}%\")\n",
    "\n",
    "                else:\n",
    "                    d_diff = max(0, df_temp.loc[df_temp.deviceID == device, :]['SOC_batt'].iloc[-1] - row['SOC_batt']) #current inter-row difference\n",
    "                    diff = max(d_diff_prev, d_diff)\n",
    "                    stored_energy[grid_key] = df_temp.loc[df_temp.deviceID == device, :][f'Energy_Saved_{label}'].iloc[-1]\n",
    "                    stored_energy[grid_key] += diff\n",
    "                    \n",
    "                    if diff != 0: \n",
    "                        df_temp.loc[i:, f'Energy_Saved_{label}'] = stored_energy[grid_key]\n",
    "                    else:\n",
    "                        df_temp.loc[i:, f'Energy_Saved_{label}'] = df_temp.loc[df_temp.deviceID == device, :][f'Energy_Saved_{label}'].iloc[-1]\n",
    "\n",
    "                    d_diff_prev=diff\n",
    "                    print(f\"CHANGE [OFF]: Accumulated {diff:.2f}% for device {device}. Total stored: {stored_energy[grid_key]:.2f}%\")\n",
    "\n",
    "        else:\n",
    "            # Update last sensed time when the sensor turns ON\n",
    "            last_sensed_time[grid_key] = current_time\n",
    "            df_temp.at[i, f'Sensor_ON_{label}'] = True\n",
    "            d_diff_prev=0\n",
    "\n",
    "            if device == df_temp.iloc[i-1]['deviceID']:\n",
    "\n",
    "                # Ensure stored_energy is initialized per grid cell without overwriting previous values\n",
    "                if grid_key not in stored_energy:\n",
    "                    if i > 0:\n",
    "                        #Carry forward the stored energy from the last known row\n",
    "                        stored_energy[grid_key] = df_temp.loc[i-1, f'Energy_Saved_{label}']\n",
    "                        df_temp.loc[i:, f'Energy_Saved_{label}'] = stored_energy[grid_key]\n",
    "                    elif i == 0:\n",
    "                        stored_energy[grid_key] = 0  # First iteration, no prior energy \n",
    "                        df_temp.loc[i:, f'Energy_Saved_{label}'] = stored_energy[grid_key]  \n",
    "                print(f\"[ON]: device {device}. Total stored: {stored_energy[grid_key]:.2f}%\")\n",
    "\n",
    "            else:\n",
    "                 # Ensure stored_energy is initialized per grid cell without overwriting previous values\n",
    "                if grid_key not in stored_energy:\n",
    "                    if i > 0:\n",
    "                        #Carry forward the stored energy from the last known row\n",
    "                        stored_energy[grid_key] = df_temp.loc[df_temp.deviceID == device, :][f'Energy_Saved_{label}'].iloc[-1]\n",
    "                        df_temp.loc[i:, f'Energy_Saved_{label}'] = stored_energy[grid_key]\n",
    "                    elif i == 0:\n",
    "                        stored_energy[grid_key] = 0  # First iteration, no prior energy \n",
    "                        df_temp.loc[i:, f'Energy_Saved_{label}'] = stored_energy[grid_key]                 \n",
    "                \n",
    "                print(f\"CHANGE [ON]: device {device}. Total stored: {stored_energy[grid_key]:.2f}%\")\n",
    "\n",
    "    \n",
    "    # Compute new SOC_batt with savings\n",
    "    df_temp[f'SOC_batt_{label}'] = df_temp['SOC_batt'] + df_temp[f'Energy_Saved_{label}']\n",
    "    df_temp[f'SOC_batt_{label}'] = df_temp[f'SOC_batt_{label}'].clip(upper=100)\n",
    "\n",
    "    # Compute SOC depletion for this threshold\n",
    "    daily_soc = df_temp.groupby(['Date', 'deviceID'])[f'SOC_batt_{label}'].mean()\n",
    "    soc_depletion_results[label] = daily_soc\n",
    "\n",
    "\n",
    "# Baseline: Compute SOC depletion without constraints\n",
    "soc_depletion_results[\"Baseline\"] = df_temp.groupby(['Date', 'deviceID'])['SOC_batt'].mean()\n",
    "\n",
    "# Convert results to a DataFrame for plotting\n",
    "soc_depletion_df = pd.DataFrame(soc_depletion_results)\n",
    "\n",
    "# Save the updated dataset with sensor states and energy savings for each threshold\n",
    "output_path = \"/workspace/data/updated_SOC_batt_with_energy_savings.xlsx\"\n",
    "df_temp.to_excel(output_path, index=False)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define line styles and transparency levels for each threshold\n",
    "line_styles = {\n",
    "    \"Baseline\": \"--\",\n",
    "    # \"3 sec\": \"--\",\n",
    "    \"12 sec\": \"-\"\n",
    "}\n",
    "\n",
    "# Transparency levels for each threshold\n",
    "alpha_values = {\n",
    "    \"Baseline\": 0.5,  # 70% transparent\n",
    "    # \"3 sec\": 0.7,  # 30% transparent\n",
    "    \"12 sec\": 1   #Fully visible\n",
    "}\n",
    "\n",
    "# Predefined colors for devices\n",
    "predefined_colors = ['#007FFF', '#DC143C', '#FF4500','#39FF14', '#800080']\n",
    "device_ids = set()\n",
    "\n",
    "for soc_series in soc_depletion_results.values():\n",
    "    device_ids.update(soc_series.index.get_level_values('deviceID').unique())\n",
    "\n",
    "# Create a color map using predefined colors\n",
    "color_map = {device_id: predefined_colors[i % len(predefined_colors)] for i, device_id in enumerate(sorted(device_ids))}\n",
    "\n",
    "# Plot SOC depletion for different devices and thresholds\n",
    "plt.figure(figsize=(12, 6))\n",
    "\n",
    "# Iterate over thresholds and plot per device\n",
    "for label, soc_series in soc_depletion_results.items():  # soc_series is a MultiIndexed Series\n",
    "    for device_id in soc_series.index.get_level_values('deviceID').unique():  # Get unique devices\n",
    "        device_data = soc_series[soc_series.index.get_level_values('deviceID') == device_id]\n",
    "        plt.plot(\n",
    "            device_data.index.get_level_values('Date'),  # X-axis: Dates\n",
    "            device_data.values,  # Y-axis: SOC values\n",
    "            linestyle=line_styles[label],\n",
    "            color=color_map[device_id],  # Use predefined color for the device\n",
    "            # marker='o',\n",
    "            # markersize=3,\n",
    "            alpha=alpha_values[label],  # Apply transparency per threshold\n",
    "            label=f\"Device {device_id} - {label}\"\n",
    "        )\n",
    "\n",
    "plt.xlabel('Date')\n",
    "plt.ylabel('Mean SOC (%)')\n",
    "plt.title('SOC Depletion Comparison Across Devices and Time Constraints')\n",
    "\n",
    "# Place the legend outside the plot\n",
    "plt.legend(\n",
    "    bbox_to_anchor=(1.05, 1),  # Place legend to the right of the plot\n",
    "    loc='upper left',          # Align legend to the top-left of the bounding box\n",
    "    borderaxespad=0.           # Reduce spacing between the legend and the plot\n",
    ")\n",
    "plt.grid(True)\n",
    "plt.xticks(rotation=45)\n",
    "\n",
    "# Adjust layout to make room for the legend\n",
    "plt.tight_layout()\n",
    "\n",
    "# Show plot\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute the count of times the sensor was turned OFF for each constraint scenario\n",
    "off_counts = {}\n",
    "\n",
    "# Iterate over different time thresholds\n",
    "for label, TIME_THRESHOLD in time_thresholds.items():\n",
    "    df_copy = df.copy()  # Work on a copy of the dataset\n",
    "    df_copy['Sensor_ON'] = True  # Default: Sensor is ON\n",
    "\n",
    "    # Track last sensed timestamp per grid cell\n",
    "    last_sensed_time = {}\n",
    "    off_count = 0\n",
    "\n",
    "    for i, row in df_copy.iterrows():\n",
    "        grid_key = (row['Lat_Grid'], row['Log_Grid'])  # Unique grid identifier\n",
    "        current_time = row['Timestamp']\n",
    "\n",
    "        # If the grid was sensed recently (within the threshold), turn OFF the sensor\n",
    "        if grid_key in last_sensed_time and (current_time - last_sensed_time[grid_key]).total_seconds() < TIME_THRESHOLD:\n",
    "            df_copy.at[i, 'Sensor_ON'] = False\n",
    "            off_count += 1\n",
    "        else:\n",
    "            # Update last sensed time when the sensor turns ON\n",
    "            last_sensed_time[grid_key] = current_time\n",
    "\n",
    "    off_counts[label] = off_count\n",
    "\n",
    "# Convert to DataFrame for better visualization\n",
    "off_counts_df = pd.DataFrame.from_dict(off_counts, orient='index', columns=['Sensor OFF Count'])\n",
    "\n",
    "# Display results\n",
    "off_counts_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract unique days where depletion occurred\n",
    "depleted_days = df_post_depletion['Timestamp'].dt.date.unique()\n",
    "\n",
    "# Loop through each depleted day for visualization\n",
    "for day in depleted_days:\n",
    "    # Filter data for the current depleted day\n",
    "    df_day_pre = df_pre_coverage[df_pre_coverage['Date'] == day]\n",
    "    df_day_new = df_new_coverage_only[df_new_coverage_only['Date'] == day]\n",
    "    df_day_actual = df_post_depletion[df_post_depletion['Timestamp'].dt.date == day]\n",
    "    df_day_predicted = df_post_depletion[df_post_depletion['Timestamp'].dt.date == day]\n",
    "\n",
    "    # Convert to GeoDataFrames\n",
    "    gdf_pre = grid_gdf.loc[grid_gdf.index.isin(df_day_pre['Grid_Cell'])].copy()\n",
    "    gdf_pre['Color'] = 'black'  # Pre-depletion trajectory\n",
    "\n",
    "    gdf_new = grid_gdf.loc[grid_gdf.index.isin(df_day_new['Grid_Cell'])].copy()\n",
    "    gdf_new['Color'] = 'green'  # Newly sensed due to 10min rule\n",
    "\n",
    "    gdf_actual = grid_gdf.loc[grid_gdf.index.isin(df_day_actual['Grid_Cell'])].copy()\n",
    "    gdf_actual['Color'] = 'red'  # Actual trajectory after depletion\n",
    "\n",
    "    predicted_grid_cells = df_day_predicted['Predicted_Grid_Cell'].dropna().unique()\n",
    "    gdf_predicted = grid_gdf.loc[grid_gdf.index.isin(predicted_grid_cells)].copy()\n",
    "    gdf_predicted['Color'] = 'blue'  # Predicted trajectory\n",
    "\n",
    "    # Skip if no relevant data for the day\n",
    "    if gdf_pre.empty and gdf_new.empty and gdf_actual.empty and gdf_predicted.empty:\n",
    "        print(f\"Skipping {day}: No valid data for plotting.\")\n",
    "        continue\n",
    "\n",
    "    # Create the plot\n",
    "    fig, ax = plt.subplots(figsize=(12, 12))\n",
    "\n",
    "    # Plot Grid Cells (Background)\n",
    "    grid_gdf.plot(ax=ax, color='lightgrey', edgecolor='grey', alpha=0.2)\n",
    "\n",
    "    # Plot Pre-Depletion Trajectory (Black)\n",
    "    if not gdf_pre.empty:\n",
    "        gdf_pre.plot(ax=ax, color='black', alpha=0.5, edgecolor='black', label=\"Pre-Depletion Trajectory\")\n",
    "\n",
    "    # Plot Actual Post-Depletion Trajectory (Red)\n",
    "    if not gdf_actual.empty:\n",
    "        gdf_actual.plot(ax=ax, color='red', alpha=0.5, edgecolor='red', label=\"Actual Trajectory (Post-Depletion)\")\n",
    "\n",
    "    # Plot Predicted Post-Depletion Trajectory (Blue)\n",
    "    if not gdf_predicted.empty:\n",
    "        gdf_predicted.plot(ax=ax, color='blue', alpha=0.5, edgecolor='blue', label=\"Predicted Trajectory\")\n",
    "\n",
    "    # Plot Newly Sensed Cells Due to 10-Minute Rule (Green)\n",
    "    if not gdf_new.empty:\n",
    "        gdf_new.plot(ax=ax, color='green', alpha=0.5, edgecolor='green', label=\"Newly Sensed Cells (10-min Interval)\")\n",
    "\n",
    "    # Add Basemap\n",
    "    try:\n",
    "        ctx.add_basemap(ax, crs=grid_gdf.crs.to_string(), source=ctx.providers.OpenStreetMap.Mapnik)\n",
    "    except Exception as e:\n",
    "        print(f\"Basemap Error on {day}: {e}\")\n",
    "\n",
    "    # Formatting\n",
    "    ax.set_title(f\"Trajectory Visualization with 10-Minute Sensing Constraint - {day}\", fontsize=14)\n",
    "    ax.legend()\n",
    "    ax.set_axis_off()\n",
    "\n",
    "    # Show plot\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dynamically get the bounds from the data\n",
    "min_lat, max_lat = df['Lat'].min(), df['Lat'].max()\n",
    "min_lon, max_lon = df['Log'].min(), df['Log'].max()\n",
    "\n",
    "# Define grid size (120x120 meters)\n",
    "grid_size = 120\n",
    "lat_resolution = grid_size / 111320  # Convert meters to latitude degrees\n",
    "lon_resolution_at_lat = lambda lat: grid_size / (111320 * np.cos(np.radians(lat)))\n",
    "\n",
    "# Generate grid covering the dataset area\n",
    "grid = []\n",
    "lat = min_lat\n",
    "while lat < max_lat:\n",
    "    lon = min_lon\n",
    "    while lon < max_lon:\n",
    "        lon_res = lon_resolution_at_lat(lat)\n",
    "        grid.append(Polygon([\n",
    "            (lon, lat),\n",
    "            (lon + lon_res, lat),\n",
    "            (lon + lon_res, lat + lat_resolution),\n",
    "            (lon, lat + lat_resolution)\n",
    "        ]))\n",
    "        lon += lon_res\n",
    "    lat += lat_resolution\n",
    "\n",
    "# Create an empty GeoDataFrame for the grid\n",
    "grid_gdf = gpd.GeoDataFrame({'geometry': grid, 'Count': 0}, crs=\"EPSG:4326\")\n",
    "\n",
    "# Create a GeoDataFrame for the data points\n",
    "df_gdf = gpd.GeoDataFrame(df, geometry=gpd.points_from_xy(df['Log'], df['Lat']), crs=\"EPSG:4326\")\n",
    "\n",
    "# Assign each measurement to a grid square\n",
    "for index, point in df_gdf.iterrows():\n",
    "    match = grid_gdf.contains(point.geometry)\n",
    "    if match.any():\n",
    "        grid_gdf.loc[match.idxmax(), 'Count'] += 1\n",
    "\n",
    "# Apply Fractional Power Scaling\n",
    "gamma = 0.3  # Adjust for visibility\n",
    "grid_gdf['Scaled_Count'] = (grid_gdf['Count'] + 1) ** gamma\n",
    "\n",
    "# Normalize values for color mapping\n",
    "norm = Normalize(vmin=grid_gdf['Scaled_Count'].min(), vmax=grid_gdf['Scaled_Count'].max())\n",
    "cmap = plt.get_cmap('jet')\n",
    "\n",
    "# Convert scaled values to hex colors\n",
    "grid_gdf['Color'] = grid_gdf['Scaled_Count'].apply(lambda x: to_hex(cmap(norm(x))))\n",
    "\n",
    "# Create Folium map centered on Stockholm\n",
    "m = folium.Map(location=[df['Lat'].mean(), df['Log'].mean()], zoom_start=12, tiles='Cartodb dark_matter')\n",
    "\n",
    "# Function to color the grid based on scaled counts\n",
    "def style_function(feature):\n",
    "    color = feature['properties']['Color']  # Get precomputed color\n",
    "    return {\n",
    "        'fillColor': color,\n",
    "        'color': 'black',\n",
    "        'weight': 0.1,\n",
    "        'fillOpacity': 0.4\n",
    "    }\n",
    "\n",
    "# Add grid layer to Folium\n",
    "folium.GeoJson(\n",
    "    grid_gdf,\n",
    "    name=\"Measurement Grid\",\n",
    "    style_function=style_function,\n",
    "    tooltip=folium.GeoJsonTooltip(fields=['Count'], aliases=[\"Measurements:\"])\n",
    ").add_to(m)\n",
    "\n",
    "# Add layer control\n",
    "folium.LayerControl().add_to(m)\n",
    "\n",
    "# Display the map\n",
    "m\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initial Analysis\n",
    "#### Check Data Distribution\n",
    "- Before plotting, inspect the distribution of the Count column to confirm the skew. If the Count values have a large range (e.g., some counts are much higher than others), you can apply a logarithmic scale to the color mapping. This makes smaller variations more distinguishable.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Group by spatial grid and count occurrences\n",
    "coverage = df.groupby(['Lat_Grid', 'Log_Grid']).size().reset_index(name='Count')\n",
    "\n",
    "# Count the frequency of each unique coverage count\n",
    "coverage_freq = coverage['Count'].value_counts().reset_index()\n",
    "coverage_freq.columns = ['Coverage Count', 'Frequency']\n",
    "\n",
    "# Sort in descending order\n",
    "coverage_freq = coverage_freq.sort_values(by='Coverage Count', ascending=False)\n",
    "\n",
    "# Find the maximum coverage count\n",
    "max_count = coverage['Count'].max()\n",
    "\n",
    "sns.histplot(coverage['Count'], bins=max_count, kde=True, color='blue')\n",
    "plt.title('Distribution of Coverage Counts')\n",
    "plt.xlabel('Coverage Count')\n",
    "plt.ylabel('Frequency')\n",
    "plt.ylim(0, 2)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df['Timestamp'] = pd.to_datetime(df['Timestamp'], unit='s')\n",
    "\n",
    "# # Define Zipf-Mandelbrot function\n",
    "# def zipf_mandelbrot_func(r, s, q, C):\n",
    "#     return C / (r + np.abs(q)) ** s  # Ensure q is positive\n",
    "\n",
    "# # Define resolutions to test\n",
    "# spatial_resolutions = [0.00001, 0.0001, 0.001, 0.01]\n",
    "# temporal_resolutions = ['10S', '30S', '1T', '5T']\n",
    "\n",
    "# # Store results\n",
    "# results = []\n",
    "\n",
    "# for spatial_resolution in spatial_resolutions:\n",
    "#     for temporal_resolution in temporal_resolutions:\n",
    "#         # Create spatial grid\n",
    "#         df['Lat_Grid'] = (df['Lat'] // spatial_resolution) * spatial_resolution\n",
    "#         df['Log_Grid'] = (df['Log'] // spatial_resolution) * spatial_resolution\n",
    "        \n",
    "#         # Create temporal bins\n",
    "#         df['Time_Bin'] = df['Timestamp'].dt.floor(temporal_resolution)\n",
    "\n",
    "#         # Group by spatial grid and count occurrences\n",
    "#         coverage = df.groupby(['Lat_Grid', 'Log_Grid']).size().reset_index(name='Count')\n",
    "\n",
    "#         # Sort data in Zipfian order\n",
    "#         sorted_counts = np.sort(coverage['Count'])[::-1]  # Descending order\n",
    "#         ranks = np.arange(1, len(sorted_counts) + 1)  # Rank numbers\n",
    "\n",
    "#         # Fit Zipf-Mandelbrot\n",
    "#         try:\n",
    "#             params, _ = curve_fit(zipf_mandelbrot_func, ranks, sorted_counts, \n",
    "#                                   p0=[1, 1, max(sorted_counts)], \n",
    "#                                   bounds=([0.5, 0.0001, 0], [3, 10, np.inf]))\n",
    "\n",
    "#             s_fit, q_fit, C_fit = params\n",
    "#             expected_values = zipf_mandelbrot_func(ranks, s_fit, q_fit, C_fit)\n",
    "            \n",
    "#             # Compute residuals\n",
    "#             residuals = sorted_counts - expected_values\n",
    "#             std_residuals = np.std(residuals)\n",
    "            \n",
    "#             # Perform KS test\n",
    "#             ks_stat, p_value = kstest(sorted_counts, zipf_mandelbrot_func, args=(s_fit, q_fit, C_fit))\n",
    "\n",
    "#             # Compute AIC (Akaike Information Criterion)\n",
    "#             AIC = -2 * np.log(p_value) + 2 * 3  # 3 parameters: s, q, C\n",
    "\n",
    "#             # Store results\n",
    "#             results.append({\n",
    "#                 'Spatial_Resolution': spatial_resolution,\n",
    "#                 'Temporal_Resolution': temporal_resolution,\n",
    "#                 'KS_Statistic': ks_stat,\n",
    "#                 'p_value': p_value,\n",
    "#                 'Std_Residuals': std_residuals,\n",
    "#                 'AIC': AIC\n",
    "#             })\n",
    "\n",
    "#         except RuntimeError:\n",
    "#             print(f\"Fit failed for Spatial={spatial_resolution}, Temporal={temporal_resolution}\")\n",
    "\n",
    "# # Convert results to DataFrame\n",
    "# results_df = pd.DataFrame(results)\n",
    "\n",
    "# # Select the best resolution (min AIC, high p-value, low KS statistic)\n",
    "# best_result = results_df.sort_values(by=['AIC', 'KS_Statistic'], ascending=[True, True]).iloc[0]\n",
    "# print(\"Best Resolution Parameters:\")\n",
    "# print(best_result)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sort coverage counts in descending order (ranked frequencies)\n",
    "sorted_counts = np.sort(coverage['Count'])[::-1]  # Descending order\n",
    "ranks = np.arange(1, len(sorted_counts) + 1)  # Rank numbers\n",
    "\n",
    "# Define Zipf-Mandelbrot function: f(r) = C / (r + q)^s\n",
    "def zipf_mandelbrot_func(r, s, q, C):\n",
    "    return C / (r + np.abs(q)) ** s  # Ensure q is positive\n",
    "\n",
    "# Fit Zipf-Mandelbrot with constraints to avoid numerical issues\n",
    "params, _ = curve_fit(zipf_mandelbrot_func, ranks, sorted_counts, p0=[1, 1, max(sorted_counts)], bounds=([0.5, 0.0001, 0], [3, 10, np.inf]))\n",
    "s_fit, q_fit, C_fit = params\n",
    "\n",
    "# Compute expected values from the fitted Zipf-Mandelbrot model\n",
    "expected_values = zipf_mandelbrot_func(ranks, s_fit, q_fit, C_fit)\n",
    "\n",
    "# Compute residuals (Observed - Expected)\n",
    "residuals = sorted_counts - expected_values\n",
    "relative_residuals = residuals / expected_values  # Normalize residuals\n",
    "\n",
    "# Plot Residuals\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.scatter(ranks, residuals, alpha=0.6, color=\"red\", label=\"Residuals (Observed - Expected)\")\n",
    "plt.axhline(0, linestyle=\"--\", color=\"black\", alpha=0.6)\n",
    "plt.xscale(\"log\")\n",
    "plt.yscale(\"linear\")\n",
    "plt.xlabel(\"Rank\", fontsize=12)\n",
    "plt.ylabel(\"Residual (Observed - Expected)\", fontsize=12)\n",
    "plt.title(\"Residuals from Zipf-Mandelbrot Fit\", fontsize=14)\n",
    "plt.legend()\n",
    "plt.grid(True, which=\"both\", linestyle=\"--\", alpha=0.5)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a threshold for outliers (e.g., 1.2x expected value)\n",
    "tolerance_factor = 3\n",
    "\n",
    "# Identify outliers (values too far from expected Zipfian behavior)\n",
    "outlier_mask = (sorted_counts > expected_values * tolerance_factor) | (sorted_counts < expected_values / tolerance_factor)\n",
    "\n",
    "# Count the number of removed points\n",
    "num_outliers = outlier_mask.sum()\n",
    "print(f\"Number of detected outliers: {num_outliers}\")\n",
    "\n",
    "# Remove outliers from dataset\n",
    "filtered_counts = sorted_counts[~outlier_mask]\n",
    "filtered_ranks = ranks[~outlier_mask]\n",
    "\n",
    "# Exclude extreme values (top 5% and bottom 5%)\n",
    "lower_bound_c = int(0.1 * len(filtered_counts))\n",
    "upper_bound_c = int(0.9 * len(filtered_counts))\n",
    "filtered_counts=filtered_counts[lower_bound_c:upper_bound_c]\n",
    "lower_bound_r = int(0.1 * len(filtered_ranks))\n",
    "upper_bound_r = int(0.9 * len(filtered_ranks))\n",
    "filtered_ranks = filtered_ranks[lower_bound_r:upper_bound_r]\n",
    "\n",
    "# Re-Fit Zipf-Mandelbrot with filtered data\n",
    "params_filtered, _ = curve_fit(\n",
    "    zipf_mandelbrot_func, \n",
    "    filtered_ranks, \n",
    "    filtered_counts, \n",
    "    p0=[1, 1, max(filtered_counts)], \n",
    "    bounds=([0.5, 0.0001, 0], [3, 10, np.inf])\n",
    ")\n",
    "\n",
    "# Extract new parameters\n",
    "s_fit_filtered, q_fit_filtered, C_fit_filtered = params_filtered\n",
    "\n",
    "# Compute expected values with new parameters\n",
    "expected_values_filtered = zipf_mandelbrot_func(filtered_ranks, s_fit_filtered, q_fit_filtered, C_fit_filtered)\n",
    "\n",
    "# Plot cleaned data vs. new Zipf-Mandelbrot fit\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.scatter(filtered_ranks, filtered_counts, label=\"Filtered Data (No Outliers)\", alpha=0.6, color=\"blue\")\n",
    "plt.plot(filtered_ranks, expected_values_filtered, 'r-', linewidth=2, label=\"Re-Fitted Zipf-Mandelbrot Model\")\n",
    "\n",
    "plt.xscale(\"log\")\n",
    "plt.yscale(\"log\")\n",
    "plt.xlabel(\"Rank\", fontsize=12)\n",
    "plt.ylabel(\"Coverage Count\", fontsize=12)\n",
    "plt.title(\"Zipf-Mandelbrot Fit After Outlier Removal & Re-Fitting\", fontsize=14)\n",
    "plt.legend()\n",
    "plt.grid(True, which=\"both\", linestyle=\"--\", alpha=0.5)\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exclude extreme values (top 5% and bottom 5%)\n",
    "lower_bound = int(0.1 * len(filtered_counts))\n",
    "upper_bound = int(0.9 * len(filtered_counts))\n",
    "\n",
    "ks_stat_truncated, p_value_truncated = kstest(\n",
    "    filtered_counts[lower_bound:upper_bound], \n",
    "    zipf_mandelbrot_func, \n",
    "    args=(s_fit_filtered, q_fit_filtered, C_fit_filtered)\n",
    ")\n",
    "ks_stat_after, p_value_after = kstest(filtered_counts, zipf_mandelbrot_func, args=(s_fit_filtered, q_fit_filtered, C_fit_filtered))\n",
    "\n",
    "\n",
    "print(f\"Truncated KS Test - Statistic: {ks_stat_truncated:.4f}, p-value: {p_value_truncated:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Estimated Zipf-Mandelbrot Exponent (s): {s_fit:.4f}\")\n",
    "from scipy.stats import norm\n",
    "\n",
    "# Compute log-likelihood\n",
    "log_likelihood = np.sum(norm.logpdf(filtered_counts, loc=zipf_mandelbrot_func(filtered_ranks, s_fit_filtered, q_fit_filtered, C_fit_filtered), scale=np.std(filtered_counts)))\n",
    "AIC_fixed = -2 * log_likelihood + 2 * 3  # 3 parameters: s, q, C\n",
    "\n",
    "print(f\"Fixed AIC: {AIC_fixed:.4f}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define pure Zipf function: f(r) = C / r^s\n",
    "def zipf_func(r, s, C):\n",
    "    return C / r ** s\n",
    "\n",
    "# Fit pure Zipf\n",
    "params_zipf, _ = curve_fit(zipf_func, filtered_ranks, filtered_counts, \n",
    "                           p0=[1, max(filtered_counts)], \n",
    "                           bounds=([0.5, 0], [3, np.inf]))\n",
    "\n",
    "s_zipf, C_zipf = params_zipf\n",
    "expected_values_zipf = zipf_func(filtered_ranks, s_zipf, C_zipf)\n",
    "\n",
    "# Compute Log-Likelihood and AIC for pure Zipf\n",
    "log_likelihood_zipf = np.sum(norm.logpdf(\n",
    "    filtered_counts, \n",
    "    loc=expected_values_zipf, \n",
    "    scale=np.std(filtered_counts)\n",
    "))\n",
    "AIC_zipf = -2 * log_likelihood_zipf + 2 * 2  # 2 parameters: s, C\n",
    "\n",
    "print(f\"Pure Zipf Log-Likelihood: {log_likelihood_zipf:.4f}\")\n",
    "print(f\"Pure Zipf AIC: {AIC_zipf:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Top 10% (frequent ranks)\n",
    "top_residuals = filtered_counts[:int(0.1 * len(filtered_counts))] - expected_values_filtered[:int(0.1 * len(filtered_counts))]\n",
    "\n",
    "# Bottom 10% (rare ranks)\n",
    "bottom_residuals = filtered_counts[-int(0.1 * len(filtered_counts)):] - expected_values_filtered[-int(0.1 * len(filtered_counts)):]\n",
    "\n",
    "# Plot residuals\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.scatter(filtered_ranks[:int(0.1 * len(filtered_ranks))], top_residuals, color='red', alpha=0.6)\n",
    "plt.axhline(0, linestyle='--', color='black', alpha=0.6)\n",
    "plt.xscale('log')\n",
    "plt.title(\"Top 10% Residuals\")\n",
    "plt.xlabel(\"Rank\")\n",
    "plt.ylabel(\"Residual\")\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.scatter(filtered_ranks[-int(0.1 * len(filtered_ranks)):], bottom_residuals, color='blue', alpha=0.6)\n",
    "plt.axhline(0, linestyle='--', color='black', alpha=0.6)\n",
    "plt.xscale('log')\n",
    "plt.title(\"Bottom 10% Residuals\")\n",
    "plt.xlabel(\"Rank\")\n",
    "plt.ylabel(\"Residual\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Focus on middle 80% of ranks\n",
    "lower_bound = int(0.1 * len(filtered_counts))\n",
    "upper_bound = int(0.9 * len(filtered_counts))\n",
    "\n",
    "ks_stat_truncated, p_value_truncated = kstest(\n",
    "    filtered_counts[lower_bound:upper_bound], \n",
    "    zipf_mandelbrot_func, \n",
    "    args=(s_fit_filtered, q_fit_filtered, C_fit_filtered)\n",
    ")\n",
    "\n",
    "print(f\"Truncated KS Test - Statistic: {ks_stat_truncated:.4f}, p-value: {p_value_truncated:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "expected_values_filtered = zipf_mandelbrot_func(filtered_ranks, s_fit_filtered, q_fit_filtered, C_fit_filtered)\n",
    "residuals_filtered = filtered_counts - expected_values_filtered\n",
    "\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.scatter(filtered_ranks, residuals_filtered, alpha=0.6, color=\"red\", label=\"Residuals (Observed - Expected)\")\n",
    "\n",
    "plt.axhline(0, linestyle=\"--\", color=\"black\", alpha=0.6)\n",
    "plt.xscale(\"log\")\n",
    "plt.yscale(\"linear\")\n",
    "plt.xlabel(\"Rank\", fontsize=12)\n",
    "plt.ylabel(\"Residual (Observed - Expected)\", fontsize=12)\n",
    "plt.title(\"Residuals After Outlier Removal\", fontsize=14)\n",
    "plt.legend()\n",
    "plt.grid(True, which=\"both\", linestyle=\"--\", alpha=0.5)\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot\n",
    "- Analyze sensor coverage by aggregating the spatial grid.\n",
    "- Visualize coverage heatmaps. To better visualize the data, apply logarithmic scaling to the color values. This will compress the range of large values and expand the smaller values for more differentiation in color. \n",
    "- We apply Fractional Power Scaling: Highlights smaller values significantly, making subtle differences more visible. We Raise the log-transformed values to a fractional power $ \\log(x+1)^{0.5} $. This amplifies small differences while keeping the general scale.\n",
    "##### Note:\n",
    "- If \\( x = 0 \\), the standard `np.log(x)` would result in an error because the logarithm of 0 is undefined. \n",
    "`np.log1p(x)` handles this safely by adding \\( 1 \\) to the input before computing the logarithm, ensuring it works for non-negative numbers, including \\( 0 \\).\n",
    "- The square root further compresses the range of the values.  \n",
    "It emphasizes smaller differences by reducing the impact of large values. For example:  \n",
    "$\\log(x+1)^{0.5} $ grows slower than $\\log(x+1)$ as $\\ x $ increases.\n",
    "\n",
    "**This transformation is particularly useful for skewed data, for `Count` values, where:**\n",
    "\n",
    "- Most data points are small.\n",
    "- A few extreme values (outliers) dominate.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up the figure size\n",
    "plt.figure(figsize=(10, 8))\n",
    "\n",
    "# Apply logarithmic scaling\n",
    "log_scaled_values = np.log1p(coverage['Count'])**0.5 \n",
    "\n",
    "# Apply logarithmic scaling to color values\n",
    "sc = plt.scatter(\n",
    "    coverage['Log_Grid'], \n",
    "    coverage['Lat_Grid'], \n",
    "    c=log_scaled_values,  \n",
    "    cmap='jet', \n",
    "    s=30, \n",
    "    edgecolor='k', \n",
    "    alpha=0.8\n",
    ")\n",
    "\n",
    "# Add a color bar with the original scale in the label\n",
    "cbar = plt.colorbar(sc)\n",
    "cbar.set_label('âˆš(Log(Coverage Count + 1))', fontsize=12)\n",
    "cbar.ax.tick_params(labelsize=10)\n",
    "\n",
    "# Add labels and title with improved font sizes\n",
    "plt.xlabel('Longitude Grid', fontsize=14)\n",
    "plt.ylabel('Latitude Grid', fontsize=14)\n",
    "plt.title('Coverage Heatmap with Logarithmic Scale', fontsize=16)\n",
    "\n",
    "# Add grid lines for reference\n",
    "plt.grid(visible=True, linestyle='--', alpha=0.6)\n",
    "\n",
    "# Improve tick sizes for better readability\n",
    "plt.xticks(fontsize=12)\n",
    "plt.yticks(fontsize=12)\n",
    "\n",
    "# Show the plot\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Analyse the Temporal Sampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate time difference in seconds between consecutive rows\n",
    "df['Delta_t'] = df['Timestamp'].diff().dt.total_seconds()  \n",
    "\n",
    "# Define an expected interval in seconds (e.g., 60 seconds)\n",
    "expected_interval = 60\n",
    "\n",
    "# Count the total number of occurrences of measurements\n",
    "tot_count = df['Delta_t'].count()\n",
    "print(f\"Number of values: {tot_count}\")\n",
    "\n",
    "# Count the number of occurrences of low frequency measurements\n",
    "highf_count = (df['Delta_t'] > expected_interval).sum()\n",
    "print(f\"Number of values higher than 60sec: {highf_count}\")\n",
    "\n",
    "# Count the number of occurrences of 0.0\n",
    "zero_count = (df['Timestamp'].diff().dt.total_seconds() == 0.0).sum()\n",
    "print(f\"Number of values equal to 0.0sec: {zero_count}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Energy and Coverage Model Preparation\n",
    "- Create columns to represent:\n",
    "\n",
    "    - Whether a street segment is already covered.\n",
    "    - Battery state changes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Filtering out Outliners"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate time difference in seconds between consecutive rows\n",
    "df['Delta_t'] = df['Timestamp'].diff().dt.total_seconds()  \n",
    "\n",
    "# Define a threshold for acceptable intervals (e.g., 60 seconds)\n",
    "acceptable_threshold = 60   # in seconds\n",
    "\n",
    "# Filter out rows with large Delta_t\n",
    "df = df[df['Delta_t'] <= acceptable_threshold]\n",
    "\n",
    "# Drop rows with Delta_t equal to zero\n",
    "df = df[df['Delta_t'] > 0]\n",
    "\n",
    "# Reset the index\n",
    "df = df.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The battery capacity is 10,000 mAh, so SOC_Change (calculated from current and time) must be converted into a percentage of the total capacity before being added to `SOC_batt`.\n",
    "\n",
    "### SOC Update Formula:\n",
    "$SOC_{new} = SOC_{old} + \\frac{\\Delta SOC_{mAh}}{C_{batt}} \\times 100$ \n",
    "\n",
    "and \n",
    "\n",
    "$\\Delta SOC_{mAh} = -1 \\times I_{batt} \\times \\Delta t$ (mAh change based on current and time)\n",
    "\n",
    "Where:\n",
    "- $SOC_{old}$ is $SOC_{batt}$\n",
    "- $\\ I_{batt} $: Net current (`current_batt`) in mA (positive for consumption, negative for storage).\n",
    "- $\\ \\Delta t $: Time difference in hours between consecutive rows.\n",
    "- $\\ C_{batt} $: Battery capacity in mAh (10,000 mAh).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Battery capacity in mAh\n",
    "battery_capacity = 10000\n",
    "\n",
    "# Calculate time difference in hours between consecutive rows\n",
    "df['Delta_t'] = df['Delta_t'] / 3600  # Time difference in hours\n",
    "\n",
    "# Calculate SOC change (in %) using the corrected formula\n",
    "df['SOC_Change'] = (-1 * df['current_batt'] * df['Delta_t'] / battery_capacity) * 100\n",
    "\n",
    "# Set SOC_Change to 0 when SOC is saturated\n",
    "df.loc[df['SOC_batt'] >= 90, 'SOC_Change'] = 0\n",
    "\n",
    "# Ensure SOC values are capped between 0 and 100\n",
    "df['SOC_batt'] = df['SOC_batt'] + df['SOC_Change']\n",
    "df['SOC_batt'] = df['SOC_batt'].clip(lower=0, upper=100)\n",
    "\n",
    "# Assume that is >90%, charging is stopped\n",
    "df.loc[df['SOC_batt'] >= 90, 'SOC_Change'] = 0\n",
    "\n",
    "# Preview the updated DataFrame\n",
    "print(df[['Timestamp', 'Lat', 'Log', 'SOC_batt', 'SOC_Change', 'Delta_t', 'current_batt']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot SOC over time\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.plot(df['Timestamp'], df['SOC_batt'], label='State of Charge (SOC)', color='blue', linewidth=1.5)\n",
    "plt.xlabel('Time', fontsize=14)\n",
    "plt.ylabel('SOC (%)', fontsize=14)\n",
    "plt.title('Battery State of Charge Over Time', fontsize=16)\n",
    "plt.grid(alpha=0.5, linestyle='--')\n",
    "plt.legend(fontsize=12)\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter rows where SOC is increasing or decreasing\n",
    "increasing = df[df['SOC_Change'] > 0]\n",
    "decreasing = df[df['SOC_Change'] < 0]\n",
    "\n",
    "print(f\"Number of times SOC increases: {len(increasing)}\")\n",
    "print(f\"Number of times SOC decreases: {len(decreasing)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mark covered segments (spatio-temporal condition)\n",
    "df['Is_Covered'] = df.duplicated(subset=['Lat_Grid', 'Log_Grid', 'Time_Bin'], keep='first')\n",
    "\n",
    "# Preview the updated DataFrame\n",
    "print(df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def energy_aware_switch(row):\n",
    "    if row['Is_Covered'] and row['SOC_batt'] > 20:\n",
    "        return 'OFF'\n",
    "    elif not row['Is_Covered'] and row['SOC_batt'] > 10:\n",
    "        return 'ON'\n",
    "    else:\n",
    "        return 'IDLE'\n",
    "\n",
    "df['Sensor_State'] = df.apply(energy_aware_switch, axis=1)\n",
    "\n",
    "df['Cumulative_Spatial_Coverage'] = (~df['Is_Covered']).cumsum()\n",
    "temporal_coverage = df.groupby(['Lat_Grid', 'Log_Grid', 'Time_Bin']).size().reset_index(name='Frequency')\n",
    "\n",
    "high_coverage_cells = temporal_coverage[temporal_coverage['Frequency'] > 2]\n",
    "high_coverage_cells\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize cumulative coverage and energy tracking\n",
    "df['Cumulative_Coverage'] = 0\n",
    "df['Average_SOC'] = 0\n",
    "\n",
    "# Initialize variables\n",
    "cumulative_coverage = set()  # To store unique covered grid cells\n",
    "soc_list = []  # To store SOC levels\n",
    "\n",
    "# Simulate over the data\n",
    "for idx, row in df.iterrows():\n",
    "    # Update cumulative coverage if sensor is ON\n",
    "    if row['Sensor_State'] == 'ON':\n",
    "        cumulative_coverage.add((row['Lat_Grid'], row['Log_Grid']))\n",
    "\n",
    "    # Update SOC tracking\n",
    "    soc_list.append(row['SOC_batt'])\n",
    "\n",
    "    # Update DataFrame\n",
    "    df.at[idx, 'Cumulative_Coverage'] = len(cumulative_coverage)\n",
    "    df.at[idx, 'Average_SOC'] = sum(soc_list) / len(soc_list)\n",
    "\n",
    "# Preview results\n",
    "print(df[['Timestamp', 'Cumulative_Coverage', 'Average_SOC']])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot cumulative coverage over time\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(df['Timestamp'], df['Cumulative_Coverage'], label='Cumulative Coverage', color='b')\n",
    "plt.xlabel('Time', fontsize=14)\n",
    "plt.ylabel('Cumulative Coverage', fontsize=14)\n",
    "plt.title('Cumulative Coverage Over Time', fontsize=16)\n",
    "plt.grid(True, linestyle='--', alpha=0.6)\n",
    "plt.legend(fontsize=12)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Plot average SOC over time\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(df['Timestamp'], df['Average_SOC'], label='Average SOC', color='g')\n",
    "plt.xlabel('Time', fontsize=14)\n",
    "plt.ylabel('Average SOC (%)', fontsize=14)\n",
    "plt.title('Average SOC Over Time', fontsize=16)\n",
    "plt.grid(True, linestyle='--', alpha=0.6)\n",
    "plt.legend(fontsize=12)\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Baseline: All sensors ON\n",
    "df['Baseline_Coverage'] = 0\n",
    "df['Baseline_SOC'] = 0.0\n",
    "\n",
    "# Initialize variables\n",
    "baseline_coverage = set()\n",
    "baseline_soc_list = []\n",
    "\n",
    "for idx, row in df.iterrows():\n",
    "    # Assume sensors are always ON\n",
    "    baseline_coverage.add((row['Lat_Grid'], row['Log_Grid']))\n",
    "    baseline_soc_list.append(row['SOC_batt'])\n",
    "\n",
    "    # Update DataFrame\n",
    "    df.at[idx, 'Baseline_Coverage'] = len(baseline_coverage)\n",
    "    df.at[idx, 'Baseline_SOC'] = sum(baseline_soc_list) / len(baseline_soc_list)\n",
    "\n",
    "# Compare cumulative coverage and SOC\n",
    "print(df[['Timestamp', 'Cumulative_Coverage', 'Baseline_Coverage', 'Average_SOC', 'Baseline_SOC']])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Total cumulative coverage\n",
    "energy_aware_coverage = df['Cumulative_Coverage'].iloc[-1]\n",
    "baseline_coverage = df['Baseline_Coverage'].iloc[-1]\n",
    "\n",
    "# Average SOC\n",
    "energy_aware_avg_soc = df['Average_SOC'].mean()\n",
    "baseline_avg_soc = df['Baseline_SOC'].mean()\n",
    "\n",
    "# Improvement metrics\n",
    "coverage_improvement = (energy_aware_coverage - baseline_coverage) / baseline_coverage * 100\n",
    "soc_savings = (baseline_avg_soc - energy_aware_avg_soc) / baseline_avg_soc * 100\n",
    "\n",
    "# Print results\n",
    "print(f\"Energy-Aware Total Coverage: {energy_aware_coverage}\")\n",
    "print(f\"Baseline Total Coverage: {baseline_coverage}\")\n",
    "print(f\"Coverage Improvement: {coverage_improvement:.2f}%\")\n",
    "print(f\"Energy Savings: {soc_savings:.2f}%\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
